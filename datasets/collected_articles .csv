Discipline,Subfield,Methodology,Title,Abstract,URL,Source
Computer Science,Algorithms and Data Structures,Quantitative,Data structures for maintaining set partitions,"<jats:title>Abstract</jats:title><jats:p>Efficiently maintaining the partition induced by a set of features is an important problem in building decision‐tree classifiers. In order to identify a small set of discriminating features, we need the capability of efficiently adding and removing specific features and determining the effect of these changes on the induced classification or partition. In this paper we introduce a variety of randomized and deterministic data structures to support these operations on both general and geometrically induced set partitions. We give both Monte Carlo and Las Vegas data structures that realize near‐optimal time bounds and are practical to implement. We then provide a faster solution to this problem in the geometric setting. Finally, we present a data structure that efficiently estimates the number of partitions separating elements. © 2004 Wiley Periodicals, Inc. Random Struct. Alg., 2004</jats:p>",https://doi.org/10.1002/rsa.20025,CrossRef
Computer Science,Algorithms and Data Structures,Quantitative,Segment-Based Clustering of Hyperspectral Images Using Tree-Based Data Partitioning Structures,"<jats:p>Hyperspectral image classification has been increasingly used in the field of remote sensing. In this study, a new clustering framework for large-scale hyperspectral image (HSI) classification is proposed. The proposed four-step classification scheme explores how to effectively use the global spectral information and local spatial structure of hyperspectral data for HSI classification. Initially, multidimensional Watershed is used for pre-segmentation. Region-based hierarchical hyperspectral image segmentation is based on the construction of Binary partition trees (BPT). Each segmented region is modeled while using first-order parametric modelling, which is then followed by a region merging stage using HSI regional spectral properties in order to obtain a BPT representation. The tree is then pruned to obtain a more compact representation. In addition, principal component analysis (PCA) is utilized for HSI feature extraction, so that the extracted features are further incorporated into the BPT. Finally, an efficient variant of k-means clustering algorithm, called filtering algorithm, is deployed on the created BPT structure, producing the final cluster map. The proposed method is tested over eight publicly available hyperspectral scenes with ground truth data and it is further compared with other clustering frameworks. The extensive experimental analysis demonstrates the efficacy of the proposed method.</jats:p>",https://doi.org/10.3390/a13120330,CrossRef
Computer Science,Algorithms and Data Structures,Quantitative,Efficient Data Structures for Range Shortest Unique Substring Queries,"<jats:p>Let T[1,n] be a string of length n and T[i,j] be the substring of T starting at position i and ending at position j. A substring T[i,j] of T is a repeat if it occurs more than once in T; otherwise, it is a unique substring of T. Repeats and unique substrings are of great interest in computational biology and information retrieval. Given string T as input, the Shortest Unique Substring problem is to find a shortest substring of T that does not occur elsewhere in T. In this paper, we introduce the range variant of this problem, which we call the Range Shortest Unique Substring problem. The task is to construct a data structure over T answering the following type of online queries efficiently. Given a range [α,β], return a shortest substring T[i,j] of T with exactly one occurrence in [α,β]. We present an O(nlogn)-word data structure with O(logwn) query time, where w=Ω(logn) is the word size. Our construction is based on a non-trivial reduction allowing for us to apply a recently introduced optimal geometric data structure [Chan et al., ICALP 2018]. Additionally, we present an O(n)-word data structure with O(nlogϵn) query time, where ϵ&gt;0 is an arbitrarily small constant. The latter data structure relies heavily on another geometric data structure [Nekrich and Navarro, SWAT 2012].</jats:p>",https://doi.org/10.3390/a13110276,CrossRef
Computer Science,Algorithms and Data Structures,Quantitative,Voxelisation Algorithms and Data Structures: A Review,"<jats:p>Voxel-based data structures, algorithms, frameworks, and interfaces have been used in computer graphics and many other applications for decades. There is a general necessity to seek adequate digital representations, such as voxels, that would secure unified data structures, multi-resolution options, robust validation procedures and flexible algorithms for different 3D tasks. In this review, we evaluate the most common properties and algorithms for voxelisation of 2D and 3D objects. Thus, many voxelisation algorithms and their characteristics are presented targeting points, lines, triangles, surfaces and solids as geometric primitives. For lines, we identify three groups of algorithms, where the first two achieve different voxelisation connectivity, while the third one presents voxelisation of curves. We can say that surface voxelisation is a more desired voxelisation type compared to solid voxelisation, as it can be achieved faster and requires less memory if voxels are stored in a sparse way. At the same time, we evaluate in the paper the available voxel data structures. We split all data structures into static and dynamic grids considering the frequency to update a data structure. Static grids are dominated by SVO-based data structures focusing on memory footprint reduction and attributes preservation, where SVDAG and SSVDAG are the most advanced methods. The state-of-the-art dynamic voxel data structure is NanoVDB which is superior to the rest in terms of speed as well as support for out-of-core processing and data management, which is the key to handling large dynamically changing scenes. Overall, we can say that this is the first review evaluating the available voxelisation algorithms for different geometric primitives as well as voxel data structures.</jats:p>",https://doi.org/10.3390/s21248241,CrossRef
Computer Science,Algorithms and Data Structures,Quantitative,"Key Concepts, Weakness and Benchmark on Hash Table Data Structures","<jats:p>Most computer programs or applications need fast data structures. The performance of a data structure is necessarily influenced by the complexity of its common operations; thus, any data-structure that exhibits a theoretical complexity of amortized constant time in several of its main operations should draw a lot of attention. Such is the case of a family of data structures that is called hash tables. However, what is the real efficiency of these hash tables? That is an interesting question with no simple answer and there are some issues to be considered. Of course, there is not a unique hash table; in fact, there are several sub-groups of hash tables, and, even more, not all programming languages use the same variety of hash tables in their default hash table implementation, neither they have the same interface. Nevertheless, all hash tables do have a common issue: they have to solve hash collisions; that is a potential weakness and it also induces a classification of hash tables according to the strategy to solve collisions. In this paper, some key concepts about hash tables are exposed and some definitions about those key concepts are reviewed and clarified, especially in order to study the characteristics of the main strategies to implement hash tables and how they deal with hash collisions. Then, some benchmark cases are designed and presented to assess the performance of hash tables. The cases have been designed to be randomized, to be self-tested, to be representative of a real user cases, and to expose and analyze the impact of different factors over the performance across different hash tables and programming languages. Then, all cases have been programmed using C++, Java and Python and analyzed in terms of interfaces and efficiency (time and memory). The benchmark yields important results about the performance of these structures and its (lack of) relationship with complexity analysis.</jats:p>",https://doi.org/10.3390/a15030100,CrossRef
Computer Science,Algorithms and Data Structures,Qualitative,ANALYSIS OF ALGORITHMS FOR THE FORMATION OF CODE STRUCTURES AFFECTING THE QUALITY OF DATA TRANSMISSION IN INFOCOMMUNICATION SYSTEMS,"<jats:p>Achieving the best quality indicators of transmission through the channels of modern communication systems is always of urgent importance for developers and users of information communication systems. At the same time, to ensure high transmission reliability and performance, various coding methods and methods of converting the transmitted data can be used. Accordingly, each of the proposed data processing methods has its advantages and disadvantages, which determines their attractiveness or application limitations for the data being transmitted. In this paper, the analysis of algorithms for forming signal code structures of traditional interference-resistant positional codes, codes with an even number of units, and a nine-element Hamming code is carried out. Classical positional coding is inferior in such important data transmission parameters as information capacity and entropy. However, in some cases, for example, when the results of data transfer quality parameters calculations are significantly influenced by the language of the transmitted textual information or the volume of the analyzed text (the influence is more likely to be noticeable in individual cases, and not in general), then an important value acquires the possibility of applying a certain/adapted type of encoding of the transmitted data. The article conducts theoretical research and calculates the main qualitative parameters of the studied code constructions for various initial conditions. A comparative analysis and assessment of the influence of the studied parameters on the quality of data transmission was carried out. Conducted theoretical studies show that, under certain conditions, positional coding provides a significant gain in the main quality indicators of data processing and can be successfully applied for coding and transmission of digital data through the channels of modern information transmission systems. In order to practically confirm the reliability of the obtained results, relevant studies should be conducted, for example, with simulation on a software model of a virtual transmission system with different coding principles on modern computer systems.</jats:p>",https://doi.org/10.28925/2663-4023.2024.24.9098,CrossRef
Computer Science,Algorithms and Data Structures,Qualitative,Avoiding a giant component,"<jats:title>Abstract</jats:title><jats:p>Let<jats:italic>e</jats:italic><jats:sub>1</jats:sub>, <jats:italic>e</jats:italic>′<jats:sub>1</jats:sub>;<jats:italic>e</jats:italic><jats:sub>2</jats:sub>, <jats:italic>e</jats:italic>′<jats:sub>2</jats:sub>;…;<jats:italic>e<jats:sub>i</jats:sub></jats:italic>, <jats:italic>e</jats:italic>′<jats:sub><jats:italic>i</jats:italic></jats:sub>;⋅⋅⋅ be a sequence of ordered pairs of edges chosen uniformly at random from the edge set of the complete graph<jats:italic>K<jats:sub>n</jats:sub></jats:italic>(i.e. we sample with replacement). This sequence is used to form a graph by choosing at stage<jats:italic>i</jats:italic>,<jats:italic>i</jats:italic>=1,…, one edge from<jats:italic>e<jats:sub>i</jats:sub></jats:italic>,<jats:italic>e</jats:italic>′<jats:sub><jats:italic>i</jats:italic></jats:sub>to be an edge in the graph, where the choice at stage<jats:italic>i</jats:italic>is based only on the observation of the edges that have appeared by stage<jats:italic>i</jats:italic>. We show that these choices can be made so that<jats:bold>whp</jats:bold>the size of the largest component of the graph formed at stage 0.535<jats:italic>n</jats:italic>is polylogarithmic in<jats:italic>n</jats:italic>. This resolves a question of Achlioptas. © 2001 John Wiley &amp; Sons, Inc. Random Struct. Alg., 19, 75–85, 2001</jats:p>",https://doi.org/10.1002/rsa.1019,CrossRef
Computer Science,Algorithms and Data Structures,Qualitative,Feasibility of machine learning algorithms for classifying damaged offshore jacket structures using SCADA data,"<jats:title>Abstract</jats:title>
               <jats:p>The best practise for structural damage detection currently relies on the installation of structural health monitoring systems for the collection of dedicated high frequency measurements. Switching to the employment of the wind turbine’s SCADA (Supervisory Control and Data Acquisition) signals and their commonly recorded low frequency statistics can lead to a reduction in the number of ad-hoc monitoring sensors and quantity of data required. In this paper, aero-hydro-servo-elastic simulations for a model of a turbine are used to assess its loads and any changes in the dynamics under healthy state and a damaged configuration case study. To prove the feasibility of the damage detection through low-resolution data, the statistics of the typically recorded signals from the SCADA and the structural monitoring systems are fed into a database for training and testing of classification algorithms. The ability of the machine learning models to generalise the classification for both stochasticity and uncertainties in the environmental conditions are tested. Decision tree-based classifiers showed the capability to capture the damage for the majority of the operating conditions considered. Though the setup of the traditional SCADA sensors had to be supplemented with an additional structural health monitoring sensor, the detection of the damage has been shown feasible by referring to low-frequency statistics only.</jats:p>",https://doi.org/10.1088/1742-6596/1669/1/012021,CrossRef
Computer Science,Algorithms and Data Structures,Qualitative,Infinite paths in randomly oriented lattices,"<jats:title>Abstract</jats:title><jats:p>The square lattice is used to generate an oriented graph in which a rightward or upward arrow is present on each edge with probability <jats:italic>a</jats:italic>, and a leftward or downward arrow with probability <jats:italic>b</jats:italic>. Independence between different edges of the square lattice is assumed, but nothing is assumed concerning the dependence between the two possible orientations at any given edge. A property of self‐duality is exploited to show that, when <jats:italic>a</jats:italic>+<jats:italic>b</jats:italic>=1, the process is, in a sense to be made precise, either critical or supercritical, but not subcritical. This observation enables progress with the percolation problem in which each horizontal edge is oriented rightward with probability <jats:italic>p</jats:italic> and otherwise leftward, and each vertical edge is oriented upward with probability <jats:italic>p</jats:italic> and otherwise downward. © 2001 John Wiley &amp; Sons, Inc. Random Struct. Alg., 18, 257–266, 2001</jats:p>",https://doi.org/10.1002/rsa.1007,CrossRef
Computer Science,Algorithms and Data Structures,Qualitative,Temporal Multimodal Data-Processing Algorithms Based on Algebraic System of Aggregates,"<jats:p>In many tasks related to an object’s observation or real-time monitoring, the gathering of temporal multimodal data is required. Such data sets are semantically connected as they reflect different aspects of the same object. However, data sets of different modalities are usually stored and processed independently. This paper presents an approach based on the application of the Algebraic System of Aggregates (ASA) operations that enable the creation of an object’s complex representation, referred to as multi-image (MI). The representation of temporal multimodal data sets as the object’s MI yields simple data-processing procedures as it provides a solid semantic connection between data describing different features of the same object, process, or phenomenon. In terms of software development, the MI is a complex data structure used for data processing with ASA operations. This paper provides a detailed presentation of this concept.</jats:p>",https://doi.org/10.3390/a16040186,CrossRef
Computer Science,Algorithms and Data Structures,Mixed Methods,Data Mining Algorithms for Smart Cities: A Bibliometric Analysis,"<jats:p>Smart cities connect people and places using innovative technologies such as Data Mining (DM), Machine Learning (ML), big data, and the Internet of Things (IoT). This paper presents a bibliometric analysis to provide a comprehensive overview of studies associated with DM technologies used in smart cities applications. The study aims to identify the main DM techniques used in the context of smart cities and how the research field of DM for smart cities evolves over time. We adopted both qualitative and quantitative methods to explore the topic. We used the Scopus database to find relative articles published in scientific journals. This study covers 197 articles published over the period from 2013 to 2021. For the bibliometric analysis, we used the Biliometrix library, developed in R. Our findings show that there is a wide range of DM technologies used in every layer of a smart city project. Several ML algorithms, supervised or unsupervised, are adopted for operating the instrumentation, middleware, and application layer. The bibliometric analysis shows that DM for smart cities is a fast-growing scientific field. Scientists from all over the world show a great interest in researching and collaborating on this interdisciplinary scientific field.</jats:p>",https://doi.org/10.3390/a14080242,CrossRef
Computer Science,Algorithms and Data Structures,Mixed Methods,Hunting for sharp thresholds,"<jats:title>Abstract</jats:title><jats:p>A basic phenomenon in random structures such as random graphs is the threshold phenomenon, where a system undergoes a swift qualitative change as result of a small change in a parameter guiding its probabilistic structure. In an earlier paper [J Amer Math Soc 12 (1999), 1017–1054] a general criterion was presented for structures to undergo such a phase transition. In this paper we give a survey of the state of the art in applying the aforementioned criterion, exemplify the techniques by proving the existence of a sharp threshold for hypergraph colorability, and present some related open problems. © 2004 Wiley Periodicals, Inc. Random Struct. Alg., 26, 2005</jats:p>",https://doi.org/10.1002/rsa.20042,CrossRef
Computer Science,Algorithms and Data Structures,Mixed Methods,Graph‐based data structures for skeleton‐based refinement algorithms,"<jats:title>Abstract</jats:title><jats:p>In this paper, we discuss a class of adaptive refinement algorithms for generating unstructured meshes in two and three dimensions. We focus on skeleton‐based refinement (SBR) algorithms as proposed by Plaza and Carey (<jats:italic>Appl. Numer. Math.</jats:italic> 2000; <jats:bold>32</jats:bold>:195) and provide an extension that involves the introduction of the graph of the skeleton for meshes consisting of simplex cells. By the use of data structures derived from the graph of the skeleton, we reformulate the SBR scheme and devise a more natural and consistent approach for this class of adaptive refinement algorithms. As an illustrative case, we discuss in detail the graphs for 2D refinement of triangulations and for 3D we propose a corresponding new face‐based data structure for tetrahedra. Experiments using the 2D algorithm and exploring the properties of the associated graph are provided. Copyright © 2001 John Wiley &amp; Sons, Ltd.</jats:p>",https://doi.org/10.1002/cnm.460,CrossRef
Computer Science,Algorithms and Data Structures,Mixed Methods,Percolation critical probabilities of matching lattice‐pairs,"<jats:title>Abstract</jats:title><jats:p>A necessary and sufficient condition is established for the strict inequality  between the critical probabilities of site percolation on a one‐ended, quasi‐transitive, plane graph  and on its matching graph . When  is transitive, strict inequality holds if and only if  is not a triangulation. The basic approach is the standard method of enhancements, but its implementation has complexity arising from the non‐Euclidean (hyperbolic) space, the study of site (rather than bond) percolation, and the generality of the assumption of quasi‐transitivity. This result is complementary to the work of the authors (“Hyperbolic site percolation,” <jats:styled-content>arXiv:2203.00981</jats:styled-content>) on the equality , where  is the critical probability for the existence of a unique infinite open cluster. It implies for transitive, one‐ended  that , with equality if and only if  is a triangulation.</jats:p>",https://doi.org/10.1002/rsa.21226,CrossRef
Computer Science,Algorithms and Data Structures,Mixed Methods,"Entropy, Triangulation, and Point Location in Planar Subdivisions","A data structure is presented for point location in connected planar
subdivisions when the distribution of queries is known in advance. The data
structure has an expected query time that is within a constant factor of
optimal. More specifically, an algorithm is presented that preprocesses a
connected planar subdivision G of size n and a query distribution D to produce
a point location data structure for G. The expected number of point-line
comparisons performed by this data structure, when the queries are distributed
according to D, is H + O(H^{2/3}+1) where H=H(G,D) is a lower bound on the
expected number of point-line comparisons performed by any linear decision tree
for point location in G under the query distribution D. The preprocessing
algorithm runs in O(n log n) time and produces a data structure of size O(n).
These results are obtained by creating a Steiner triangulation of G that has
near-minimum entropy.",http://arxiv.org/abs/0901.1908v1,arXiv
Computer Science,Algorithms and Data Structures,Design and Development,Note on distance matrix hashing,"Hashing algorithm of dynamical set of distances is described. Proposed
hashing function is residual. Data structure which implementation accelerates
computations is presented",http://arxiv.org/abs/1901.09505v2,arXiv
Computer Science,Algorithms and Data Structures,Design and Development,"Efficient algorithms for enumerating maximal common subsequences of two
  strings","We propose efficient algorithms for enumerating maximal common subsequences
(MCSs) of two strings. Efficiency of the algorithms are estimated by the
preprocessing-time, space, and delay-time complexities. One algorithm prepares
a cubic-space data structure in cubic time to output each MCS in linear time.
This data structure can be used to search for particular MCSs satisfying some
condition without performing an explicit enumeration. Another prepares a
quadratic-space data structure in quadratic time to output each MCS in linear
time, and the other prepares a linear-space data structure in quadratic time to
output each MCS in linearithmic time.",http://arxiv.org/abs/2307.10552v1,arXiv
Computer Science,Algorithms and Data Structures,Design and Development,3-Coloring in Time O(1.3217^n),"We propose a new algorithm for 3-coloring that runs in time O(1.3217^n). For
this algorithm, we make use of the time O(1.3289^n) algorithm for 3-coloring by
Beigel and Eppstein. They described a structure in all graphs, whose vertices
could be colored relatively easily. In this paper, we improve upon this
structure and present new ways to determine how the involved vertices reduce
the runtime of the algorithm.",http://arxiv.org/abs/2302.13644v1,arXiv
Computer Science,Algorithms and Data Structures,Design and Development,"A Note on the Performance of Algorithms for Solving Linear Diophantine
  Equations in the Naturals","We implement four algorithms for solving linear Diophantine equations in the
naturals: a lexicographic enumeration algorithm, a completion procedure, a
graph-based algorithm, and the Slopes algorithm. As already known, the
lexicographic enumeration algorithm and the completion procedure are slower
than the other two algorithms. We compare in more detail the graph-based
algorithm and the Slopes algorithm. In contrast to previous comparisons, our
work suggests that they are equally fast on small inputs, but the graph-based
algorithm gets much faster as the input grows. We conclude that implementations
of AC-unification algorithms should use the graph-based algorithm for maximum
efficiency.",http://arxiv.org/abs/2104.05200v1,arXiv
Computer Science,Algorithms and Data Structures,Design and Development,"Evolving Categories: Consistent Framework for Representation of Data and
  Algorithms","A concept of ""evolving categories"" is suggested to build a simple, scalable,
mathematically consistent framework for representing in uniform way both data
and algorithms. A state machine for executing algorithms becomes clear, rich
and powerful semantics, based on category theory, and still allows easy
implementation. Moreover, it gives an original insight into the nature and
semantics of algorithms.",http://arxiv.org/abs/cs/0412089v1,arXiv
Computer Science,Algorithms and Data Structures,Theoretical / Conceptual,Fast Clustering using MapReduce,"Clustering problems have numerous applications and are becoming more
challenging as the size of the data increases. In this paper, we consider
designing clustering algorithms that can be used in MapReduce, the most popular
programming environment for processing large datasets. We focus on the
practical and popular clustering problems, $k$-center and $k$-median. We
develop fast clustering algorithms with constant factor approximation
guarantees. From a theoretical perspective, we give the first analysis that
shows several clustering algorithms are in $\mathcal{MRC}^0$, a theoretical
MapReduce class introduced by Karloff et al. \cite{KarloffSV10}. Our algorithms
use sampling to decrease the data size and they run a time consuming clustering
algorithm such as local search or Lloyd's algorithm on the resulting data set.
Our algorithms have sufficient flexibility to be used in practice since they
run in a constant number of MapReduce rounds. We complement these results by
performing experiments using our algorithms. We compare the empirical
performance of our algorithms to several sequential and parallel algorithms for
the $k$-median problem. The experiments show that our algorithms' solutions are
similar to or better than the other algorithms' solutions. Furthermore, on data
sets that are sufficiently large, our algorithms are faster than the other
parallel algorithms that we tested.",http://arxiv.org/abs/1109.1579v1,arXiv
Computer Science,Algorithms and Data Structures,Theoretical / Conceptual,"Optimally selecting the top $k$ values from $X+Y$ with layer-ordered
  heaps","Selection and sorting the Cartesian sum, $X+Y$, are classic and important
problems. Here, a new algorithm is presented, which generates the top $k$
values of the form $X_i+Y_j$. The algorithm relies only on median-of-medians
and is simple to implement. Furthermore, it uses data structures contiguous in
memory, and is fast in practice. The presented algorithm is demonstrated to be
theoretically optimal.",http://arxiv.org/abs/2001.11607v2,arXiv
Computer Science,Algorithms and Data Structures,Theoretical / Conceptual,Theories of Hypergraph-Graph (HG(2)) Data Structure,"Current paper introduces a Hypergraph Graph model of data storage which can
be represented as a hybrid data structure based on Hypergraph and Graph. The
pro-posed data structure is claimed to realize complex combinatorial
structures. The formal definition of the data structure is presented along with
the proper justification from real world scenarios. The paper reports some
elementary concepts of Hypergraph and presents theoretical aspects of the
proposed data structure including the concepts of Path, Cycle etc. The detailed
analysis of weighted HG 2 is presented along with discussions on Cost involved
with HG 2 paths.",http://arxiv.org/abs/1311.7201v1,arXiv
Computer Science,Algorithms and Data Structures,Theoretical / Conceptual,Efficient Kernelization Algorithm for Bipartite Graph Matching,"Finding the maximum matching in bipartite graphs is a fundamental graph
operation widely used in various fields. To expedite the acquisition of the
maximum matching, Karp and Sipser introduced two data reduction rules aimed at
decreasing the input size. However, the KaSi algorithm, which implements the
two data reduction rules, has several drawbacks: a high upper bound on time
complexity and inefficient storage structure. The poor upper bound on time
complexity makes the algorithm lack robustness when dealing with extreme cases,
and the inefficient storage structure struggles to balance vertex merging and
neighborhood traversal operations, leading to poor performance on real-life
graphs. To address these issues, we introduced MVM, an algorithm incorporating
three novel optimization strategies to implement the data reduction rules. Our
theoretical analysis proves that the MVM algorithm, even when using data
structures with the worst search efficiency, can still maintain near-linear
time complexity, ensuring the algorithm's robustness. Additionally, we designed
an innovative storage format that supports efficient vertex merging operations
while preserving the locality of edge sets, thus ensuring the efficiency of
neighborhood traversals in graph algorithms. Finally, we conduct evaluations on
both real-life and synthetic graphs. Extensive experiments demonstrate the
superiority of our method.",http://arxiv.org/abs/2412.00704v1,arXiv
Computer Science,Algorithms and Data Structures,Theoretical / Conceptual,Nearest Neighbor based Clustering Algorithm for Large Data Sets,"Clustering is an unsupervised learning technique in which data or objects are
grouped into sets based on some similarity measure. Most of the clustering
algorithms assume that the main memory is infinite and can accommodate the set
of patterns. In reality many applications give rise to a large set of patterns
which does not fit in the main memory. When the data set is too large, much of
the data is stored in the secondary memory. Input/Outputs (I/O) from the disk
are the major bottleneck in designing efficient clustering algorithms for large
data sets. Different designing techniques have been used to design clustering
algorithms for large data sets. External memory algorithms are one class of
algorithms which can be used for large data sets. These algorithms exploit the
hierarchical memory structure of the computers by incorporating locality of
reference directly in the algorithm. This paper makes some contribution towards
designing clustering algorithms in the external memory model (Proposed by
Aggarwal and Vitter 1988) to make the algorithms scalable. In this paper, it is
shown that the Shared near neighbors algorithm is not very I/O efficient since
the computational complexity is same as the I/O complexity. The algorithm is
designed in the external memory model and I/O complexity is reduced. The
computational complexity remains same. We substantiate the theoretical analysis
by showing the performance of the algorithms with their traditional counterpart
by implementing in STXXL library.",http://arxiv.org/abs/1505.05962v1,arXiv
Computer Science,Artificial Intelligence and Machine Learning,Quantitative,Creativity and Artificial Intelligence: A Digital Art Perspective,"This paper describes the application of artificial intelligence to the
creation of digital art. AI is a computational paradigm that codifies
intelligence into machines. There are generally three types of artificial
intelligence and these are machine learning, evolutionary programming and soft
computing. Machine learning is the statistical approach to building intelligent
systems. Evolutionary programming is the use of natural evolutionary systems to
design intelligent machines. Some of the evolutionary programming systems
include genetic algorithm which is inspired by the principles of evolution and
swarm optimization which is inspired by the swarming of birds, fish, ants etc.
Soft computing includes techniques such as agent based modelling and fuzzy
logic. Opportunities on the applications of these to digital art are explored.",http://arxiv.org/abs/1807.08195v1,arXiv
Computer Science,Artificial Intelligence and Machine Learning,Quantitative,Comprehensible Artificial Intelligence on Knowledge Graphs: A survey,"Artificial Intelligence applications gradually move outside the safe walls of
research labs and invade our daily lives. This is also true for Machine
Learning methods on Knowledge Graphs, which has led to a steady increase in
their application since the beginning of the 21st century. However, in many
applications, users require an explanation of the Artificial Intelligences
decision. This led to increased demand for Comprehensible Artificial
Intelligence. Knowledge Graphs epitomize fertile soil for Comprehensible
Artificial Intelligence, due to their ability to display connected data, i.e.
knowledge, in a human- as well as machine-readable way. This survey gives a
short history to Comprehensible Artificial Intelligence on Knowledge Graphs.
Furthermore, we contribute by arguing that the concept Explainable Artificial
Intelligence is overloaded and overlapping with Interpretable Machine Learning.
By introducing the parent concept Comprehensible Artificial Intelligence, we
provide a clear-cut distinction of both concepts while accounting for their
similarities. Thus, we provide in this survey a case for Comprehensible
Artificial Intelligence on Knowledge Graphs consisting of Interpretable Machine
Learning on Knowledge Graphs and Explainable Artificial Intelligence on
Knowledge Graphs. This leads to the introduction of a novel taxonomy for
Comprehensible Artificial Intelligence on Knowledge Graphs. In addition, a
comprehensive overview of the research on Comprehensible Artificial
Intelligence on Knowledge Graphs is presented and put into the context of the
taxonomy. Finally, research gaps in the field of Comprehensible Artificial
Intelligence on Knowledge Graphs are identified for future research.",http://arxiv.org/abs/2404.03499v1,arXiv
Computer Science,Artificial Intelligence and Machine Learning,Quantitative,"Surveying the reach and maturity of machine learning and artificial
  intelligence in astronomy","Machine learning (automated processes that learn by example in order to
classify, predict, discover or generate new data) and artificial intelligence
(methods by which a computer makes decisions or discoveries that would usually
require human intelligence) are now firmly established in astronomy. Every
week, new applications of machine learning and artificial intelligence are
added to a growing corpus of work. Random forests, support vector machines, and
neural networks (artificial, deep, and convolutional) are now having a genuine
impact for applications as diverse as discovering extrasolar planets, transient
objects, quasars, and gravitationally-lensed systems, forecasting solar
activity, and distinguishing between signals and instrumental effects in
gravitational wave astronomy. This review surveys contemporary, published
literature on machine learning and artificial intelligence in astronomy and
astrophysics. Applications span seven main categories of activity:
classification, regression, clustering, forecasting, generation, discovery, and
the development of new scientific insight. These categories form the basis of a
hierarchy of maturity, as the use of machine learning and artificial
intelligence emerges, progresses or becomes established.",http://arxiv.org/abs/1912.02934v1,arXiv
Computer Science,Artificial Intelligence and Machine Learning,Quantitative,"Design of a dynamic and self adapting system, supported with artificial
  intelligence, machine learning and real time intelligence for predictive
  cyber risk analytics in extreme environments, cyber risk in the colonisation
  of Mars","Multiple governmental agencies and private organisations have made
commitments for the colonisation of Mars. Such colonisation requires complex
systems and infrastructure that could be very costly to repair or replace in
cases of cyber attacks. This paper surveys deep learning algorithms, IoT cyber
security and risk models, and established mathematical formulas to identify the
best approach for developing a dynamic and self adapting system for predictive
cyber risk analytics supported with Artificial Intelligence and Machine
Learning and real time intelligence in edge computing. The paper presents a new
mathematical approach for integrating concepts for cognition engine design,
edge computing and Artificial Intelligence and Machine Learning to automate
anomaly detection. This engine instigates a step change by applying Artificial
Intelligence and Machine Learning embedded at the edge of IoT networks, to
deliver safe and functional real time intelligence for predictive cyber risk
analytics. This will enhance capacities for risk analytics and assists in the
creation of a comprehensive and systematic understanding of the opportunities
and threats that arise when edge computing nodes are deployed, and when
Artificial Intelligence and Machine Learning technologies are migrated to the
periphery of the internet and into local IoT networks.",http://arxiv.org/abs/2005.12150v2,arXiv
Computer Science,Artificial Intelligence and Machine Learning,Quantitative,Physics Enhanced Artificial Intelligence,"We propose that intelligently combining models from the domains of Artificial
Intelligence or Machine Learning with Physical and Expert models will yield a
more ""trustworthy"" model than any one model from a single domain, given a
complex and narrow enough problem. Based on mean-variance portfolio theory and
bias-variance trade-off analysis, we prove combining models from various
domains produces a model that has lower risk, increasing user trust. We call
such combined models - physics enhanced artificial intelligence (PEAI), and
suggest use cases for PEAI.",http://arxiv.org/abs/1903.04442v1,arXiv
Computer Science,Artificial Intelligence and Machine Learning,Qualitative,"B-SMART: A Reference Architecture for Artificially Intelligent Autonomic
  Smart Buildings","The pervasive application of artificial intelligence and machine learning
algorithms is transforming many industries and aspects of the human experience.
One very important industry trend is the move to convert existing human
dwellings to smart buildings, and to create new smart buildings. Smart
buildings aim to mitigate climate change by reducing energy consumption and
associated carbon emissions. To accomplish this, they leverage artificial
intelligence, big data, and machine learning algorithms to learn and optimize
system performance. These fields of research are currently very rapidly
evolving and advancing, but there has been very little guidance to help
engineers and architects working on smart buildings apply artificial
intelligence algorithms and technologies in a systematic and effective manner.
In this paper we present B-SMART: the first reference architecture for
autonomic smart buildings. B-SMART facilitates the application of artificial
intelligence techniques and technologies to smart buildings by decoupling
conceptually distinct layers of functionality and organizing them into an
autonomic control loop. We also present a case study illustrating how B-SMART
can be applied to accelerate the introduction of artificial intelligence into
an existing smart building.",http://arxiv.org/abs/2211.03219v1,arXiv
Computer Science,Artificial Intelligence and Machine Learning,Qualitative,Seeding the Singularity for A.I,"The singularity refers to an idea that once a machine having an artificial
intelligence surpassing the human intelligence capacity is created, it will
trigger explosive technological and intelligence growth. I propose to test the
hypothesis that machine intelligence capacity can grow autonomously starting
with an intelligence comparable to that of bacteria - microbial intelligence.
The goal will be to demonstrate that rapid growth in intelligence capacity can
be realized at all in artificial computing systems. I propose the following
three properties that may allow an artificial intelligence to exhibit a steady
growth in its intelligence capacity: (i) learning with the ability to modify
itself when exposed to more data, (ii) acquiring new functionalities (skills),
and (iii) expanding or replicating itself. The algorithms must demonstrate a
rapid growth in skills of dataprocessing and analysis and gain qualitatively
different functionalities, at least until the current computing technology
supports their scalable development. The existing algorithms that already
encompass some of these or similar properties, as well as missing abilities
that must yet be implemented, will be reviewed in this work. Future
computational tests could support or oppose the hypothesis that artificial
intelligence can potentially grow to the level of superintelligence which
overcomes the limitations in hardware by producing necessary processing
resources or by changing the physical realization of computation from using
chip circuits to using quantum computing principles.",http://arxiv.org/abs/1908.01766v1,arXiv
Computer Science,Artificial Intelligence and Machine Learning,Qualitative,Towards A Rigorous Science of Interpretable Machine Learning,"As machine learning systems become ubiquitous, there has been a surge of
interest in interpretable machine learning: systems that provide explanation
for their outputs. These explanations are often used to qualitatively assess
other criteria such as safety or non-discrimination. However, despite the
interest in interpretability, there is very little consensus on what
interpretable machine learning is and how it should be measured. In this
position paper, we first define interpretability and describe when
interpretability is needed (and when it is not). Next, we suggest a taxonomy
for rigorous evaluation and expose open questions towards a more rigorous
science of interpretable machine learning.",http://arxiv.org/abs/1702.08608v2,arXiv
Computer Science,Artificial Intelligence and Machine Learning,Qualitative,Probabilistic Artificial Intelligence,"Artificial intelligence commonly refers to the science and engineering of
artificial systems that can carry out tasks generally associated with requiring
aspects of human intelligence, such as playing games, translating languages,
and driving cars. In recent years, there have been exciting advances in
learning-based, data-driven approaches towards AI, and machine learning and
deep learning have enabled computer systems to perceive the world in
unprecedented ways. Reinforcement learning has enabled breakthroughs in complex
games such as Go and challenging robotics tasks such as quadrupedal locomotion.
  A key aspect of intelligence is to not only make predictions, but reason
about the uncertainty in these predictions, and to consider this uncertainty
when making decisions. This is what this manuscript on ""Probabilistic
Artificial Intelligence"" is about. The first part covers probabilistic
approaches to machine learning. We discuss the differentiation between
""epistemic"" uncertainty due to lack of data and ""aleatoric"" uncertainty, which
is irreducible and stems, e.g., from noisy observations and outcomes. We
discuss concrete approaches towards probabilistic inference and modern
approaches to efficient approximate inference.
  The second part of the manuscript is about taking uncertainty into account in
sequential decision tasks. We consider active learning and Bayesian
optimization -- approaches that collect data by proposing experiments that are
informative for reducing the epistemic uncertainty. We then consider
reinforcement learning and modern deep RL approaches that use neural network
function approximation. We close by discussing modern approaches in model-based
RL, which harness epistemic and aleatoric uncertainty to guide exploration,
while also reasoning about safety.",http://arxiv.org/abs/2502.05244v1,arXiv
Computer Science,Artificial Intelligence and Machine Learning,Qualitative,SELM: Software Engineering of Machine Learning Models,"One of the pillars of any machine learning model is its concepts. Using
software engineering, we can engineer these concepts and then develop and
expand them. In this article, we present a SELM framework for Software
Engineering of machine Learning Models. We then evaluate this framework through
a case study. Using the SELM framework, we can improve a machine learning
process efficiency and provide more accuracy in learning with less processing
hardware resources and a smaller training dataset. This issue highlights the
importance of an interdisciplinary approach to machine learning. Therefore, in
this article, we have provided interdisciplinary teams' proposals for machine
learning.",http://arxiv.org/abs/2103.11249v1,arXiv
Computer Science,Artificial Intelligence and Machine Learning,Mixed Methods,"Artificial Intelligence Technology analysis using Artificial
  Intelligence patent through Deep Learning model and vector space model","Thanks to rapid development of artificial intelligence technology in recent
years, the current artificial intelligence technology is contributing to many
part of society. Education, environment, medical care, military, tourism,
economy, politics, etc. are having a very large impact on society as a whole.
For example, in the field of education, there is an artificial intelligence
tutoring system that automatically assigns tutors based on student's level. In
the field of economics, there are quantitative investment methods that
automatically analyze large amounts of data to find investment laws to create
investment models or predict changes in financial markets. As such, artificial
intelligence technology is being used in various fields. So, it is very
important to know exactly what factors have an important influence on each
field of artificial intelligence technology and how the relationship between
each field is connected. Therefore, it is necessary to analyze artificial
intelligence technology in each field. In this paper, we analyze patent
documents related to artificial intelligence technology. We propose a method
for keyword analysis within factors using artificial intelligence patent data
sets for artificial intelligence technology analysis. This is a model that
relies on feature engineering based on deep learning model named KeyBERT, and
using vector space model. A case study of collecting and analyzing artificial
intelligence patent data was conducted to show how the proposed model can be
applied to real world problems.",http://arxiv.org/abs/2111.11295v1,arXiv
Computer Science,Artificial Intelligence and Machine Learning,Mixed Methods,Using Artificial Intelligence to Improve Classroom Learning Experience,"This paper explores advancements in Artificial Intelligence technologies to
enhance classroom learning, highlighting contributions from companies like IBM,
Microsoft, Google, and ChatGPT, as well as the potential of brain signal
analysis. The focus is on improving students learning experiences by using
Machine Learning algorithms to : identify a student preferred learning style
and predict academic dropout risk. A Logistic Regression algorithm is applied
for binary classification using six predictor variables, such as assessment
scores, lesson duration, and preferred learning style, to accurately identify
learning preferences. A case study, with 76,519 candidates and 35 predictor
variables, assesses academic dropout risk using Logistic Regression, achieving
a test accuracy of 87.39%. In comparison, the Stochastic Gradient Descent
classifier achieved an accuracy of 83.1% on the same dataset.",http://arxiv.org/abs/2503.05709v1,arXiv
Computer Science,Artificial Intelligence and Machine Learning,Mixed Methods,"The Top 10 Topics in Machine Learning Revisited: A Quantitative
  Meta-Study","Which topics of machine learning are most commonly addressed in research?
This question was initially answered in 2007 by doing a qualitative survey
among distinguished researchers. In our study, we revisit this question from a
quantitative perspective. Concretely, we collect 54K abstracts of papers
published between 2007 and 2016 in leading machine learning journals and
conferences. We then use machine learning in order to determine the top 10
topics in machine learning. We not only include models, but provide a holistic
view across optimization, data, features, etc. This quantitative approach
allows reducing the bias of surveys. It reveals new and up-to-date insights
into what the 10 most prolific topics in machine learning research are. This
allows researchers to identify popular topics as well as new and rising topics
for their research.",http://arxiv.org/abs/1703.10121v1,arXiv
Computer Science,Artificial Intelligence and Machine Learning,Mixed Methods,A Falsificationist Account of Artificial Neural Networks,"Machine learning operates at the intersection of statistics and computer
science. This raises the question as to its underlying methodology. While much
emphasis has been put on the close link between the process of learning from
data and induction, the falsificationist component of machine learning has
received minor attention. In this paper, we argue that the idea of
falsification is central to the methodology of machine learning. It is commonly
thought that machine learning algorithms infer general prediction rules from
past observations. This is akin to a statistical procedure by which estimates
are obtained from a sample of data. But machine learning algorithms can also be
described as choosing one prediction rule from an entire class of functions. In
particular, the algorithm that determines the weights of an artificial neural
network operates by empirical risk minimization and rejects prediction rules
that lack empirical adequacy. It also exhibits a behavior of implicit
regularization that pushes hypothesis choice toward simpler prediction rules.
We argue that taking both aspects together gives rise to a falsificationist
account of artificial neural networks.",http://arxiv.org/abs/2205.01421v1,arXiv
Computer Science,Artificial Intelligence and Machine Learning,Mixed Methods,Towards Benchmarking Explainable Artificial Intelligence Methods,"The currently dominating artificial intelligence and machine learning
technology, neural networks, builds on inductive statistical learning. Neural
networks of today are information processing systems void of understanding and
reasoning capabilities, consequently, they cannot explain promoted decisions in
a humanly valid form. In this work, we revisit and use fundamental philosophy
of science theories as an analytical lens with the goal of revealing, what can
be expected, and more importantly, not expected, from methods that aim to
explain decisions promoted by a neural network. By conducting a case study we
investigate a selection of explainability method's performance over two mundane
domains, animals and headgear. Through our study, we lay bare that the
usefulness of these methods relies on human domain knowledge and our ability to
understand, generalise and reason. The explainability methods can be useful
when the goal is to gain further insights into a trained neural network's
strengths and weaknesses. If our aim instead is to use these explainability
methods to promote actionable decisions or build trust in ML-models they need
to be less ambiguous than they are today. In this work, we conclude from our
study, that benchmarking explainability methods, is a central quest towards
trustworthy artificial intelligence and machine learning.",http://arxiv.org/abs/2208.12120v1,arXiv
Computer Science,Artificial Intelligence and Machine Learning,Design and Development,"ARTIFICIAL INTELLIGENCE, MACHINE LEARNING AND ROBOTICS: THE CORE OF TOMORROW’S INDUSTRIES","<jats:p>The second issue of AI, ML, and Robotics in Business arrives amid global uncertainty and rapid technological evolution. As industries confront climate instability, labor shortages, and digital transformation, artificial intelligence, machine learning, and robotics are becoming essential engines of resilience and reinvention. This issue highlights sector-specific advances—from AI-driven precision agriculture and ethical frameworks in hospitality to team science and prevention strategies in healthcare. Contributors illustrate how these technologies move beyond automation to elevate human intelligence, ethics, and collaboration. Central themes include the integration of AI into trust-based services, interdisciplinary innovation, and the cultivation of diverse intelligences to navigate the Intelligence Era. NVIDIA’s GTC 2025 conference emphasizes this paradigm shift, revealing AI’s potential to build adaptive, human-augmenting systems across industries. These works signal a new frontier: one where progress lies not in replacing human effort, but in preparing society to lead with vision, empathy, and collective intelligence.</jats:p>",https://doi.org/10.32473/aimlrb.1.2.139237,CrossRef
Computer Science,Artificial Intelligence and Machine Learning,Design and Development,Artificial Intelligence and Deep Learning Applications: A Review,"<jats:p>Deep Learning, a buzz in the artificial intelligence field, is the subset of machine learning. It teaches computers to learn from examples in order to perform a task that is intuitive to humans. It is also known as a deep neural network or deep neural learning. In deep learning, neural networks have a significant role. These are a set of algorithms that we implement to identify relevant relationships in datasets, and they follow the process that imitates the human brain. Neural networks depict the behavior of the human brain and enable computer algorithms to identify trends. It also solves complex problems in the domain of machine learning, AI, and data science.Deep learning deploys artificial neural networks to recognize the hidden patterns of data in the dataset provided. These algorithms are trained over an adequate amount of time and applied to a data set.</jats:p>",https://doi.org/10.55529/jaimlnn.12.10.13,CrossRef
Computer Science,Artificial Intelligence and Machine Learning,Design and Development,Topic: How Artificial Intelligence and Machine Learning Can Impact Market Design,"<jats:p>Background: This research examines how market knowledge and artificial intelligence (AI) interact in different market designs such as business-to-business (B2B) settings while taking emerging technologies and the changing digitalization landscape into account. Objective: The main goal is to understand how AI affects market knowledge in different market designs such as businessto-business (B2B) contexts, taking into account language barriers, practical difficulties, and the revolutionary effects on decision-making and customer interactions. Result: They underscore the transformative potential of artificial intelligence (AI) by highlighting how it shapes market knowledge, encourages customized approaches, and improves marketing efficacy in the business-to-business (B2B) space. Conclusion: In order to create a path for responsible AI integration in B2B marketing, the study concludes with recommendations for standardized terminology related to AI, practical insights into implementation challenges, and ethical issues</jats:p>",https://doi.org/10.33140/aurdp.01.01.03,CrossRef
Computer Science,Artificial Intelligence and Machine Learning,Design and Development,Applications of Artificial Intelligence to Cryptography,"<jats:p>This paper considers some recent advances in the field of Cryptography using Artificial Intelligence (AI) It specifically considers the applications of Machine Learning (ML) and Evolutionary Computing (EC) concepts used to generate ciphers. A short overview is given on Artificial Neural Networks (ANNs) and the principles of Deep Learning (DL) using Deep ANNs.  In this context, the paper considers: (i) the implementation of EC and ANNs to generate unique and unclonable ciphers; (ii) ML strategies for detecting the genuine randomness (or otherwise) of binary streams for applications in Cryptanalysis.  The paper aims to provide an overview on how AI can be applied for encrypting data and undertaking cryptanalysis of such data and other encrypted data classes in order to assess the cryptographic strength of an encryption algorithm. For example, to detect patterns of intercepted data streams that are signatures of encrypted data. An application is presented which includes authentication of high-value documents such as bank notes, using smartphones.  Using an antenna of a smartphone to read (in the near field) an embedded flexible integrate circuit with a non-programmable coprocessor, ultra-strong encrypted information can be used on-line for validation.</jats:p>",https://doi.org/10.14738/tmlai.83.8219,CrossRef
Computer Science,Artificial Intelligence and Machine Learning,Design and Development,The Theory of Natural-Artificial Intelligence,"<jats:p>In recent times, mankind is seeking for certain peculiar solutions to multiple facets containing an identically very fundamental philosophy i.e., certainly intend to have indeterminism as a primordial prerequisite; however, that indeterminism is itself like a void filled with determinism as analogous to the quantum computing as qubits and the corresponding complexity. In the meantime, there are algorithms and mathematical frameworks and those in general; yield the required distinctions in the underlying theories constructed upon principles which then give rise to respective objectifications. But, when it comes to the Artificial Intelligence and Machine Learning, then there find some mathematical gaps in order to connect other regimes in relation of one and the other. The proposed discovery in this paper is about quilting some of those gaps as like the whole structure of Artificial Intelligence is yet to be developed in the realm concerning with responsive analysis in betwixt to humans and machines or beyond to such analogy. Hence, the entire introduction &amp; incitement of this theory is to mathematically determine the deep rationality as responsive manifestation of human brain with a designed computing and both with the highest potential degree of attributions or overlaps and both the conditions will be shown mathematically herewith as identifications that make each other separate and clear to persuade.</jats:p>",https://doi.org/10.24018/ejai.2022.1.1.2,CrossRef
Computer Science,Artificial Intelligence and Machine Learning,Theoretical / Conceptual,Research and Analysis of Machine Learning Algorithm in Artificial Intelligence,"<jats:p>This article firstly explains the concepts of artificial intelligence and algorithm separately, then determines the research status of artificial in-telligence and machine learning in the background of the increasing pop-ularity of artificial intelligence, and finally briefly describes the machine learning algorithm in the field of artificial intelligence, as well as puts for-ward appropriate development prospects, in order to provide theoretical reference for industry insider</jats:p>",https://doi.org/10.30564/aia.v2i2.1801,CrossRef
Computer Science,Artificial Intelligence and Machine Learning,Theoretical / Conceptual,Application of the EDAS Method in Artificial Intelligence A Comprehensive Analysis,"<jats:p>The rapid advancement of artificial intelligence (AI) raises critical questions about its values, ethics, and societal impact. Different philosophical perspectives, including utilitarianism, Kantian ethics, and human agency, guide the debate on how AI systems should operate. AI involves creating intelligent agents that learn and make decisions to achieve specific goals. Its transformative potential spans various industries, influencing productivity, innovation, and even ethical considerations in autonomous systems. This paper explores these complexities, offering insights into AI's evolving landscape.

Research significance:This research is significant as it explores the ethical, philosophical, and practical implications of artificial intelligence (AI), impacting technology, society, and human decision-making. By examining value alignment, human agency, and adaptive learning, the study advances our understanding of how AI can enhance productivity, innovation, and daily life. Additionally, addressing transparency and explain ability in AI systems is crucial for fostering trust, especially in critical domains like healthcare and security, influencing policy and future AI development.

Methology: Alternatives:Building information modelling (BIM), Robotics and automation, Computer vision, AI-Driven Construction Analytics, Cognitive Automation, and Visual AI Systems.

Evaluation Parameters:Complexity, Aesthetics, sufficient budget, Regulatory measures.

Result: The results show that Visual AI Systems received the highest ranking, whereas Computer vision received the lowest ranking.

Conclusion: Visual AI Systems has the highest value for artificial intelligence according to the EDAS approach.</jats:p>",https://doi.org/10.55124/jaim.v1i2.254,CrossRef
Computer Science,Artificial Intelligence and Machine Learning,Theoretical / Conceptual,A Historical Review and Philosophical Examination of the two Paradigms in Artificial Intelligence Research,"<jats:p>Artificial intelligence (AI) is a field that has undergone significant changes and challenges over time. This paper reviews the historical development of AI and representative philosophical thinking, and also considers the methodology and applications of AI, and anticipates its continued advancement. It discusses two main paradigms: symbolism and connectionism, which differ in how they explain and implement intelligence through symbols or artificial neural networks. However, neither paradigm is the final answer to AI research but rather reflects the best answer at a given time. The paper also analyzes the shortcomings of both paradigms from a philosophical perspective and argues that the most fundamental philosophical issue therein is understanding the difference between biological and artificial intelligence.</jats:p>",https://doi.org/10.24018/ejai.2023.2.2.23,CrossRef
Computer Science,Artificial Intelligence and Machine Learning,Theoretical / Conceptual,The democratization of innovation in africa - A perspective driven by artificial intelligence trends,"<jats:p xml:lang=""en"">This paper provides a unique perspective on the democratization of innovation in Africa through Artificial Intelligence (AI) trends. This work avoids a problem statement and methodology, instead providing a holistic view of Africa's evolving innovation landscape, particularly in relation to AI. This unconventional approach serves as a preface to future study in the growing field of AI in Africa. By examining the intersection of AI trends and democratization of innovation, this paper offers insights into the transformative potential of AI technologies in addressing societal challenges, empowering local communities, and driving inclusive growth across the continent. As Africa embraces AI as a catalyst for progress, this abstract sets the stage for further exploration of the implications, opportunities, and challenges that lie ahead in harnessing the power of AI to unlock Africa's vast potential for innovation and development.</jats:p>",https://doi.org/10.26634/jaim.3.1.20923,CrossRef
Computer Science,Artificial Intelligence and Machine Learning,Theoretical / Conceptual,Progression of artificial intelligence/machine learning in geotechnical engineering,"<jats:sec>
<jats:title>Purpose</jats:title>
<jats:p>This short review paper aims to examine the evolution of artificial intelligence (AI)/machine learning (ML) in the realm of geotechnical engineering.</jats:p>
</jats:sec>
<jats:sec>
<jats:title>Design/methodology/approach</jats:title>
<jats:p>This paper gives a brief overview of AI/ML technology and discusses its current trend and future directions in geotechnical engineering.</jats:p>
</jats:sec>
<jats:sec>
<jats:title>Findings</jats:title>
<jats:p>Despite limitations, the use of AI/ML techniques has several significant benefits that make them powerful and practical tools in the field of geotechnical engineering.</jats:p>
</jats:sec>
<jats:sec>
<jats:title>Originality/value</jats:title>
<jats:p>This paper explored the integration of AI/ML promising technology into geotechnical design and practice and highlighted its transformative impacts in terms of benefits, limitations and potential future directions.</jats:p>
</jats:sec>",https://doi.org/10.1108/mlag-10-2024-0010,CrossRef
Computer Science,Computer Systems and Architecture,Quantitative,"Architectural Design Alternatives based on Cloud/Edge/Fog Computing for
  Connected Vehicles","As vehicles playing an increasingly important role in people's daily life,
requirements on safer and more comfortable driving experience have arisen.
Connected vehicles (CVs) can provide enabling technologies to realize these
requirements and have attracted widespread attentions from both academia and
industry. These requirements ask for a well-designed computing architecture to
support the Quality-of-Service (QoS) of CV applications. Computation offloading
techniques, such as cloud, edge, and fog computing, can help CVs process
computation-intensive and large-scale computing tasks. Additionally, different
cloud/edge/fog computing architectures are suitable for supporting different
types of CV applications with highly different QoS requirements, which
demonstrates the importance of the computing architecture design. However, most
of the existing surveys on cloud/edge/fog computing for CVs overlook the
computing architecture design, where they (i) only focus on one specific
computing architecture and (ii) lack discussions on benefits, research
challenges, and system requirements of different architectural alternatives. In
this paper, we provide a comprehensive survey on different architectural design
alternatives based on cloud/edge/fog computing for CVs. The contributions of
this paper are: (i) providing a comprehensive literature survey on existing
proposed architectural design alternatives based on cloud/edge/fog computing
for CVs, (ii) proposing a new classification of computing architectures based
on cloud/edge/fog computing for CVs: computation-aided and computation-enabled
architectures, (iii) presenting a holistic comparison among different
cloud/edge/fog computing architectures for CVs based on functional requirements
of CV systems, including advantages, disadvantages, and research challenges.",http://arxiv.org/abs/2009.12509v1,arXiv
Computer Science,Computer Systems and Architecture,Quantitative,"The failure tolerance of mechatronic software systems to random and
  targeted attacks","This paper describes a complex networks approach to study the failure
tolerance of mechatronic software systems under various types of hardware
and/or software failures. We produce synthetic system architectures based on
evidence of modular and hierarchical modular product architectures and known
motifs for the interconnection of physical components to software. The system
architectures are then subject to various forms of attack. The attacks simulate
failure of critical hardware or software. Four types of attack are
investigated: degree centrality, betweenness centrality, closeness centrality
and random attack. Failure tolerance of the system is measured by a 'robustness
coefficient', a topological 'size' metric of the connectedness of the attacked
network. We find that the betweenness centrality attack results in the most
significant reduction in the robustness coefficient, confirming betweenness
centrality, rather than the number of connections (i.e. degree), as the most
conservative metric of component importance. A counter-intuitive finding is
that ""designed"" system architectures, including a bus, ring, and star
architecture, are not significantly more failure-tolerant than interconnections
with no prescribed architecture, that is, a random architecture. Our research
provides a data-driven approach to engineer the architecture of mechatronic
software systems for failure tolerance.",http://arxiv.org/abs/1310.1050v1,arXiv
Computer Science,Computer Systems and Architecture,Quantitative,Knowledge-aware Evolutionary Graph Neural Architecture Search,"Graph neural architecture search (GNAS) can customize high-performance graph
neural network architectures for specific graph tasks or datasets. However,
existing GNAS methods begin searching for architectures from a zero-knowledge
state, ignoring the prior knowledge that may improve the search efficiency. The
available knowledge base (e.g. NAS-Bench-Graph) contains many rich
architectures and their multiple performance metrics, such as the accuracy
(#Acc) and number of parameters (#Params). This study proposes exploiting such
prior knowledge to accelerate the multi-objective evolutionary search on a new
graph dataset, named knowledge-aware evolutionary GNAS (KEGNAS). KEGNAS employs
the knowledge base to train a knowledge model and a deep multi-output Gaussian
process (DMOGP) in one go, which generates and evaluates transfer architectures
in only a few GPU seconds. The knowledge model first establishes a
dataset-to-architecture mapping, which can quickly generate candidate transfer
architectures for a new dataset. Subsequently, the DMOGP with architecture and
dataset encodings is designed to predict multiple performance metrics for
candidate transfer architectures on the new dataset. According to the predicted
metrics, non-dominated candidate transfer architectures are selected to
warm-start the multi-objective evolutionary algorithm for optimizing the #Acc
and #Params on a new dataset. Empirical studies on NAS-Bench-Graph and five
real-world datasets show that KEGNAS swiftly generates top-performance
architectures, achieving 4.27% higher accuracy than advanced evolutionary
baselines and 11.54% higher accuracy than advanced differentiable baselines. In
addition, ablation studies demonstrate that the use of prior knowledge
significantly improves the search performance.",http://arxiv.org/abs/2411.17339v1,arXiv
Computer Science,Computer Systems and Architecture,Quantitative,SoK: Microservice Architectures from a Dependability Perspective,"The microservice software architecture leverages the idea of splitting large
monolithic applications into multiple smaller services that interact using
lightweight communication schemes. While the microservice architecture has
proven its ability to support modern business applications, it also introduces
new possible weak points in a system. Some scientific literature surveys have
already addressed fault tolerance or security concerns but most of them lack
analysis on the fault and vulnerability coverage that is introduced by
microservice architectures. We explore the known faults and vulnerabilities
that microservice architecture might suffer from, and the recent scientific
literature that addresses them. We emphasize runtime detection and recovery
mechanisms instead of offline prevention and mitigation mechanisms to limit the
scope of this document.",http://arxiv.org/abs/2503.03392v1,arXiv
Computer Science,Computer Systems and Architecture,Quantitative,"Architectures of Meaning, A Systematic Corpus Analysis of NLP Systems","This paper proposes a novel statistical corpus analysis framework targeted
towards the interpretation of Natural Language Processing (NLP) architectural
patterns at scale. The proposed approach combines saturation-based lexicon
construction, statistical corpus analysis methods and graph collocations to
induce a synthesis representation of NLP architectural patterns from corpora.
The framework is validated in the full corpus of Semeval tasks and demonstrated
coherent architectural patterns which can be used to answer architectural
questions on a data-driven fashion, providing a systematic mechanism to
interpret a largely dynamic and exponentially growing field.",http://arxiv.org/abs/2107.08124v1,arXiv
Computer Science,Computer Systems and Architecture,Qualitative,Architecture implications of pads as a scarce resource,"<jats:p>Due to non-ideal technology scaling, delivering a stable supply voltage is increasingly challenging. Furthermore, com- petition for limited chip interface resources (i.e., C4 pads) between power supply and I/O, and the loss of such resources to electromigration, means that constructing a power deliverynetwork (PDN) that satisfies noise margins without compromising performance is and will remain a critical problem for architects and circuit designers alike. Simple guardbanding will no longer work, as the consequent performance penalty will grow with technology scaling</jats:p>
          <jats:p>In this paper, we develop a pre-RTL PDN model, VoltSpot, for the purpose of studying the performance and noise tradeoffs among power supply and I/O pad allocation, the effectiveness of noise mitigation techniques, and the consequent implications of electromigration-induced PDN pad failure. Our simulations demonstrate that, despite their integral role in the PDN, power/ground pads can be aggressively reduced (by conversion into I/O pads) to their electromigration limit with minimal performance impact from extra voltage noise - provided the system implements a suitable noise-mitigation strategy. The key observation is that even though reducing power/ground pads significantly increases the number of voltage emergencies, the average noise amplitude increase is small. Overall, we can triple I/O bandwidth while maintaining target lifetimes and incurring only 1.5% slowdown</jats:p>",https://doi.org/10.1145/2678373.2665728,CrossRef
Computer Science,Computer Systems and Architecture,Qualitative,"Towards Reference Architectures for Trustworthy Collaborative
  Cyber-Physical Systems: Reference Architectures as Boundary Objects","This paper presents our work-in-progress study on reference architectures as
boundary objects for realizing trustworthy collaborative Cyber-Physical Systems
(CPS). Furthermore, the preliminary results from interviews with systems
engineering experts from industry and academia are also discussed. The
interview results reveal challenges in using reference architectures during the
system development process. Furthermore, exactly which trustworthiness
attributes (security, availability, reliability, etc.) should be addressed to
realize trustworthy collaborative CPS is identified as an open question, which
we will address in our future work.",http://arxiv.org/abs/2108.12771v1,arXiv
Computer Science,Computer Systems and Architecture,Qualitative,Conceptual Modeling for Computer Organization and Architecture,"Understanding computer system hardware, including how computers operate, is
essential for undergraduate students in computer engineering and science.
Literature shows students learning computer organization and assembly language
often find fundamental concepts difficult to comprehend within the topic
materials. Tools have been introduced to improve students comprehension of the
interaction between computer architecture, assembly language, and the operating
system. One such tool is the Little Man Computer (LMC) model that operates in a
way similar to a computer but that is easier to understand. Even though LMC
does not have modern CPUs with multiple cores nor executes multiple
instructions, it nevertheless shows the basic principles of the von Neumann
architecture. LMC aims to introduce students to such concepts as code and
instruction sets. In this paper, LMC is used for an additional purpose: a tool
with which to experiment using a new modeling language (i.e., a thinging
machine; TM) in the area of computer organization and architecture without
involving complexity in the subject. That is, the simplicity of LMC facilitates
the application of TM without going deep into computer
organization/architecture materials. Accordingly, the paper (a) provides a new
way for using the LMC model for whatever purpose (e.g., education) and (b)
demonstrates that TM can be used to build an abstract level of description in
the organization/architect field. The resultant schematics from the TM model of
LMC offer an initial case study that supports our thesis that TM is a viable
method for hardware/software-independent descriptions in the computer
organization and architect field of study.",http://arxiv.org/abs/2103.01773v1,arXiv
Computer Science,Computer Systems and Architecture,Qualitative,A System Architecture for Software-Defined Industrial Internet of Things,"Wireless sensor networks have been a driving force of the Industrial Internet
of Things (IIoT) advancement in the process control and manufacturing industry.
The emergence of IIoT opens great potential for the ubiquitous field device
connectivity and manageability with an integrated and standardized architecture
from low-level device operations to high-level data-centric application
interactions. This technological development requires software definability in
the key architectural elements of IIoT, including wireless field devices, IIoT
gateways, network infrastructure, and IIoT sensor cloud services. In this
paper, a novel software-defined IIoT (SD-IIoT) is proposed in order to solve
essential challenges in a holistic IIoT system, such as reliability, security,
timeliness scalability, and quality of service (QoS). A new IIoT system
architecture is proposed based on the latest networking technologies such as
WirelessHART, WebSocket, IETF constrained application protocol (CoAP) and
software-defined networking (SDN). A new scheme based on CoAP and SDN is
proposed to solve the QoS issues. Computer experiments in a case study are
implemented to show the effectiveness of the proposed system architecture.",http://arxiv.org/abs/1507.08810v1,arXiv
Computer Science,Computer Systems and Architecture,Qualitative,"Adapting the Function Approximation Architecture in Online Reinforcement
  Learning","The performance of a reinforcement learning (RL) system depends on the
computational architecture used to approximate a value function. Deep learning
methods provide both optimization techniques and architectures for
approximating nonlinear functions from noisy, high-dimensional observations.
However, prevailing optimization techniques are not designed for
strictly-incremental online updates. Nor are standard architectures designed
for observations with an a priori unknown structure: for example, light sensors
randomly dispersed in space. This paper proposes an online RL prediction
algorithm with an adaptive architecture that efficiently finds useful nonlinear
features. The algorithm is evaluated in a spatial domain with high-dimensional,
stochastic observations. The algorithm outperforms non-adaptive baseline
architectures and approaches the performance of an architecture given
side-channel information. These results are a step towards scalable RL
algorithms for more general problems, where the observation structure is not
available.",http://arxiv.org/abs/2106.09776v1,arXiv
Computer Science,Computer Systems and Architecture,Mixed Methods,Architecture of multi-agent systems for generative automatic matching among heterogeneous systems,"<jats:p>This paper presents the generative automatic matching (GAM) approach, implemented through a multi-agent system (MAS), to address the challenges of heterogeneity across meta-models. GAM integrates automatic meta-model matching with model generation, offering a comprehensive solution to complex systems involving diverse architectures. The key innovation lies in its ability to automate both the detection of correspondences and the transformation of models, improving the precision and recall of matching processes. The system's scalability and adaptability are enhanced by MAS, allowing for efficient management of diverse meta-models. The approach was evaluated through relational to big data UML meta-models (RBDU) case study. The results demonstrated high accuracy, with precision and recall metrics approaching 1, underscoring the robustness of GAM in managing heterogeneous systems. Compared to traditional methods, GAM offers significant advantages, including automated matching and generation, adaptability to various domains, and superior performance metrics. The study contributes to the field of model-driven engineering (MDE) by formalizing a method that effectively bridges the gap between heterogeneous meta-models. Future research will focus on refining matching heuristics, expanding case studies.</jats:p>",https://doi.org/10.11591/ijece.v15i2.pp2345-2355,CrossRef
Computer Science,Computer Systems and Architecture,Mixed Methods,Computer architecture courses in electrical engineering departments,"<jats:p>This paper traces the history of computer architecture courses in electrical engineering departments. Previously unpublished data from the Fall 1972 COSINE survey are given to show current computer architecture course offerings and texts. Computer architecture courses offered in 1972-73 are analyzed, compared with ACM and COSINE recommendations, and classified into five categories: introductory computer engineering courses with a computer architecture flavor, software-oriented computer organization courses, hardware-oriented computer organization courses, case study courses, and topical seminars. Future trends in computer architecture education are predicted.</jats:p>",https://doi.org/10.1145/633642.803984,CrossRef
Computer Science,Computer Systems and Architecture,Mixed Methods,Omega: An Architecture for AI Unification,"We introduce the open-ended, modular, self-improving Omega AI unification
architecture which is a refinement of Solomonoff's Alpha architecture, as
considered from first principles. The architecture embodies several crucial
principles of general intelligence including diversity of representations,
diversity of data types, integrated memory, modularity, and higher-order
cognition. We retain the basic design of a fundamental algorithmic substrate
called an ""AI kernel"" for problem solving and basic cognitive functions like
memory, and a larger, modular architecture that re-uses the kernel in many
ways. Omega includes eight representation languages and six classes of neural
networks, which are briefly introduced. The architecture is intended to
initially address data science automation, hence it includes many problem
solving methods for statistical tasks. We review the broad software
architecture, higher-order cognition, self-improvement, modular neural
architectures, intelligent agents, the process and memory hierarchy, hardware
abstraction, peer-to-peer computing, and data abstraction facility.",http://arxiv.org/abs/1805.12069v1,arXiv
Computer Science,Computer Systems and Architecture,Mixed Methods,Centralization potential of automotive E/E architectures,"Current automotive E/E architectures are subject to significant
transformations: Computing-power-intensive advanced driver-assistance systems,
bandwidth-hungry infotainment systems, the connection of the vehicle with the
internet and the consequential need for cyber-security drives the
centralization of E/E architectures. A centralized architecture is often seen
as a key enabler to master those challenges. Available research focuses mostly
on the different types of E/E architectures and contrasts their advantages and
disadvantages. There is a research gap on guidelines for system designers and
function developers to analyze the potential of their systems for
centralization. The present paper aims to quantify centralization potential
reviewing relevant literature and conducting qualitative interviews with
industry practitioners. In literature, we identified seven key automotive
system properties reaching limitations in current automotive architectures:
busload, functional safety, computing power, feature dependencies, development
and maintenance costs, error rate, modularity and flexibility. These properties
serve as quantitative evaluation criteria to estimate whether centralization
would enhance overall system performance. In the interviews, we have validated
centralization and its fundament - the conceptual systems engineering - as
capabilities to mitigate these limitations. By focusing on practical insights
and lessons learned, this research provides system designers with actionable
guidance to optimize their systems, addressing the outlined challenges while
avoiding monolithic architecture. This paper bridges the gap between
theoretical research and practical application, offering valuable takeaways for
practitioners.",http://arxiv.org/abs/2409.10690v1,arXiv
Computer Science,Computer Systems and Architecture,Mixed Methods,Computer Analysis of Architecture Using Automatic Image Understanding,"In the past few years, computer vision and pattern recognition systems have
been becoming increasingly more powerful, expanding the range of automatic
tasks enabled by machine vision. Here we show that computer analysis of
building images can perform quantitative analysis of architecture, and quantify
similarities between city architectural styles in a quantitative fashion.
Images of buildings from 18 cities and three countries were acquired using
Google StreetView, and were used to train a machine vision system to
automatically identify the location of the imaged building based on the image
visual content. Experimental results show that the automatic computer analysis
can automatically identify the geographical location of the StreetView image.
More importantly, the algorithm was able to group the cities and countries and
provide a phylogeny of the similarities between architectural styles as
captured by StreetView images. These results demonstrate that computer vision
and pattern recognition algorithms can perform the complex cognitive task of
analyzing images of buildings, and can be used to measure and quantify visual
similarities and differences between different styles of architectures. This
experiment provides a new paradigm for studying architecture, based on a
quantitative approach that can enhance the traditional manual observation and
analysis. The source code used for the analysis is open and publicly available.",http://arxiv.org/abs/1807.04892v4,arXiv
Computer Science,Computer Systems and Architecture,Design and Development,Applying Slicing Technique to Software Architectures,"Software architecture is receiving increasingly attention as a critical
design level for software systems. As software architecture design resources
(in the form of architectural specifications) are going to be accumulated, the
development of techniques and tools to support architectural understanding,
testing, reengineering, maintenance, and reuse will become an important issue.
This paper introduces a new form of slicing, named architectural slicing, to
aid architectural understanding and reuse. In contrast to traditional slicing,
architectural slicing is designed to operate on the architectural specification
of a software system, rather than the source code of a program. Architectural
slicing provides knowledge about the high-level structure of a software system,
rather than the low-level implementation details of a program. In order to
compute an architectural slice, we present the architecture information flow
graph which can be used to represent information flows in a software
architecture. Based on the graph, we give a two-phase algorithm to compute an
architectural slice.",http://arxiv.org/abs/cs/0105008v1,arXiv
Computer Science,Computer Systems and Architecture,Design and Development,Decision Support Systems Architectures,"This paper presents the main components of the decision assisting systems.
Further on three types of architectures of these systems are described,
analyzed, and respectively compared, namely: the network architecture, the
centralized architecture and the hierarchical architecture.",http://arxiv.org/abs/0906.0863v1,arXiv
Computer Science,Computer Systems and Architecture,Design and Development,"Reliability of fault-tolerant system architectures for automated driving
  systems","Automated driving functions at high levels of autonomy operate without driver
supervision. The system itself must provide suitable responses in case of
hardware element failures. This requires fault-tolerant approaches using domain
ECUs and multicore processors operating in lockstep mode. The selection of a
suitable architecture for fault-tolerant vehicle systems is currently
challenging. Lockstep CPUs enable the implementation of majority redundancy or
M-out-of-N ($M$oo$N$) architectures. In addition to structural redundancy,
diversity redundancy in the ECU architecture is also relevant to fault
tolerance. Two fault-tolerant ECU architecture groups exist: architectures with
one ECU (system on a chip) and architectures consisting of multiple
communicating ECUs. The single-ECU systems achieve higher reliability, whereas
the multi-ECU systems are more robust against dependent failures, such as
common-cause or cascading failures, due to their increased potential for
diversity redundancy. Yet, it remains not fully understood how different types
of architectures influence the system reliability. The work aims to design
architectures with respect to CPU and sensor number, $M$oo$N$ expression, and
hardware element reliability. The results enable a direct comparison of
different architecture types. We calculate their reliability and quantify the
effort to achieve high safety requirements. Markov processes allow comparing
sensor and CPU architectures by varying the number of components and failure
rates. The objective is to evaluate systems' survival probability and fault
tolerance and design suitable sensor-CPU architectures. The results show that
the system architecture strongly influences the reliability. However, a
suitable system architecture must have a trade-off between reliability and
self-diagnostics that parallel systems without majority redundancies do not
provide.",http://arxiv.org/abs/2210.04040v1,arXiv
Computer Science,Computer Systems and Architecture,Design and Development,"Functional Augmented State Transfer (FAST) Architecture for
  Computationally Intensive Network Applications","We describe a novel architecture that combines the simplicity of RESTful
architecture with the power of functional programming for delivering
web-services. Although, RESTful architecture has been quite useful in
simplifying the development of scalable systems, it is not suited for all types
of network applications. Our architecture improves upon the RESTful
architecture to provide scalable framework for computationally intensive
network applications. The proposed architecture is ideal for applications that
involve data management and data analysis/calculations on data. Data analytics
and financial calculations are two areas where the architecture can be applied
efficiently.",http://arxiv.org/abs/1607.05075v1,arXiv
Computer Science,Computer Systems and Architecture,Design and Development,A Survey of Machine Learning for Computer Architecture and Systems,"It has been a long time that computer architecture and systems are optimized
for efficient execution of machine learning (ML) models. Now, it is time to
reconsider the relationship between ML and systems, and let ML transform the
way that computer architecture and systems are designed. This embraces a
twofold meaning: improvement of designers' productivity, and completion of the
virtuous cycle. In this paper, we present a comprehensive review of the work
that applies ML for computer architecture and system design. First, we perform
a high-level taxonomy by considering the typical role that ML techniques take
in architecture/system design, i.e., either for fast predictive modeling or as
the design methodology. Then, we summarize the common problems in computer
architecture/system design that can be solved by ML techniques, and the typical
ML techniques employed to resolve each of them. In addition to emphasis on
computer architecture in a narrow sense, we adopt the concept that data centers
can be recognized as warehouse-scale computers; sketchy discussions are
provided in adjacent computer systems, such as code generation and compiler; we
also give attention to how ML techniques can aid and transform design
automation. We further provide a future vision of opportunities and potential
directions, and envision that applying ML for computer architecture and systems
would thrive in the community.",http://arxiv.org/abs/2102.07952v2,arXiv
Computer Science,Computer Systems and Architecture,Theoretical / Conceptual,Banyan networks for partitioning multiprocessor systems,"<jats:p>This paper describes a class of partitioning networks, called banyans, whose cost function grows more slowly than that of the crossbar and whose fan-out requirements are independent of network size. Such networks can economically partition the resources of large modular systems into a wide variety of subsystems. Any possible partition can be realized by paralleling several networks or by multiplexing a single network in a manner to be described later. Results will be given indicating that a cost/performance advantage over the crossbar can be obtained for large systems and that the crossbar can, in fact, be considered a non-optimal special case of a banyan network. Inherent fail-soft capability and the existence of rapid control algorithms which can be largely performed by distributed logic within the network are also important attributes of banyans.</jats:p>
          <jats:p>This paper presents fundamental properties and preliminary simulation results of banyan partitioning networks. A more detailed treatment, including proofs of theoretical properties, is reserved for reference (5).</jats:p>",https://doi.org/10.1145/633642.803967,CrossRef
Computer Science,Computer Systems and Architecture,Theoretical / Conceptual,Modeling Network Architecture: A Cloud Case Study,"The Internet s ability to support a wide range of services depends on the
network architecture and theoretical and practical innovations necessary for
future networks. Network architecture in this context refers to the structure
of a computer network system as well as interactions among its physical
components, their configuration, and communication protocols. Various
descriptions of architecture have been developed over the years with an
unusually large number of superficial icons and symbols. This situation has
created a need for more coherent systematic representations of network
architecture. This paper is intended to refine the design, analysis, and
documentation of network architecture by adopting a conceptual model called a
thinging (abstract) machine (TM), which views all components of a network in
terms of a single notion: the flow of things in a TM. Since cloud computing has
become increasingly popular in the last few years as a model for a shared pool
of networks, servers, storage, and applications, we apply the TM to model a
real case study of cloud networks. The resultant model introduces an integrated
representation of computer networks.",http://arxiv.org/abs/2004.10350v1,arXiv
Computer Science,Computer Systems and Architecture,Theoretical / Conceptual,Assessing Random Dynamical Network Architectures for Nanoelectronics,"Independent of the technology, it is generally expected that future nanoscale
devices will be built from vast numbers of densely arranged devices that
exhibit high failure rates. Other than that, there is little consensus on what
type of technology and computing architecture holds most promises to go far
beyond today's top-down engineered silicon devices. Cellular automata (CA) have
been proposed in the past as a possible class of architectures to the von
Neumann computing architecture, which is not generally well suited for future
parallel and fine-grained nanoscale electronics. While the top-down engineered
semi-conducting technology favors regular and locally interconnected
structures, future bottom-up self-assembled devices tend to have irregular
structures because of the current lack precise control over these processes. In
this paper, we will assess random dynamical networks, namely Random Boolean
Networks (RBNs) and Random Threshold Networks (RTNs), as alternative computing
architectures and models for future information processing devices. We will
illustrate that--from a theoretical perspective--they offer superior properties
over classical CA-based architectures, such as inherent robustness as the
system scales up, more efficient information processing capabilities, and
manufacturing benefits for bottom-up designed devices, which motivates this
investigation. We will present recent results on the dynamic behavior and
robustness of such random dynamical networks while also including manufacturing
issues in the assessment.",http://arxiv.org/abs/0805.2684v1,arXiv
Computer Science,Human-Computer Interaction,Quantitative,The Sensorium: A Multimodal Neurofeedback Environment,"<jats:p>The Sensorium is a neurofeedback environment that allows people to experience signals from their nonperceptible body processes visually and auditorily. Various (neuro-)physiological rhythms and frequencies are projected simultaneously as soundscapes and “lightscapes” into the environment. A wireless physiological amplifier device sends signals such as EEG and ECG to a computer for real-time processing using the modified brain-computer interface software “Thought Translation Device” (TTD). The TTD performs signal filtering, parametric orchestral sonification, and light control. In a pilot study, 20 participants have been exposed to their ongoing brain and heart signals while sitting inside the Sensorium, a small room equipped with a speaker and lighting system. Almost all of them reported an increase in contentment, relaxation, happiness, and inner harmony. They also reported a widening in their body consciousness. In future, therapeutic paradigms will be developed and the treatment effects on people with psychosomatic diseases will be evaluated.</jats:p>",https://doi.org/10.1155/2011/724204,CrossRef
Computer Science,Human-Computer Interaction,Quantitative,Wide Bezel Televisions Decrease Immersive Experiences,"<jats:p>This study explored how telepresence could be affected by stimuli from reality that distracts people while they are watching television. The sample comprised of 36 undergraduate and graduate students from a university in South Korea (age range: 18–38 years, <jats:italic>M</jats:italic> = 22.61, and SD = 4.12). A between-subjects experimental design was employed with two types of viewing equipment (a television screen vs. a television screen with side screens that act as stimuli from reality) and two bezel widths (2 cm vs. 10 cm) to examine how each condition influenced the viewers’ perceived telepresence. The results revealed that participants’ perception of telepresence was not affected by the type of viewing equipment. However, the level of telepresence was affected by the bezel width: the thinner the bezel, the more telepresence felt by the viewers. These findings provide important insights that can guide the future designs of screen bezels for televisions and other devices in order to more effectively create immersive virtual worlds. Future studies are needed to examine the relationship between central vision and telepresence.</jats:p>",https://doi.org/10.1155/2020/9349560,CrossRef
Computer Science,Human-Computer Interaction,Quantitative,"Human Computer Interaction, Cognitive Cybernetic &amp; Captological Education","<jats:p>This paper was inspired by the topics by Marshal McLuhan about cibernetisation media understanding, associated with new findings in intelligent systems that lead towards technological anthropomorphisation, and Larsen's model of cognitive controller mood. The results of research conducted from 1990 to the present day outlining the issues associated with captology are presented, and their transfer to specific areas in education outlined. The objective of the theory is to comprehend, interpret and describe the appearance of various disciplines in the natural and social sciences relating to cognitive cybernetics and Human Computer Interaction. In accordance with the unique principles, multidisciplinarity is replaced by pluriperspectivity, and an approach to integrating research methods with engineering design. The theory answers questions using cognitive cybernetics and its recognition and transformation of Descartes's saying: ""cogito ergo sum"", (I think, therefore I am). Work presents the relationship and correlation between man and technology as Human computer interaction with technological definitions Intelligent Systems and Captology. Special attention is focused on today's modern education with the use of virtual media and the cultural matrix within which the particular media is active. For intelligent educations systems to become more useful and acceptable, we need to consider the “system” as a synergistic composition of software behaviors, and the human interacting.  Human interaction must be dominant and having considered the ruling. This cannot be achieved with today's captological educational media. Captological educational media stifles people, casts their most important, (social), role in education and makes them unhappy? Human Computer Interaction, as a strategy and philosophy, is the future of education!</jats:p>",https://doi.org/10.18063/phci.v1i2.759,CrossRef
Computer Science,Human-Computer Interaction,Quantitative,Pointing Devices for Wearable Computers,"<jats:p>We present a survey of pointing devices for wearable computers, which are body-mounted devices that users can access at any time. Since traditional pointing devices (i.e., mouse, touchpad, and trackpoint) were designed to be used on a steady and flat surface they are inappropriate for wearable computers. Just as the advent of laptops resulted in the development of the touchpad and trackpoint, the emergence of wearable computers is leading to the development of pointing devices designed for them. However, unlike laptops, since wearable computers are operated from different body positions under different environmental conditions for different uses, researchers have developed a variety of innovative pointing devices for wearable computers characterized by their sensing mechanism, control mechanism, and form factor. We survey a representative set of pointing devices for wearable computers using an “adaptation of traditional devices” versus “new devices” dichotomy and study devices according to their control and sensing mechanisms and form factor. The objective of this paper is to showcase a variety of pointing devices developed for wearable computers and bring structure to the design space for wearable pointing devices. We conclude that a de facto pointing device for wearable computers, unlike laptops, is not likely to emerge.</jats:p>",https://doi.org/10.1155/2014/527320,CrossRef
Computer Science,Human-Computer Interaction,Quantitative,An Intelligent Framework for Website Usability,"<jats:p>With the major advances of the Internet throughout the past couple of years, websites have come to play a central role in the modern marketing business program. However, simply owning a website is not enough for a business to prosper on the Web. Indeed, it is the level of usability of a website that determines if a user stays or abandons it for another competing one. It is therefore crucial to understand the importance of usability on the web, and consequently the need for its evaluation. Nonetheless, there exist a number of obstacles preventing software organizations from successfully applying sound website usability evaluation strategies in practice. From this point of view automation of the latter is extremely beneficial, which not only assists designers in creating more usable websites, but also enhances the Internet users’ experience on the Web and increases their level of satisfaction. As a means of addressing this problem, an Intelligent Usability Evaluation (IUE) tool is proposed that automates the usability evaluation process by employing a Heuristic Evaluation technique in an intelligent manner through the adoption of several research-based AI methods. Experimental results show there exists a high correlation between the tool and human annotators when identifying the considered usability violations.</jats:p>",https://doi.org/10.1155/2014/479286,CrossRef
Computer Science,Human-Computer Interaction,Qualitative,A Common Framework for Audience Interactivity,"Audience interactivity is interpreted differently across domains. This
research develops a framework to describe audience interactivity across a broad
range of experiences. We build on early work characterizing child audience
interactivity experiences, expanding on these findings with an extensive review
of literature in theater, games, and theme parks, paired with expert interviews
in those domains. The framework scaffolds interactivity as nested spheres of
audience influence, and comprises a series of dimensions of audience
interactivity including a Spectrum of Audience Interactivity. This framework
aims to develop a common taxonomy for researchers and practitioners working
with audience interactivity experiences.",http://arxiv.org/abs/1710.03320v2,arXiv
Computer Science,Human-Computer Interaction,Qualitative,Probabilistic Multimodal Modeling for Human-Robot Interaction Tasks,"Human-robot interaction benefits greatly from multimodal sensor inputs as
they enable increased robustness and generalization accuracy. Despite this
observation, few HRI methods are capable of efficiently performing inference
for multimodal systems. In this work, we introduce a reformulation of
Interaction Primitives which allows for learning from demonstration of
interaction tasks, while also gracefully handling nonlinearities inherent to
multimodal inference in such scenarios. We also empirically show that our
method results in more accurate, more robust, and faster inference than
standard Interaction Primitives and other common methods in challenging HRI
scenarios.",http://arxiv.org/abs/1908.04955v1,arXiv
Computer Science,Human-Computer Interaction,Qualitative,Understanding Mental Models of AI through Player-AI Interaction,"Designing human-centered AI-driven applications require deep understandings
of how people develop mental models of AI. Currently, we have little knowledge
of this process and limited tools to study it. This paper presents the position
that AI-based games, particularly the player-AI interaction component, offer an
ideal domain to study the process in which mental models evolve. We present a
case study to illustrate the benefits of our approach for explainable AI.",http://arxiv.org/abs/2103.16168v1,arXiv
Computer Science,Human-Computer Interaction,Qualitative,"Clo(o)k: Human-Time Interactions Through a Clock That ""Looks""","What if a clock could do more than tell time - what if it could look around?
This project explores the conceptualization, design, and construction of a
timepiece with visual perception capabilities, featuring three types of
human-time interactions. Informal observations during a demonstration highlight
its unique user experiences. https://www.zhuoyuelyu.com/clook",http://arxiv.org/abs/2303.14557v2,arXiv
Computer Science,Human-Computer Interaction,Qualitative,"Interaction-Required Suggestions for Control, Ownership, and Awareness
  in Human-AI Co-Writing","This paper explores interaction designs for generative AI interfaces that
necessitate human involvement throughout the generation process. We argue that
such interfaces can promote cognitive engagement, agency, and thoughtful
decision-making. Through a case study in text revision, we present and analyze
two interaction techniques: (1) using a predictive-text interaction to type the
assistant's response to a revision request, and (2) highlighting potential edit
opportunities in a document. Our implementations demonstrate how these
approaches reveal the landscape of writing possibilities and enable
fine-grained control. We discuss implications for human-AI writing partnerships
and future interaction design directions.",http://arxiv.org/abs/2504.08726v1,arXiv
Computer Science,Human-Computer Interaction,Mixed Methods,Visual Enhancement for Sports Entertainment by Vision-Based Augmented Reality,"<jats:p>This paper presents visually enhanced sports entertainment applications: AR Baseball Presentation System and Interactive AR Bowling System. We utilize vision-based augmented reality for getting immersive feeling. First application is an observation system of a virtual baseball game on the tabletop. 3D virtual players are playing a game on a real baseball field model, so that users can observe the game from favorite view points through a handheld monitor with a web camera. Second application is a bowling system which allows users to roll a real ball down a real bowling lane model on the tabletop and knock down virtual pins. The users watch the virtual pins through the monitor. The lane and the ball are also tracked by vision-based tracking. In those applications, we utilize multiple 2D markers distributed at arbitrary positions and directions. Even though the geometrical relationship among the markers is unknown, we can track the camera in very wide area.</jats:p>",https://doi.org/10.1155/2008/145363,CrossRef
Computer Science,Human-Computer Interaction,Mixed Methods,DeepSI: Interactive Deep Learning for Semantic Interaction,"In this paper, we design novel interactive deep learning methods to improve
semantic interactions in visual analytics applications. The ability of semantic
interaction to infer analysts' precise intents during sensemaking is dependent
on the quality of the underlying data representation. We propose the
$\text{DeepSI}_{\text{finetune}}$ framework that integrates deep learning into
the human-in-the-loop interactive sensemaking pipeline, with two important
properties. First, deep learning extracts meaningful representations from raw
data, which improves semantic interaction inference. Second, semantic
interactions are exploited to fine-tune the deep learning representations,
which then further improves semantic interaction inference. This feedback loop
between human interaction and deep learning enables efficient learning of user-
and task-specific representations. To evaluate the advantage of embedding the
deep learning within the semantic interaction loop, we compare
$\text{DeepSI}_{\text{finetune}}$ against a state-of-the-art but more basic use
of deep learning as only a feature extractor pre-processed outside of the
interactive loop. Results of two complementary studies, a human-centered
qualitative case study and an algorithm-centered simulation-based quantitative
experiment, show that $\text{DeepSI}_{\text{finetune}}$ more accurately
captures users' complex mental models with fewer interactions.",http://arxiv.org/abs/2305.18357v1,arXiv
Computer Science,Human-Computer Interaction,Mixed Methods,"Towards Effective Human-AI Collaboration in GUI-Based Interactive Task
  Learning Agents","We argue that a key challenge in enabling usable and useful interactive task
learning for intelligent agents is to facilitate effective Human-AI
collaboration. We reflect on our past 5 years of efforts on designing,
developing and studying the SUGILITE system, discuss the issues on
incorporating recent advances in AI with HCI principles in mixed-initiative
interactions and multi-modal interactions, and summarize the lessons we
learned. Lastly, we identify several challenges and opportunities, and describe
our ongoing work",http://arxiv.org/abs/2003.02622v1,arXiv
Computer Science,Human-Computer Interaction,Mixed Methods,"Contextualizing Large-Scale Domain Knowledge for Conceptual Modeling and
  Simulation","We present an interactive modeling tool, VERA, that scaffolds the acquisition
of domain knowledge involved in conceptual modeling and agent-based
simulations. We describe the knowledge engineering process of contextualizing
large-scale domain knowledge. Specifically, we use the ontology of biotic
interactions in Global Biotic Interactions, and the trait data of species in
Encyclopedia of Life to facilitate the model construction. Learners can use
VERA to construct qualitative conceptual models of ecological phenomena, run
them as quantitative simulations, and review their predictions.",http://arxiv.org/abs/2209.02579v1,arXiv
Computer Science,Human-Computer Interaction,Mixed Methods,VizWiz Dataset Browser: A Tool for Visualizing Machine Learning Datasets,"We present a visualization tool to exhaustively search and browse through a
set of large-scale machine learning datasets. Built on the top of the VizWiz
dataset, our dataset browser tool has the potential to support and enable a
variety of qualitative and quantitative research, and open new directions for
visualizing and researching with multimodal information. The tool is publicly
available at https://vizwiz.org/browse.",http://arxiv.org/abs/1912.09336v1,arXiv
Computer Science,Human-Computer Interaction,Design and Development,A Functional Driver Analyzing Concept,"<jats:p>It is evident that a lot of accidents occur because of drowsiness or inattentiveness of the driver. The logical consequence is that we have to find methods to better analyze the driver. A lot of research has been spent on camera-based systems which focus on the driver's eye gaze or his head movement. But there are few systems that provide camera-free driver analyzing. This is the main goal of the work presented here which is structured in three phases, with the operational goal of having a working driver analyzer implemented in a car. The main question is: is it possible to make statements concerning the driver and his state by using vehicle data from the CAN Bus only? This paper describes the current state of driver analyzing, our overall system architecture, as well as future work. At the moment, we focus on detecting the driving style of a person.</jats:p>",https://doi.org/10.1155/2011/413964,CrossRef
Computer Science,Human-Computer Interaction,Design and Development,Developing a Child Friendly Text-to-Speech System,"<jats:p>This paper discusses the implementation details of a child friendly, good quality, English text-to-speech (TTS) system that is phoneme-based, concatenative, easy to set up and use with little memory. Direct waveform concatenation and linear prediction coding (LPC) are used. Most existing TTS systems are unit-selection based, which use standard speech databases available in neutral adult voices. Here reduced memory is achieved by the concatenation of phonemes and by replacing phonetic wave files with their LPC coefficients. Linguistic analysis was used to reduce the algorithmic complexity instead of signal processing techniques. Sufficient degree of customization and generalization catering to the needs of the child user had been included through the provision for vocabulary and voice selection to suit the requisites of the child. Prosody had also been incorporated. This inexpensive TTS system was implemented in MATLAB, with the synthesis presented by means of a graphical user interface (GUI), thus making it child friendly. This can be used not only as an interesting language learning aid for the normal child but it also serves as a speech aid to the vocally disabled child. The quality of the synthesized speech was evaluated using the mean opinion score (MOS).</jats:p>",https://doi.org/10.1155/2008/597971,CrossRef
Computer Science,Human-Computer Interaction,Design and Development,Enhancing Human-Computer Interaction in Augmented Reality (AR) and Virtual Reality (VR) Environments: The Role of Adaptive Interfaces and Haptic Feedback Systems,"<jats:p>Human-Computer Interaction (HCI) in Augmented Reality (AR) and Virtual Reality (VR) environments has emerged as a critical area of research, shaping how users interact with immersive technologies. This study explores the design and development of advanced HCI frameworks in AR/VR environments, with a particular focus on adaptive interfaces and haptic feedback systems. Adaptive interfaces employ real-time data to personalize user experiences, while haptic feedback systems provide tactile sensations to bridge the gap between digital and physical interactions. The integration of these technologies enhances user engagement, immersion, and efficiency across applications such as gaming, training simulations, healthcare, and remote collaboration. This research investigates the challenges in creating intuitive, responsive systems and examines cutting-edge solutions in multimodal interactions, gesture recognition, and real-time feedback processing. By analyzing user performance and satisfaction, this study contributes to the refinement of AR/VR systems, paving the way for next-generation immersive environments.</jats:p>",https://doi.org/10.62802/jfxtjt43,CrossRef
Computer Science,Human-Computer Interaction,Design and Development,RoboTable: An Infrastructure for Intuitive Interaction with Mobile Robots in a Mixed-Reality Environment,"<jats:p>This paper presents the design, development, and testing of a tabletop interface called RoboTable, which is an infrastructure supporting intuitive interaction with both mobile robots and virtual components in a mixed-reality environment. With a flexible software toolkit and specifically developed robots, the platform enables various modes of interaction with mobile robots. Using this platform, prototype applications are developed for two different application domains:<jats:italic>RoboPong</jats:italic>investigates the efficiency of the RoboTable system in game applications, and<jats:italic>ExploreRobot</jats:italic>explores the possibility of using robots and intuitive interaction to enhance learning.</jats:p>",https://doi.org/10.1155/2012/301608,CrossRef
Computer Science,Human-Computer Interaction,Design and Development,A Text-Based Chat System Embodied with an Expressive Agent,"<jats:p>Life-like characters are playing vital role in social computing by making human-computer interaction more easy and spontaneous. Nowadays, use of these characters to interact in online virtual environment has gained immense popularity. In this paper, we proposed a framework for a text-based chat system embodied with a life-like virtual agent that aims at natural communication between the users. To achieve this kind of system, we developed an agent that performs some nonverbal communications such as generating facial expression and motions by analyzing the text messages of the users. More specifically, this agent is capable of generating facial expressions for six basic emotions such as happy, sad, fear, angry, surprise, and disgust along with two additional emotions, irony and determined. Then to make the interaction between the users more realistic and lively, we added motions such as eye blink and head movements. We measured our proposed system from different aspects and found the results satisfactory, which make us believe that this kind of system can play a significant role in making an interaction episode more natural, effective, and interesting. Experimental evaluation reveals that the proposed agent can display emotive expressions correctly 93% of the time by analyzing the users’ text input.</jats:p>",https://doi.org/10.1155/2017/8962762,CrossRef
Computer Science,Human-Computer Interaction,Theoretical / Conceptual,"Transitioning Between Audience and Performer: Co-Designing Interactive
  Music Performances with Children","Live interactions have the potential to meaningfully engage audiences during
musical performances, and modern technologies promise unique ways to facilitate
these interactions. This work presents findings from three co-design sessions
with children that investigated how audiences might want to interact with live
music performances, including design considerations and opportunities. Findings
from these sessions also formed a Spectrum of Audience Interactivity in live
musical performances, outlining ways to encourage interactivity in music
performances from the child perspective.",http://arxiv.org/abs/1702.06236v1,arXiv
Computer Science,Human-Computer Interaction,Theoretical / Conceptual,Interacting with Thoughtful AI,"We envision the concept of Thoughtful AI, a new human-AI interaction paradigm
in which the AI behaves as a continuously thinking entity. Unlike conventional
AI systems that operate on a turn-based, input-output model, Thoughtful AI
autonomously generates, develops, and communicates its evolving thought process
throughout an interaction. In this position paper, we argue that this
thoughtfulness unlocks new possibilities for human-AI interaction by enabling
proactive AI behavior, facilitating continuous cognitive alignment with users,
and fostering more dynamic interaction experiences. We outline the conceptual
foundations of Thoughtful AI, illustrate its potential through example
projects, and envision how this paradigm can transform human-AI interaction in
the future.",http://arxiv.org/abs/2502.18676v2,arXiv
Computer Science,Human-Computer Interaction,Theoretical / Conceptual,Utilising Explanations to Mitigate Robot Conversational Failures,"This paper presents an overview of robot failure detection work from HRI and
adjacent fields using failures as an opportunity to examine robot explanation
behaviours. As humanoid robots remain experimental tools in the early 2020s,
interactions with robots are situated overwhelmingly in controlled
environments, typically studying various interactional phenomena. Such
interactions suffer from real-world and large-scale experimentation and tend to
ignore the 'imperfectness' of the everyday user. Robot explanations can be used
to approach and mitigate failures, by expressing robot legibility and
incapability, and within the perspective of common-ground. In this paper, I
discuss how failures present opportunities for explanations in interactive
conversational robots and what the potentials are for the intersection of HRI
and explainability research.",http://arxiv.org/abs/2307.04462v1,arXiv
Computer Science,Human-Computer Interaction,Theoretical / Conceptual,"Inflation of Interactivity? Analyzing and Understanding Embodied
  Interaction in Interactive Art through a New Three-dimensional Model","This insight paper examines embodied interaction in interactive art, focusing
on body embodiment, bodily sensation (i.e., somaesthetic), and audience-artwork
interaction. The authors propose a new three-dimensional descriptive model of
interactive art based on literature and apply to analyze a curated corpus of 49
award-winning artworks from the Prix Ars Electronica between 2009 and 2023. The
analysis reveals emergent patterns of interactive art that deepen the
understanding of interactive art from an embodied perspective and prepare the
ground for future research and art practices. This paper has discovered that
embodied interaction remains under-explored in interactive art rather than an
inflation of interactivity. Notable research gaps persist in exploring virtual
embodiment within sociocultural contexts using immersive technologies.
Furthermore, it also underscores the need to revisit the sociological and
etymological roots of interaction to enhance interpersonality and relationality
and advocates for a paradigm shift in future research and practice in
interactive art.",http://arxiv.org/abs/2409.00047v1,arXiv
Computer Science,Human-Computer Interaction,Theoretical / Conceptual,Ten Conceptual Dimensions of Context,"This paper attempts to synthesize various conceptualizations of the term
""context"" as found in computing literature. Ten conceptual dimensions of
context thus emerge -- location; user, task, and system characteristics;
physical, social, organizational, and cultural environments; time-related
aspects, and historical information. Together, the ten dimensions of context
provide a comprehensive view of the notion of context, and allow for a more
systematic examination of the influence of context and contextual information
on human-system or human-AI interactions.",http://arxiv.org/abs/2111.04472v1,arXiv
Computer Science,Software Engineering Principles,Quantitative,Software metrics and measurement principles,"<jats:p>Software measurement is widely advocated as a fundamental constituent of an engineering approach to planning and controlling software development. Unfortunately, there is a dichotomy between the quantity of developed metrics and those used. This paper provides a tutorial review of software engineering measurement indicating the depth and breadth of the field. Individual metrics are not described due to the interest of this paper being on the measurement process and not the products of that process. Generic problems have been identified within existing measurement processes, these provide learning points for the expression of measurement principles. These principles are classified and described according to their position within the formulation, analysis and application stages of measurement. Conclusions are elaborated that suggest that existing measurement frameworks for applying measurement - often called measurement methods - do not provide sufficient support for the principles and their continued use will only serve to replicate the problems. In order to improve the products i.e. metrics, the measurement process requires improvement through inclusion of these principles in a new method.</jats:p>",https://doi.org/10.1145/181610.181625,CrossRef
Computer Science,Software Engineering Principles,Quantitative,Principles and Measurement Models for Software Assurance,<jats:p>Ensuring and sustaining software product integrity requires that all project stakeholders share a common understanding of the status of the product throughout the development and sustainment processes. Accurately measuring the product’s status helps achieve this shared understanding. This paper presents an effective measurement model organized by seven principles that capture the fundamental managerial and technical concerns of development and sustainment. These principles guided the development of the measures presented in the paper. Data from the quantitative measures help organizational stakeholders make decisions about the performance of their overall software assurance processes. Complementary risk-based data help them make decisions relative to the assessment of risk. The quantitative and risk-based measures form a comprehensive model to assess program and organizational performance. An organization using this model will be able to assess its performance to ensure secure and trustworthy products.</jats:p>,https://doi.org/10.4018/jsse.2013010101,CrossRef
Computer Science,Software Engineering Principles,Quantitative,TOWARD SOFTWARE ENGINEERING PRINCIPLES BASED ON ISLAMIC ETHICAL VALUES,"<jats:p>Software is the core for Computer-based applications which became  an essential part for critical control systems, health and human life  guard systems, financial and banking systems, educational and other  systems. It requires qualified software engineers professionally and  ethically. L.R and survey results show that software engineering  professionals facing several ethical related problems which are costly,  harmful and affected high ratio of people. Professional organizations  like ACM, IEEE, ABET and CSAC have established codes of ethics to help  software engineering professionals to understand and manage their  ethical responsibilities. Islam considers ethics an essential factor to  build individuals,communities and society. Islamic Ethics are set of  moral principles and guidance that recognizes what is right behavior  from wrong, which are comprehensive, stable, fair, and historically  prove success in building ethically great society. The 1.3 billions of  Muslims with 10s of thousands of software engineers should have an  effective role in software development and life, which requires them to  understand and implement ethics, specially the Islamic ethics in their  work. This paper is a frame-work for modeling software engineering  principle. It focuses mainly on adopting a new version of software  engineering principle based on Islamic ethical values.</jats:p>",https://doi.org/10.31436/iiumej.v9i2.99,CrossRef
Computer Science,Software Engineering Principles,Quantitative,Software engineering principles to improve quality and performance of R software,"<jats:p>Today’s computational researchers are expected to be highly proficient in using software to solve a wide range of problems ranging from processing large datasets to developing personalized treatment strategies from a growing range of options. Researchers are well versed in their own field, but may lack formal training and appropriate mentorship in software engineering principles. Two major themes not covered in most university coursework nor current literature are software testing and software optimization. Through a survey of all currently available Comprehensive R Archive Network packages, we show that reproducible and replicable software tests are frequently not available and that many packages do not appear to employ software performance and optimization tools and techniques. Through use of examples from an existing R package, we demonstrate powerful testing and optimization techniques that can improve the quality of any researcher’s software.</jats:p>",https://doi.org/10.7717/peerj-cs.175,CrossRef
Computer Science,Software Engineering Principles,Quantitative,Principles of survey research part 6,"<jats:p>This article is the last of our series of articles on survey research. In it, we discuss how to analyze survey data. We provide examples of correct and incorrect analysis techniques used in software engineering surveys.</jats:p>",https://doi.org/10.1145/638750.638758,CrossRef
Computer Science,Software Engineering Principles,Qualitative,Developing a process framework using principles of value‐based software engineering,"<jats:title>Abstract</jats:title><jats:p>In this article we present a software process framework using the 4 + 1 theory and principles of value‐based software engineering (VBSE). The value‐based process framework serves as a 6‐step process guide, and explains critical interactions between the five theories in the 4 + 1 theory of value‐based software engineering. This article also applies the process framework to a supply chain organization through a case study analysis to illustrate its strength in practice. Copyright © 2007 John Wiley &amp; Sons, Ltd.</jats:p>",https://doi.org/10.1002/spip.333,CrossRef
Computer Science,Software Engineering Principles,Qualitative,Research Dojo,"<jats:p>This report summarizes key findings from a workshop held at the 14th International Conference on Agile Software Development (XP2013) called ""Research Dojo: Collaborative Approaches for our Agile Community"".</jats:p>
          <jats:p>Both software development and research are knowledge-intensive endeavors. While agile approaches have been increasingly adopted in software development projects, whether such approaches can beneficially be applied to conducting research is a phenomenon yet to be fully explored.</jats:p>
          <jats:p>The objective of the workshop was to gain a deeper understanding of the similarities and differences between academic research and agile software development, in order to explore whether agile practices can also be used for collaboratively conducted research. The opinions of the workshop participants are summarized and observations of the research dojo session carried out by the participants are reported. We conclude by identifying further areas for investigation.</jats:p>",https://doi.org/10.1145/2507288.2507324,CrossRef
Computer Science,Software Engineering Principles,Qualitative,"Review of ""Dr. Peeling's principles of management","<jats:p>For agent-based supply chain integration &amp; coordination (SCIC), agent architecture is the foundation and core content. In this paper, agent internal architecture and an infrastructure for SCIC are discussed in detail. For agent internal architecture, generic agent internal architecture reference is given and a concrete agent internal architecture for SCIC is put forward. For infrastructure, agent grid technology is introduced into SCIC. At the same time, agent grid technology and SCIC are bridged, and detail analysis is given. Finally, a case study is showed.</jats:p>",https://doi.org/10.1145/882240.882260,CrossRef
Computer Science,Software Engineering Principles,Qualitative,A Process for Monitoring the Impact of Architecture Principles on Sustainability: An Industrial Case Study,"<jats:p>Architecture principles affect a software system holistically. Given their alignment with a business strategy, they should be incorporated within the validation process covering aspects of sustainability. However, current research discusses the influence of architecture principles on sustainability in a limited context. Our objective was to introduce a reusable process for monitoring and evaluating the impact of architecture principles on sustainability from a software architecture perspective. We sought to demonstrate the application of such a process in professional practice. A qualitative case study was conducted in the context of a Dutch airport management company. Data collection involved a case analysis and the execution of two rounds of expert interviews. We (i) identified a set of case-related key performance indicators, (ii) utilized commonly accepted measurement tools, and (iii) employed graphical representations in the form of spider charts to monitor the sustainability impacts. The real-world observations were evaluated through a concluding focus group. Our findings indicated that architecture principles were a feasible mechanism with which to address sustainability across all different architecture layers within the enterprise. The experts considered the sustainability analysis valuable in guiding the software architecture process towards sustainability. With the emphasis on principles, we facilitate industry adoption by embedding sustainability in existing mechanisms.</jats:p>",https://doi.org/10.3390/software3010006,CrossRef
Computer Science,Software Engineering Principles,Qualitative,Morescient GAI for Software Engineering (Extended Version),"The ability of Generative AI (GAI) technology to automatically check,
synthesize and modify software engineering artifacts promises to revolutionize
all aspects of software engineering. Using GAI for software engineering tasks
is consequently one of the most rapidly expanding fields of software
engineering research, with over a hundred LLM-based code models having been
published since 2021. However, the overwhelming majority of existing code
models share a major weakness - they are exclusively trained on the syntactic
facet of software, significantly lowering their trustworthiness in tasks
dependent on software semantics. To address this problem, a new class of
""Morescient"" GAI is needed that is ""aware"" of (i.e., trained on) both the
semantic and static facets of software. This, in turn, will require a new
generation of software observation platforms capable of generating large
quantities of execution observations in a structured and readily analyzable
way. In this paper, we present a vision and roadmap for how such ""Morescient""
GAI models can be engineered, evolved and disseminated according to the
principles of open science.",http://arxiv.org/abs/2406.04710v2,arXiv
Computer Science,Software Engineering Principles,Mixed Methods,Probabilistic Software Modeling,"Software Engineering and the implementation of software has become a
challenging task as many tools, frameworks and languages must be orchestrated
into one functioning piece. This complexity increases the need for testing and
analysis methodologies that aid the developers and engineers as the software
grows and evolves. The amount of resources that companies budget for testing
and analysis is limited, highlighting the importance of automation for economic
software development. We propose Probabilistic Software Modeling, a new
paradigm for software modeling that builds on the fact that software is an
easy-to-monitor environment from which statistical models can be built.
Probabilistic Software Modeling provides increased comprehension for engineers
without changing the level of abstraction. The approach relies on the recursive
decomposition principle of object-oriented programming to build hierarchies of
probabilistic models that are fitted via observations collected at runtime of a
software system. This leads to a network of models that mirror the static
structure of the software system while modeling its dynamic runtime behavior.
The resulting models can be used in applications such as test-case generation,
anomaly and outlier detection, probabilistic program simulation, or state
predictions. Ideally, probabilistic software modeling allows the use of the
entire spectrum of statistical modeling and inference for software, enabling
in-depth analysis and generative procedures for software.",http://arxiv.org/abs/1806.08942v2,arXiv
Computer Science,Software Engineering Principles,Mixed Methods,"Scaling Agile Development in Mechatronic Organizations - A Comparative
  Case Study","Agile software development principles enable companies to successfully and
quickly deliver software by meeting their customers' expectations while
focusing on high quality. Many companies working with pure software systems
have adopted these principles, but implementing them in companies dealing with
non-pure software products is challenging. We identified a set of goals and
practices to support large-scale agile development in companies that develop
software-intense mechatronic systems. We used an inductive approach based on
empirical data collected during a longitudinal study with six companies in the
Nordic region. The data collection took place over two years through focus
group workshops, individual on-site interviews, and complementary surveys. The
primary benefit of large-scale agile development is improved quality, enabled
by practices that support regular or continuous integration between teams
delivering software, hardware, and mechanics. In this regard, the most
beneficial integration cycle for deliveries is every four weeks; while
continuous integra- tion on a daily basis would favor software teams, other
disciplines does not seem to benefit from faster integration cycles. We
identified 108 goals and development practices supporting agile principles
among the companies, most of them concerned with integration; therefrom, 26
agile practices are unique to the mechatronics domain to support adopting agile
beyond pure software development teams. 16 of these practices are considered as
key enablers, confirmed by our control cases.",http://arxiv.org/abs/1703.00206v2,arXiv
Computer Science,Software Engineering Principles,Mixed Methods,"Guiding Principles for Using Mixed Methods Research in Software
  Engineering","Mixed methods research is often used in software engineering, but researchers
outside of the social or human sciences often lack experience when using these
designs. This paper provides guiding principles and advice on how to design
mixed method research, and to encourage the intentional, rigorous, and
innovative application of mixed methods in software engineering. It also
presents key properties of core mixed method research designs. Through a number
of fictitious but recognizable software engineering research scenarios, we
showcase how to choose suitable designs and consider the inevitable trade-offs
any design choice leads to. We describe several antipatterns that illustrate
what to avoid in mixed method research, and when mixed method research should
be considered over other approaches.",http://arxiv.org/abs/2404.06011v4,arXiv
Computer Science,Software Engineering Principles,Mixed Methods,"A sustainable infrastructure concept for improved accessibility,
  reusability, and archival of research software","Research software is an integral part of most research today and it is widely
accepted that research software artifacts should be accessible and
reproducible. However, the sustainable archival of research software artifacts
is an ongoing effort. We identify research software artifacts as snapshots of
the current state of research and an integral part of a sustainable cycle of
software development, research, and publication. We develop requirements and
recommendations to improve the archival, access, and reuse of research software
artifacts based on installable, configurable, extensible research software, and
sustainable public open-access infrastructure. The described goal is to enable
the reuse and exploration of research software beyond published research
results, in parallel with reproducibility efforts, and in line with the FAIR
principles for data and software. Research software artifacts can be reused in
varying scenarios. To this end, we design a multi-modal representation concept
supporting multiple reuse scenarios. We identify types of research software
artifacts that can be viewed as different modes of the same software-based
research result, for example, installation-free configurable browser-based apps
to containerized environments, descriptions in journal publications and
software documentation, or source code with installation instructions. We
discuss how the sustainability and reuse of research software are enhanced or
enabled by a suitable archive infrastructure. Finally, at the example of a
pilot project at the University of Stuttgart, Germany -- a collaborative effort
between research software developers and infrastructure providers -- we outline
practical challenges and experiences",http://arxiv.org/abs/2301.12830v1,arXiv
Computer Science,Software Engineering Principles,Mixed Methods,Measuring Object-Oriented Design Principles,"The idea of automatizing the assessment of objectoriented design is not new.
Different approaches define and apply their own quality models, which are
composed of single metrics or combinations thereof, to operationalize software
design. However, single metrics are too fine-grained to identify core design
flaws and they cannot provide hints for making design improvements. In order to
deal with these weaknesses of metric-based models, rules-based approaches have
proven successful in the realm of source-code quality. Moreover, for developing
a well-designed software system, design principles play a key role, as they
define fundamental guidelines and help to avoid pitfalls. Therefore, this
thesis will enhance and complete a rule-based quality reference model for
operationalizing design principles and will provide a measuring tool that
implements these rules. The validation of the design quality model and the
measurement tool will be based on various industrial projects. Additionally,
quantitative and qualitative surveys will be conducted in order to get
validated results on the value of object-oriented design principles for
software development",http://arxiv.org/abs/1602.07127v1,arXiv
Computer Science,Software Engineering Principles,Design and Development,Identify Compliance During Software Development Using System Engineering Principles,"<jats:p>Errors made during the requirements collection and analysis phase make it very difficult to maintain the software product and cost the company extra costs. The difficulty of directly collecting requirements from stakeholders is due to inconsistencies between the major stakeholder groups, as well as related factors in the collection of requirements itself, as well as the selected methodology for the process of converting stakeholder requirements into development requirements. As a solution, it is necessary to use a high level of prioritization in order to distinguish among many requirements the necessary for successful implementation of the product, as well as to correctly allocate compliance with the requirements in such a way that each group of stakeholders is satisfied, but at the same time setting the goals of the supersystem more priority  than the goals of the subsystem. This article discusses the methodology of system engineering to solve issues related to the identification of possible contradictions of requirements.
Keywords: System engineering, requirements engineering, business process, requirements, software product, analysis.</jats:p>",https://doi.org/10.18502/keg.v5i3.6765,CrossRef
Computer Science,Software Engineering Principles,Design and Development,Fundamental Principles of Cybersecurity in The Software Testing Process,"<jats:p>The study examines the principles of ensuring cybersecurity during software testing. The focus is placed on the fact that testing should not be limited to validation checks but must also incorporate risk assessment, compliance with standards, and early-stage vulnerability analysis throughout the software development lifecycle. The study reviews key regulatory requirements (GDPR, HIPAA, PCI DSS, ISO/IEC 27001, NIST Cybersecurity Framework) and analyzes their impact on testing strategies and quality control processes. Special attention is given to the CIA triad (confidentiality, integrity, and availability) and proactive incident planning. The necessity of integrating automated tools (SAST/DAST, SIEM, RPA, etc.) and artificial intelligence algorithms is substantiated to optimize protection procedures and enhance vulnerability detection efficiency. The conclusions emphasize that achieving a high level of product resilience is only possible through the close alignment of security requirements with test scenarios and the continuous refinement of testing methodologies. The findings presented in this study will be of interest to researchers and professionals in information security, software testing specialists, and developers seeking to integrate advanced methods into the protection of information assets.</jats:p>",https://doi.org/10.37547/tajet/volume07issue04-14,CrossRef
Computer Science,Software Engineering Principles,Design and Development,SOLID Principles: Enhancing Maintainability and Scalability in Software Development,"<jats:p>In software development, achieving maintainability and scalability is critical for building robust and future-proof
applications. The SOLID principles, a set of five design guidelines introduced by Robert C. Martin, serve as a
cornerstone for object-oriented design, addressing common challenges in software architecture. This paper explores
the SOLID principles, their significance, and their practical application in enhancing software quality. Real-world
examples and case studies illustrate how adhering to these principles leads to scalable, maintainable, and flexible
software systems.</jats:p>",https://doi.org/10.55041/ijsrem10895,CrossRef
Computer Science,Software Engineering Principles,Design and Development,Separability Principles for a General Theory of Software Engineering,"<jats:p>
            The four GTSE (General Theory of Software Engineering) Workshops have brought awareness to, more or less mature, differing approaches, candidate theories for SE (Software Engineering). But one asks how to appraise the generality of these theories? And in case they are specialized sub-theories, are they amenable to combination into more general theories? The papers of the fourth GTSE Workshop addressed these questions by means of what can be collectively refer to as
            <jats:italic>Separability Principles</jats:italic>
            . In a sense, participants used well known techniques applied to design software systems to design SE theories. Separability is a powerful tool for understanding relations among SE candidate theories and guide how to assemble sub-theories into a general framework. Participants enthusiastically debated a series of related issues. The specialized vs. general theories questions were raised in diverse forms, such as, SE meaning multiple things, good predictive theories for narrow problems, ability of General theories to generate specific theories, and last but not least, whether ""General"" capture the contents of the workshop itself. The 4th GTSE edition was collocated with ICSE 2015 (International Conference of Software Engineering) in Firenze, Italy
          </jats:p>",https://doi.org/10.1145/2853073.2853093,CrossRef
Computer Science,Software Engineering Principles,Design and Development,Optimization design of electronic commerce system based on the basic principles of software engineering,"<jats:p>Based on the basic principles of software engineering, this paper optimizes the design of e-commerce system from the aspects of demand analysis, system design, coding, implementation, testing and maintenance. Specifically, this paper adopts the object-oriented software development method, using the UML modeling tools to model the system, adopted the MVC architecture mode to realize the stratification of the system, using the front-end technology such as HTML, CSS, JavaScript to realize the user interface design of the system, combined with the back-end technology such as Java, JSP, the Servlet system. Through the system test and the use of code quality control tools, the system is accepted, and the maintenance of the system is planned and designed. Finally, this paper realizes an easy-to-use, stable, safe and efficient e-commerce system.</jats:p>",https://doi.org/10.56028/aemr.6.1.403.2023,CrossRef
Computer Science,Software Engineering Principles,Theoretical / Conceptual,Principles for software environments,<jats:p>A large number of software development support environments have been developed during the last few years. Many more are being developed now. This paper presents some principles which should be followed in the development and evolution of such an environment. Stress is placed on the idea that an environment evolves and that the ultimate success or failure of the environment depends on its evolution more than on its initial appearance.</jats:p>,https://doi.org/10.1145/1005968.1005975,CrossRef
Computer Science,Software Engineering Principles,Theoretical / Conceptual,Principles of Software Engineering and their Applications,"<jats:p>Software engineering lacks maturity compared to other engineering disciplines. The research goal of this thesis is to contribute to the software engineering discipline from an engineering perspective, through the identification of software engineering fundamental principles and the description of operational guidelines for these engineering fundamental principles. This research study on the set of candidate fundamental principles will contribute to a better understanding and possibly from an engineering perspective.</jats:p>",https://doi.org/10.4028/www.scientific.net/amm.170-173.3468,CrossRef
Computer Science,Software Engineering Principles,Theoretical / Conceptual,"Using a Class-Wide, Semester-Long Project to Teach Software Engineering Principles","<jats:title>Abstract</jats:title><jats:p>A senior-level, project-based Software Engineering course taught at the University of Central Arkansas serves as the capstone course for the Computer Science Program and introduces students to the theory, tools, and techniques used to build large-scale software systems in a project-driven setting. Foundational to the course is the use of a class-wide, semesterlong course project to emphasize the theoretical aspects of the software process and the system used for scoring student performance on the project. One project is selected for the entire class with students divided into teams of four to six students to support different functional requirement areas. A milestone-driven approach is used following a modified version of the Unified Process for project development. Student scores on the project are divided into a group score, assignable via a rubric-like grade sheet, and an individual score which is determined by the individual’s effort as assigned using the task-management tool, Issue-Tracker. Experiences gained and lessons learned in teaching the course are provided as a guide for those wishing to follow a similar approach to teaching Software Engineering in the future.</jats:p>",https://doi.org/10.7603/s40601-013-0032-y,CrossRef
Computer Science,Software Engineering Principles,Theoretical / Conceptual,Report of the 2nd international workshop on principles of engineering service-oriented systems (PESOS 2010),"<jats:p>The Second International Workshop on Principles of Engineering Service-Oriented Systems (PESOS 2010) was held at the International Conference on Software Engineering, ICSE 2010 on May 1 and 2, 2010. PESOS 2010 provided a forum for presenting and discussing current work and research topics related to serviceoriented systems. The workshop had keynotes on SOA testing challenges and adaptive service-oriented systems. There were four paper sessions on the topics of service development, testing and evolution of service-oriented systems, service adaptation, and quality of service (QoS) and Service-Level Agreements (SLAs) in service-oriented environments. General discussions focused on these overall themes. These discussions resulted in the identification of research challenges for the future.</jats:p>",https://doi.org/10.1145/1838687.1838694,CrossRef
Computer Science,Software Engineering Principles,Theoretical / Conceptual,Software engineering research versus software development,"<jats:p>
            Engineering research differs greatly, both in its aims and in its methods, from traditional ""scientific"" research. While Sciences deal with the study of existing objects and phenomena, be it physically, metaphysically or conceptually, Engineering is based on
            <jats:italic>how</jats:italic>
            to do things,
            <jats:italic>how</jats:italic>
            to create new objects. For this reason, ""scientific"" research methods are not always directly applicable to research problems of an engineering nature.In the present article, we concentrate on the problems and research methods of a specific branch of engineering: Software Engineering, discussing, on the one hand, the nature of the method in this field while and, on the other, the similarity of the methods of research in Software Engineering and those of software development.
          </jats:p>",https://doi.org/10.1145/1082983.1083005,CrossRef
Software Engineering,Software Development Processes,Quantitative,A Study on Software Metrics and its Impact on Software Quality,"Software metrics offer a quantitative basis for predicting the software
development process. In this way, software quality can be improved very easily.
Software quality should be achieved to satisfy the customer with decreasing the
software cost and improve there liability of the software product. In this
research, we have discussed how the software metrics affect the quality of the
software and which stages of its development software metrics have applied. We
discussed the different software metrics and how these metrics have an impact
on software quality and reliability. These techniques have been used for
improving the quality of software and increase the revenue.",http://arxiv.org/abs/1905.12922v1,arXiv
Software Engineering,Software Development Processes,Quantitative,"Toward a Better Understanding of How to Develop Software Under Stress -
  Drafting the Lines for Future Research","The software is often produced under significant time constraints. Our idea
is to understand the effects of various software development practices on the
performance of developers working in stressful environments, and identify the
best operating conditions for software developed under stressful conditions
collecting data through questionnaires, non-invasive software measurement tools
that can collect measurable data about software engineers and the software they
develop, without intervening their activities, and biophysical sensors and then
try to recreated also in different processes or key development practices such
conditions.",http://arxiv.org/abs/1804.09044v1,arXiv
Software Engineering,Software Development Processes,Quantitative,Software Effort Estimation using parameter tuned Models,"Software estimation is one of the most important activities in the software
project. The software effort estimation is required in the early stages of
software life cycle. Project Failure is the major problem undergoing nowadays
as seen by software project managers. The imprecision of the estimation is the
reason for this problem. Assize of software size grows, it also makes a system
complex, thus difficult to accurately predict the cost of software development
process. The greatest pitfall of the software industry was the fast-changing
nature of software development which has made it difficult to develop
parametric models that yield high accuracy for software development in all
domains. We need the development of useful models that accurately predict the
cost of developing a software product. This study presents the novel analysis
of various regression models with hyperparameter tuning to get the effective
model. Nine different regression techniques are considered for model
development",http://arxiv.org/abs/2009.01660v1,arXiv
Software Engineering,Software Development Processes,Quantitative,Testing Research Software: A Survey,"Background: Research software plays an important role in solving real-life
problems, empowering scientific innovations, and handling emergency situations.
Therefore, the correctness and trustworthiness of research software are of
absolute importance. Software testing is an important activity for identifying
problematic code and helping to produce high-quality software. However, testing
of research software is difficult due to the complexity of the underlying
science, relatively unknown results from scientific algorithms, and the culture
of the research software community. Aims: The goal of this paper is to better
understand current testing practices, identify challenges, and provide
recommendations on how to improve the testing process for research software
development. Method: We surveyed members of the research software developer
community to collect information regarding their knowledge about and use of
software testing in their projects. Results: We analysed 120 responses and
identified that even though research software developers report they have an
average level of knowledge about software testing, they still find it difficult
due to the numerous challenges involved. However, there are a number of ways,
such as proper training, that can improve the testing process for research
software. Conclusions: Testing can be challenging for any type of software.
This difficulty is especially present in the development of research software,
where software engineering activities are typically given less attention. To
produce trustworthy results from research software, there is a need for a
culture change so that testing is valued and teams devote appropriate effort to
writing and executing tests.",http://arxiv.org/abs/2205.15982v1,arXiv
Software Engineering,Software Development Processes,Quantitative,Challenges and issues in collaborative software developments,"The software development process has evolved with respect to the problems in
developing large and complex applications. There is a paradigm shift towards
collaborative development, which necessitates the need to evaluate this
approach. A number of tools are used for collaborative software development
(CSD) including social media and web 2.0 features. Collaborative development
facilities are provided by IDEs and project hosting websites. In this paper, we
present a survey of collaboratively developed projects and discuss challenges
and issues in CSD. We analyze various issues of communication, coordination,
support, lifecycle management and discuss their effect on software quality.",http://arxiv.org/abs/1904.00721v1,arXiv
Software Engineering,Software Development Processes,Qualitative,"Secure Software Engineering in the Financial Services: A Practitioners'
  Perspective","Secure software engineering is a fundamental activity in modern software
development. However, while the field of security research has been advancing
quite fast, in practice, there is still a vast knowledge gap between the
security experts and the software development teams. After all, we cannot
expect developers and other software practitioners to be security experts.
Understanding how software development teams incorporate security in their
processes and the challenges they face is a step towards reducing this gap. In
this paper, we study how financial services companies ensure the security of
their software systems. To that aim, we performed a qualitative study based on
semi-structured interviews with 16 software practitioners from 11 different
financial companies in three continents. Our results shed light on the security
considerations that practitioners take during the different phases of their
software development processes, the different security practices that software
teams make use of to ensure the security of their software systems, the
improvements that practitioners perceive as important in existing
state-of-the-practice security tools, the different knowledge-sharing and
learning practices that developers use to learn more about software security,
and the challenges that software practitioners currently face when it comes to
secure their systems.",http://arxiv.org/abs/2104.03476v1,arXiv
Software Engineering,Software Development Processes,Qualitative,"Guided Support for Collaborative Modeling, Enactment and Simulation of
  Software Development Processes","Recently, the awareness of the importance of distributed software development
has been growing in the software engineering community. Economic constraints,
more and more outsourcing of development activities, and the increasing spatial
distribution of companies come along with challenges of how to organize
distributed development.
  In this article, we reason that a common process understanding is mandatory
for successful distributed development. Integrated process planning, guidance
and enactment are seen as enabling technologies to reach a unique process view.
  We sketch a synthesis of the software process modeling environment SPEARMINT
and the XCHIPS system for web-based process support. Hereby, planners and
developers are provided with collaborative planning and enactment support and
advanced process guidance via electronic process guides (EPGs). We describe the
usage of this integrated environment by using a case study for the development
of a learning system.",http://arxiv.org/abs/1402.4280v1,arXiv
Software Engineering,Software Development Processes,Qualitative,"LLMs' Reshaping of People, Processes, Products, and Society in Software
  Development: A Comprehensive Exploration with Early Adopters","Large language models (LLMs) like OpenAI ChatGPT, Google Gemini, and GitHub
Copilot are rapidly gaining traction in the software industry, but their full
impact on software engineering remains insufficiently explored. Despite their
growing adoption, there is a notable lack of formal, qualitative assessments of
how LLMs are applied in real-world software development contexts. To fill this
gap, we conducted semi-structured interviews with sixteen early-adopter
professional developers to explore their use of LLMs throughout various stages
of the software development life cycle. Our investigation examines four
dimensions: people - how LLMs affect individual developers and teams; process -
how LLMs alter software engineering workflows; product - LLM impact on software
quality and innovation; and society - the broader socioeconomic and ethical
implications of LLM adoption. Thematic analysis of our data reveals that while
LLMs have not fundamentally revolutionized the development process, they have
substantially enhanced routine coding tasks, including code generation,
refactoring, and debugging. Developers reported the most effective outcomes
when providing LLMs with clear, well-defined problem statements, indicating
that LLMs excel with decomposed problems and specific requirements.
Furthermore, these early-adopters identified that LLMs offer significant value
for personal and professional development, aiding in learning new languages and
concepts. Early-adopters, highly skilled in software engineering and how LLMs
work, identified early and persisting challenges for software engineering, such
as inaccuracies in generated content and the need for careful manual review
before integrating LLM outputs into production environments. Our study provides
a nuanced understanding of how LLMs are shaping the landscape of software
development, with their benefits, limitations, and ongoing implications.",http://arxiv.org/abs/2503.05012v1,arXiv
Software Engineering,Software Development Processes,Qualitative,Software Development Processes in Ocean System Modeling,"Scientific modeling provides mathematical abstractions of real-world systems
and builds software as implementations of these mathematical abstractions.
Ocean science is a multidisciplinary discipline developing scientific models
and simulations as ocean system models that are an essential research asset.
  In software engineering and information systems research, modeling is also an
essential activity. In particular, business process modeling for business
process management and systems engineering is the activity of representing
processes of an enterprise, so that the current process may be analyzed,
improved, and automated.
  In this paper, we employ process modeling for analyzing scientific software
development in ocean science to advance the state in engineering of ocean
system models and to better understand how ocean system models are developed
and maintained in ocean science. We interviewed domain experts in
semi-structured interviews, analyzed the results via thematic analysis, and
modeled the results via the business process modeling notation BPMN.
  The processes modeled as a result describe an aspired state of software
development in the domain, which are often not (yet) implemented. This enables
existing processes in simulation-based system engineering to be improved with
the help of these process models.",http://arxiv.org/abs/2108.08589v1,arXiv
Software Engineering,Software Development Processes,Qualitative,"Software Architecture Decision-Making Practices and Challenges: An
  Industrial Case Study","Software architecture decision-making is critical to the success of a
software system as software architecture sets the structure of the system,
determines its qualities, and has far-reaching consequences throughout the
system life cycle. The complex nature of the software development context and
the importance of the problem has led the research community to develop several
techniques, tools, and processes to assist software architects in making better
decisions. Despite these effort, the adoption of such systematic approaches
appears to be quite limited in practice. In addition, the practitioners are
also facing new challenges as different software development methods suggest
different approaches for architecture design. In this paper, we study the
current software architecture decision-making practices in the industry using a
case study conducted among professional software architects in three different
companies in Europe. As a result, we identified different software architecture
decision-making practices followed by the software teams as well as their
reasons for following them, the challenges associated with them, and the
possible improvements from the software architects' point of view. Based on
that, we recognized that improving software architecture knowledge management
can address most of the identified challenges and would result in better
software architecture decision-making.",http://arxiv.org/abs/1610.09240v1,arXiv
Software Engineering,Software Development Processes,Mixed Methods,Impact of Artificial Intelligence on Software Development Processes,"<jats:p>The emergence of Artificial Intelligence (AI) has signified a fundamental transformation in software engineering methodologies. Conventional techniques, marked by significant manual involvement, are progressively being enhanced or supplanted by intelligent systems. Artificial intelligence technologies and frameworks streamline development, decrease expenses, and improve precision. Artificial Intelligence (AI) is revolutionizing software development by automating redundant jobs, improving decision-making, and optimizing procedures throughout all phases of development. This study examines the influence of AI on software development using qualitative and quantitative assessments, concentrating on phases such as requirement collecting, coding, testing, and deployment. This analysis employs topic and keyword methodologies to explore AI's contribution to enhancing efficiency, accuracy, and team cooperation, while also addressing integration problems and ethical implications. The study emphasizes the role of AI technologies in minimizing errors, enhancing project timeframes, and elevating overall software quality. These conclusions establish a basis for forthcoming research on AI's capacity to transform software engineering methodologies.</jats:p>",https://doi.org/10.52783/jisem.v10i25s.4039,CrossRef
Software Engineering,Software Development Processes,Mixed Methods,Risk Management in Software Development Projects,"<p>Effective risk management contributes to the success of the software development project. The goal of this work was to identify risk management gaps, perspectives, the evolution of the theme and the study trends, in software development projects, using systematic literature review as a method. For the bibliometric analysis, articles referring to the topic were selected in the period from 2010 to 2018. As tools of analysis, Citespace and VOS Viewer software were used, allowing a comparative evaluation between the articles, as well as the analysis of clusters. Beyond content analysis of articles found. Gaps were identified for performance; team involvement; attention to failures; identification of tools for decision-making; and business strategy. In turn, perspectives were determined for research trends, such as the close relationship between business strategy, risk management and new management models. The research can propose new strategies and perspectives for risk management in software development and show their importance to the academic and practical spheres, demonstrating that the themes are complementary and important in the current technological and innovation sector.</p>",https://doi.org/10.4018/ijossp.2020010101,CrossRef
Software Engineering,Software Development Processes,Mixed Methods,A business process modeling‐based approach to investigate complex processes,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>The purpose of this paper is to develop a new approach to investigate complex processes, such as software development processes, using business process modeling.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>The paper presents an investigation into the use of role activity diagramming (RAD) to model complex processes in the software industry sector, with reference to the process of TestWarehouse as a case study.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>Systematic extension and quantitative analysis to RAD models led to the discovery of process bottlenecks, identification of cross functional boundary problems, and focused discussion about automation of processes.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Research limitations/implications</jats:title><jats:p>Further work is required to validate and evaluate the proposed approach using several cases with different application domains and thus generalize the adopted approach.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p>A new approach has been used successfully to understand and analyze business processes. The tools and techniques that are used to perform the approach are not complicated and do not need much specialist expertise, so the approach is not only oriented toward specialists but also toward organizations' managers and staff.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>New techniques have been developed by using process modelling to deepen the understanding and analyzing of complex organizational processes. This research implements a practical investigation which uses a case study to validate the new techniques.</jats:p></jats:sec>",https://doi.org/10.1108/14637151211215046,CrossRef
Software Engineering,Software Development Processes,Mixed Methods,Software development for implementing a model of porous structures based on three periodic surfaces,"<jats:p>Based on the original algorithm for generating three periodic surfaces implemented in the ToposPro information and analytical system, a mathematical model of a porous material was developed. The TPS Extractor software for the computer implementation of this model was developed. This software implements original algorithms for triangulation, translation, smoothing, and model solidifying. The developed triangulation algorithms were used to construct a set of three periodic surfaces, and models of the corresponding porous materials were built on their basis. Based on models of porous materials, a study of the performance of smoothing and translation algorithms was conducted. Using a solidifying algorithm for increasing the model thickness, models of porous material were created that are suitable for 3D printing. Also, samples of porous models were printed out using fused deposition modeling technology.</jats:p>",https://doi.org/10.18469/1810-3189.2022.25.1.71-79,CrossRef
Software Engineering,Software Development Processes,Mixed Methods,FQMaP: Towards a framework quantitative management of processes in small software development organizations,"<jats:p>Software development organizations need to control and improve their practices, seeking to reduce variability when executing the necessary processes to elaborate software; therefore, these organizations implement improvement plans to identify factors that affect the processes. Quantitative Management deals with identification, tracing, and control of those incident factors, using data proactively to predict performance and the effect of possible changes in a process. Reference models in software processes development such as CMMI V2.0 and ISO/IEC 33061:2021 address Quantitative Management, but are aimed at big enterprises. Other models such as MoProSoft, COMPETISOFT, and MPS.BR are aimed at small enterprises, but do not include enough elements on Quantitative Management. Execution of a systematic literature review permitted searching for works on Quantitative Management intended for small software development enterprises, indicating necessary practices and how to perform them. This search showed that a proposal is not available that incorporates Quantitative Management practices for software processes aimed small software development enterprises. The referred aspects make it difficult to adopt a Quantitative Management culture within these organizations, it which has become a problem, consisting in that small software development enterprises that do not execute quantitative management practices will have difficulty identifying and focusing on the factors that impact the process performance and, therefore, on the results of their projects. This work sought to tackle this problem by proposing the “framework for quantitative management of processes in small software development organizations” (FQMaP), which allows incorporating practices and techniques that support Quantitative Management of software development processes in these kinds of enterprises. From the evaluation of FQMaP, carried out by following Focus Group technique guidelines, it can be demonstrated that it is a simple proposal and with elements that can serve a company to quantitatively manage software development processes. Also, it has clearly specified its components, showing that its structure is familiar with other process patterns, that would facilitate their interpretation.</jats:p>",https://doi.org/10.21533/pen.v11.i4.176,CrossRef
Software Engineering,Software Development Processes,Design and Development,Some Long-Standing Quality Practices in Software Development,"The desire to build quality software systems has been the focus of most
software developers and researchers for decades. This has culminated in the
design of practices that promote quality in the designed software. Originating
from the inception of the traditional software development life cycle (SDLC),
through to the object-oriented methods, Iterative development, and now the
agile methods, these practices have persisted through different periods. Such
practices play the same quality role regardless of the perspective of the
software development process they are part of. In this paper we review three
software development methods representative of the software development
history, with the aim of i) identifying key quality practices, ii) identifying
the quality role played by the practice in the method, and iii) noting those
quality practices that have persisted through the software development history.
The identified quality practices that have persisted throughout the history of
the software development processes include prototyping, iterative development,
incremental development, risk-driven development, phase planning, and phase
retrospection. These results would be useful to method engineers who seek to
design high-quality software development methods as these practices serve as
candidates for inclusion in their development processes. Software development
practitioners seeking to design quality software would also benefit from
adopting these practices in developing their software.",http://arxiv.org/abs/2209.08348v1,arXiv
Software Engineering,Software Development Processes,Design and Development,"Comparative Analysis of Software Development Methods between Parallel,
  V-Shaped and Iterative","Any organization that will develop software is faced with a difficult choice
of choosing the right software development method. Whereas the software
development methods used, play a significant role in the overall software
development process. Software development methods are needed so that the
software development process can be systematic so that it is not only completed
within the right time frame but also must have good quality. There are various
methods of software development in System Development Lyfe Cycle (SDLC). Each
SDLC method provides a general guiding line about different software
development and has different characteristics. Each method of software
development has its drawbacks and advantages so that the selection of software
development methods should be compatible with the capacity of the software
developed. This paper will compare three different software development
methods: V-Shaped Model, Parallel Development Model, and Iterative Model with
the aim of providing an understanding of software developers to choose the
right method.",http://arxiv.org/abs/1710.07014v1,arXiv
Software Engineering,Software Development Processes,Design and Development,"Importance of Secure Software Development Processes and Tools for
  Developers","In this research paper of secure software systems, authors have discussed
what the proper development process is when it comes to creating a secure
software, which will be suited for developers and relevent stakeholders alike.
Secure Software Development Process for Developers is of crucial importance for
software engineers as more and more software-based devices are becoming
commonly available, and cloud services are evolving which require for the
software to be constantly connected to the internet. With this in mind, Secure
Software Development needs to be transformed to something most developers can
rely upon to make applied software safe and have the capability to mitigate
against potential attacks by hackers. Furthermore, in this paper, existing
Secure Software Development Process ideas and implementations are reviewed and
investigated using the research paper pool available online. Thereafter, an
approach is proposed to enhance the security aspect in software development
process to resolve security issues. Lastly, the paper concludes with final
remarks on practical implementation of security features in software
development phases for production of secure and reliable software programs and
systems.",http://arxiv.org/abs/2012.15153v1,arXiv
Software Engineering,Software Development Processes,Design and Development,"Think-on-Process: Dynamic Process Generation for Collaborative
  Development of Multi-Agent System","Software development is a collaborative endeavor that requires individuals
from different departments to work together in order to collectively develop a
high-quality software system. In this context, people have begun to explore a
method that leverages multi-agent systems based on LLMs to carry out software
development. However, existing research tends to rigidly fix the software
development process in a framework in code form, thus failing to dynamically
adjust the software development process in real-time to meet the more flexible
and variable software environment. In this paper, we propose a dynamic process
generation framework, named ToP (Think-on-Process). The core idea of ToP is to
leverage experiential knowledge (i.e., process models) to guide LLMs in
generating software development processes (i.e., instances). These instances
will guide multi-agent in software development and employ a compiler to provide
feedback on the development outcomes. Subsequently, we utilize heuristic
algorithms to filter the instances and apply process mining algorithms to
derive process model. Finally, the process model will be converted into text,
formatted as prompts, to enhance the ability of LLMs to generate other
instances. Experiments demonstrate that our framework ToP significantly
enhances the dynamic process generation capability of the GPT-3.5 and GPT-4 for
five categories of software development tasks.",http://arxiv.org/abs/2409.06568v1,arXiv
Software Engineering,Software Development Processes,Design and Development,SRS BUILDER 1.0: An Upper Type CASE Tool For Requirement Specification,"Software (SW) development is a labor intensive activity. Modern software
projects generally have to deal with producing and managing large and complex
software products. Developing such software has become an extremely challenging
job not only because of inherent complexity, but also mainly for economic
constraints unlike time, quality, maintainability concerns. Hence, developing
modern software within the budget still remains as one of the main software
crisis. The most significant way to reduce the software development cost is to
use the Computer-Aided Software Engineering (CASE) tools over the entire
Software Development Life Cycle (SDLC) process as substitute to expensive human
labor cost. We think that automation of software development methods is a
valuable support for the software engineers in coping with this complexity and
for improving quality too. This paper demonstrates the newly developed CASE
tools name ""SRS Builder 1.0"" for software requirement specification developed
at our university laboratory, University of North Bengal, India. This paper
discusses our new developed product with its functionalities and usages. We
believe the tool has the potential to play an important role in the software
development process.",http://arxiv.org/abs/1109.1651v1,arXiv
Software Engineering,Software Development Processes,Theoretical / Conceptual,Open Source Software Development Challenges,"<jats:p>GitHub is the most common code hosting and repository service for open-source software (OSS) projects. Thanks to the great variety of features, researchers benefit from GitHub to solve a wide range of OSS development challenges. In this context, the authors thought that was important to conduct a literature review on studies that used GitHub data. To reach these studies, they conducted this literature review based on a GitHub dataset source study instead of a keyword-based search in digital libraries. Since GHTorrent is the most widely known GitHub dataset according to the literature, they considered the studies that cite this dataset for the systematic literature review. In this study, they reviewed the selected 172 studies according to some criteria that used the dataset as a data source. They classified them within the scope of OSS development challenges thanks to the information they extract from the metadata of studies. They put forward some issues about the dataset and they offered the focused and attention-grabbing fields and open challenges that we encourage the researchers to study on them.</jats:p>",https://doi.org/10.4018/ijossp.2020100101,CrossRef
Software Engineering,Software Development Processes,Theoretical / Conceptual,An  Overview of Tools for Collecting Data on Software Development and Debugging Processes from Integrated Development Environments,"<jats:p>Purpose. This paper presents the findings of a review of the literature published in the twenty-first century in order to identify and analyze the current state of tools that track developer interactions with integrated development environments, as well as to recommend future research directions based on the actual state. Methodology. By systematically searching in five digital libraries we conducted a systematic review of the literature on data collection tools from integrated development environments published in the twenty-first century. Fifty-five papers were selected as primary studies. Findings. 55 articles were analyzed and the findings show that using an integrated development environment to collect usage data provides more insight into developer activities than it was previously possible. Usage data allows us to analyze how developers spend their time. With usage data, you can learn more about how developers create mental models, investigate code, conduct mini-experiments through trial and error, and what can help everyone improve performance. The research community continues to be highly active in developing tools to track developer activity. The findings indicate that more research is needed in this area to better understand and measure programmer behavior. Originality. For the first time, systematization and analysis of tools for tracking programmer's behavior in an integrated development environment have been carried out. Practical value. Our study contributes to a better understanding of the current state of research on programmer behavior in integrated development environments. An analysis of the study can help define a research agenda as a starting point for the creation of a novel practical tool.</jats:p>",https://doi.org/10.15802/stp2021/242042,CrossRef
Software Engineering,Software Development Processes,Theoretical / Conceptual,Strategies for Software and Hardware Compatibility Testing in Industrial Controllers,"<jats:p>Mass customization, small batch sizes, high variability of product types and a changing product portfolio during the life cycle of an industrial plant are current trends in the industry. Due to an increasing decoupling of the development of software and hardware components in an industrial context, compatibility problems within industrial control systems arise more and more frequently. In this publication, a strategy concept for compatibility testing is derived and discussed by means of a literature review and applied research. This four-phase strategy concept identifies incompatibilities between software and hardware components in the industrial control environment and enables test engineers to detect problems at an early stage. By automating the compatibility test on an external I-PC, the test can be run both when new software is installed on the industrial controller and when the controller is restarted. Thus, changes to the components are constantly detected and incompatibilities are avoided. Furthermore, early incompatibility detection can ensure that a system remains permanently operational. Based on a discussion, additional strategies are identified to consolidate the robustness and applicability of the presented concept.</jats:p>",https://doi.org/10.3390/pr12030580,CrossRef
Software Engineering,Software Development Processes,Theoretical / Conceptual,Compliance checking of software processes: A systematic literature review,"<jats:title>Abstract</jats:title><jats:p>The processes used to develop software need to comply with normative requirements (e.g., standards and regulations) to align with the market and the law. Manual compliance checking is challenging because there are numerous requirements with changing nature and different purposes. Despite the importance of automated techniques, there is not any systematic study in this field. This lack may hinder organizations from moving toward automated compliance checking practices. In this paper, we characterize the methods for automatic compliance checking of software processes, including used techniques, potential impacts, and challenges. For this, we undertake a systematic literature review (SLR) of studies reporting methods in this field. As a result, we identify solutions that use different techniques (e.g., anthologies and metamodels) to represent processes and their artifacts (e.g., tasks and roles). Various languages, which have diverse capabilities for managing competing and changing norms, and agile strategies, are also used to represent normative requirements. Most solutions require tool‐support concretization and enhanced capabilities to handle processes and normative diversity. Our findings outline compelling areas for future research. In particular, there is a need to select suitable languages for consolidating a generic and normative‐agnostic solution, increase automation levels, tool support, and boost the application in practice by improving usability aspects.</jats:p>",https://doi.org/10.1002/smr.2440,CrossRef
Software Engineering,Software Development Processes,Theoretical / Conceptual,ТЕХНОЛОГИЯ РАЗРАБОТКИ МУЛЬТИПЛАТФОРМЕННЫХ ПРОГРАММ НА ОСНОВЕ ЯВНЫХ СХЕМ ПРОГРАММ,"<jats:p>В статье обосновывается необходимость разработки мультиплатформенных систем - таких, части которых работают на разных платформах. Описана концептуальная схема таких систем.</jats:p>
                                                                                                            <jats:p>The article underpins the devepolment of multiplatform software. Parts of the software in context operate on different platforms. A conceptual schema of the software is discussed.</jats:p>",https://doi.org/10.34706/de-2018-02-04,CrossRef
Software Engineering,Software Design and Architecture,Quantitative,"Applying empirical software engineering to software architecture:
  challenges and lessons learned","In the last 15 years, software architecture has emerged as an important
software engineering field for managing the development and maintenance of
large, software- intensive systems. Software architecture community has
developed numerous methods, techniques, and tools to support the architecture
process (analysis, design, and review). Historically, most advances in software
architecture have been driven by talented people and industrial experience, but
there is now a growing need to systematically gather empirical evidence about
the advantages or otherwise of tools and methods rather than just rely on
promotional anecdotes or rhetoric. The aim of this paper is to promote and
facilitate the application of the empirical paradigm to software architecture.
To this end, we describe the challenges and lessons learned when assessing
software architecture research that used controlled experiments, replications,
expert opinion, systematic literature reviews, obser- vational studies, and
surveys. Our research will support the emergence of a body of knowledge
consisting of the more widely-accepted and well-formed software architecture
theories.",http://arxiv.org/abs/1701.06000v1,arXiv
Software Engineering,Software Design and Architecture,Quantitative,Archify: A Recommender System of Architectural Design Decisions,"Software architectures play a critical role in software quality assurance.
However, small and medium companies (SMC) often suffer from the absence of
professionals with skills and expertise in software architecture. That
situation potentially affects the final quality of the software products and
pressures projects budget with extra costs with consulting. This paper presents
a recommender system of architectural design decisions called Archify. The goal
is to support SMC companies in part of the effort of architecturally designing
their products. Archify implements a wizard-styled interface that guides the
developer or project manager through a set of specific questions. While the
user answers these questions, Archify buffers a set of corresponding
architectural decision recommendations. As the final result, the system
recommends a set of architectural decisions matching the project's needs
according to the requirements (as provided by the user) of the software under
development. Nineteen professionals from academia and industry evaluated
Archify through two surveys. The findings reveal that 94.7% of the participants
approved Archify as a supporting tool. Respondents also highlighted the lack of
tools supporting software architecture design, remarking the relevance of the
proposed system.",http://arxiv.org/abs/2106.08115v1,arXiv
Software Engineering,Software Design and Architecture,Quantitative,"Architectural Approaches to Overcome Challenges in the Development of
  Data-Intensive Systems","Orientation of modern software systems towards data-intensive processing
raises new difficulties in software engineering on how to build and maintain
such systems. Some of the important challenges concern the design of software
architecture. In this article, we survey the fundamental challenges when
designing data-intensive computing systems and present some of the most popular
software architectural styles together with their potential to tackle these
challenges.",http://arxiv.org/abs/2312.03049v1,arXiv
Software Engineering,Software Design and Architecture,Quantitative,Software Architecture Metrics: a literature review,"In Software Engineering, early detection of architectural issues is key. It
helps mitigate the risk of poor performance, and lowers the cost of repairing
these issues. Metrics give a quick overview of the project which helps
designers with the detection of flaws or degradation in their architecture.
Even though studies unveiled architectural metrics more than 25 years ago, they
have not yet been embraced by the industry nor the open source community. In
this study, we aim at conducting a review of existing metrics focused on the
software architecture for evaluating quality, early in the design flow and
throughout the project's lifetime. We also give guidelines of their usage and
study their relevance in different contexts.",http://arxiv.org/abs/1901.09050v1,arXiv
Software Engineering,Software Design and Architecture,Quantitative,Towards physical laws for software architecture,"Starting from the pioneering works on software architecture precious
guidelines have emerged to indicate how computer programs should be organized.
For example the ""separation of concerns"" suggests to split a program into
modules that overlap in functionality as little as possible. However these
recommendations are mainly conceptual and are thus hard to express in a
quantitative form. Hence software architecture relies on the individual
experience and skill of the designers rather than on quantitative laws. In this
article I apply the methods developed for the classification of information on
the World-Wide-Web to study the organization of Open Source programs in an
attempt to establish the statistical laws governing software architecture.",http://arxiv.org/abs/1003.5455v1,arXiv
Software Engineering,Software Design and Architecture,Qualitative,"Software Architecture Decision-Making Practices and Challenges: An
  Industrial Case Study","Software architecture decision-making is critical to the success of a
software system as software architecture sets the structure of the system,
determines its qualities, and has far-reaching consequences throughout the
system life cycle. The complex nature of the software development context and
the importance of the problem has led the research community to develop several
techniques, tools, and processes to assist software architects in making better
decisions. Despite these effort, the adoption of such systematic approaches
appears to be quite limited in practice. In addition, the practitioners are
also facing new challenges as different software development methods suggest
different approaches for architecture design. In this paper, we study the
current software architecture decision-making practices in the industry using a
case study conducted among professional software architects in three different
companies in Europe. As a result, we identified different software architecture
decision-making practices followed by the software teams as well as their
reasons for following them, the challenges associated with them, and the
possible improvements from the software architects' point of view. Based on
that, we recognized that improving software architecture knowledge management
can address most of the identified challenges and would result in better
software architecture decision-making.",http://arxiv.org/abs/1610.09240v1,arXiv
Software Engineering,Software Design and Architecture,Qualitative,"Impact of requirements volatility on software architecture: How do
  software teams keep up with ever-changing requirements?","Requirements volatility is a major issue in software development, causing
problems such as higher defect density, project delays and cost overruns.
Software architecture that guides the overall vision of software product, is
one of the areas that is greatly affected by requirements volatility. Since
critical architecture decisions are made based on the requirements at hand,
changes in requirements can result signifiant changes in architecture. With the
wide adoption of agile software development, software architectures are
designed to accommodate possible future changes. However, the changes has to be
carefully managed as unnecessary and excessive changes can bring negative
consequences. An exploratory case study was conducted to study the impact of
requirements volatility on software architecture. Fifteen semi-structured,
thematic interviews were conducted in a European software company. The research
revealed poor communication, information distortion, and external dependencies
as the main factors that cause requirement volatility and inadequate
architecture documentation, inability to trace design rationale, and increased
complexity as the main implications of requirements volatility on software
architecture. Insights from software teams' awareness of the requirement
volatility, factors contribute to it, and possible ways to mitigate its
implications will be utilized to improve the management of requirement
volatility during software architecting process.",http://arxiv.org/abs/1904.08164v1,arXiv
Software Engineering,Software Design and Architecture,Qualitative,Software Architecture in Practice: Challenges and Opportunities,"Software architecture has been an active research field for nearly four
decades, in which previous studies make significant progress such as creating
methods and techniques and building tools to support software architecture
practice. Despite past efforts, we have little understanding of how
practitioners perform software architecture related activities, and what
challenges they face. Through interviews with 32 practitioners from 21
organizations across three continents, we identified challenges that
practitioners face in software architecture practice during software
development and maintenance. We reported on common software architecture
activities at software requirements, design, construction and testing, and
maintenance stages, as well as corresponding challenges. Our study uncovers
that most of these challenges center around management, documentation, tooling
and process, and collects recommendations to address these challenges.",http://arxiv.org/abs/2308.09978v2,arXiv
Software Engineering,Software Design and Architecture,Qualitative,A Qualitative Study of Architectural Design Issues in DevOps,"Software architecture is critical in succeeding with DevOps. However,
designing software architectures that enable and support DevOps (DevOps-driven
software architectures) is a challenge for organizations. We assert that one of
the essential steps towards characterizing DevOps-driven architectures is to
understand architectural design issues raised in DevOps. At the same time, some
of the architectural issues that emerge in the DevOps context (and their
corresponding architectural practices or tactics) may stem from the context
(i.e., domain) and characteristics of software organizations. To this end, we
conducted a mixed-methods study that consists of a qualitative case study of
two teams in a company during their DevOps transformation and a content
analysis of Stack Overflow and DevOps Stack Exchange posts to understand
architectural design issues in DevOps. Our study found eight specific and
contextual architectural design issues faced by the two teams and classified
architectural design issues discussed in Stack Overflow and DevOps Stack
Exchange into 11 groups. Our aggregated results reveal that the main
characteristics of DevOps-driven architectures are: being loosely coupled and
prioritizing deployability, testability, supportability, and modifiability over
other quality attributes. Finally, we discuss some concrete implications for
research and practice.",http://arxiv.org/abs/2108.06705v2,arXiv
Software Engineering,Software Design and Architecture,Qualitative,"Software Design Pattern Model and Data Structure Algorithm Abilities on
  Microservices Architecture Design in High-tech Enterprises","This study investigates the impact of software design model capabilities and
data structure algorithm abilities on microservices architecture design within
enterprises. Utilizing a qualitative methodology, the research involved
in-depth interviews with software architects and developers who possess
extensive experience in microservices implementation. The findings reveal that
organizations emphasizing robust design models and efficient algorithms achieve
superior scalability, performance, and flexibility in their microservices
architecture. Notably, participants highlighted that a strong foundation in
these areas facilitates better service decomposition, optimizes data
processing, and enhances system responsiveness. Despite these insights, gaps
remain regarding the integration of emerging technologies and the evolving
nature of software design practices. This paper contributes to the existing
literature by underscoring the critical role of these competencies in fostering
effective microservices architectures and suggests avenues for future research
to address identified gaps",http://arxiv.org/abs/2411.04143v1,arXiv
Software Engineering,Software Design and Architecture,Mixed Methods,"From Requirements to Architecture: An AI-Based Journey to
  Semi-Automatically Generate Software Architectures","Designing domain models and software architectures represents a significant
challenge in software development, as the resulting architectures play a vital
role in fulfilling the system's quality of service. Due to time pressure,
architects often model only one architecture based on their known limited
domain understanding, patterns, and experience instead of thoroughly analyzing
the domain and evaluating multiple candidates, selecting the best fitting.
Existing approaches try to generate domain models based on requirements, but
still require time-consuming manual effort to achieve good results. Therefore,
in this vision paper, we propose a method to generate software architecture
candidates semi-automatically based on requirements using artificial
intelligence techniques. We further envision an automatic evaluation and
trade-off analysis of the generated architecture candidates using, e.g., the
architecture trade-off analysis method combined with large language models and
quantitative analyses. To evaluate this approach, we aim to analyze the quality
of the generated architecture models and the efficiency and effectiveness of
our proposed process by conducting qualitative studies.",http://arxiv.org/abs/2401.14079v1,arXiv
Software Engineering,Software Design and Architecture,Mixed Methods,Code smells: A Synthetic Narrative Review,"Code smells are symptoms of poor design and implementation choices, which
might hinder comprehension, increase code complexity and fault-proneness and
decrease maintainability of software systems. The aim of our study was to
perform a triangulation of bibliometric and thematic analysis on code smell
literature production. The search was performed on Scopus (Elsevier,
Netherlands) database using the search string code smells which resulted in 442
publications. The Go-to statement was the first bad code smells identified in
software engineering history in 1968. The literature production trend has been
positive. The most productive countries were the United States, Italy and
Brazil. Eight research themes were identified: Managing software maintenance,
Smell detection-based software refactoring, Architectural smells, Improving
software quality with multi-objective approaches, Technical debt and its
instance, Quality improvement/assurance with mining software repositories,
Programming education, Integrating the concepts of anti-pattern, design defects
and design smells. Some research gaps also emerged, namely, New uncatalogued
smell identification; Smell propagation from architectural, design, code to
test, and other possible smells; and Identification of good smells. The results
of our study can help code smell researchers and practitioners understand the
broader aspects of code smells research and its translation to practice.",http://arxiv.org/abs/2103.01088v1,arXiv
Software Engineering,Software Design and Architecture,Mixed Methods,"The Presence and the State-of-Practice of Software Architects in the
  Brazilian Industry -- A Survey","Context: Software architecture intensely impacts the software quality.
Therefore, the professional assigned to carry out the design, maintenance and
evolution of architectures needs to have certain knowledge and skills in order
not to compromise the resulting application. Objective: The aim of this work is
to understand the characteristics of the companies regarding the presence or
absence of software architects in Brazil. Method: This work uses the Survey
research as a means to collect evidence from professionals with the software
architect profile, besides descriptive statistics and thematic analysis to
analyze the results. Results: The study collected data from 105 professionals
distributed in 24 Brazilian states. Results reveal that (i) not all companies
have a software architect, (ii) in some cases, other professionals perform the
activities of a software architect and (iii) there are companies that, even
having a software architecture professional, have other roles also performing
the duties of such a professional. Conclusions: Professionals hired as software
architects have higher salaries than those hired in other roles that carry out
such activity, although many of those other professionals still have duties
that are typical of software architects.",http://arxiv.org/abs/2403.00955v1,arXiv
Software Engineering,Software Design and Architecture,Mixed Methods,"Self-Confidence of Undergraduate Students in Designing Software
  Architecture","Software architecture students, often, lack self-confidence in their ability
to use their knowledge to design software architectures. This paper
investigates the relations between undergraduate software architecture
students' self-confidence and their course expectations, cognitive levels,
preferred learning methods, and critical thinking. We developed a questionnaire
with open-ended questions to assess the self-confidence levels and related
factors, which was taken by one-hundred ten students in two semesters. The
students answers were coded and analyzed afterward. We found that
self-confidence is weakly associated with the students' critical thinking and
independent from their cognitive levels, preferred learning methods, and
expectations from the course. The results suggest that to improve the
self-confidence of the students, the instructors should work on improving the
students' critical thinking capabilities.",http://arxiv.org/abs/2102.09905v3,arXiv
Software Engineering,Software Design and Architecture,Mixed Methods,"Let's Go to the Whiteboard (Again):Perceptions from Software Architects
  on Whiteboard Architecture Meetings","The whiteboard plays a crucial role in the day-to-day lives of software
architects, as they frequently will organize meetings at the whiteboard to
discuss a new architecture, some proposed changes to the architecture, a
mismatch between the architecture and the code, and more. While much has been
studied about software architects, the architectures they produce, and how they
produce them, a detailed understanding of these whiteboards meetings is still
lacking. In this paper, we contribute a mixed-methods study involving
semi-structured interviews and a subsequent survey to understand the
perceptions of software architects on whiteboard architecture meetings. We
focus on five aspects: (1) why do they hold these meetings, what is the impact
of the experience levels of the participants in these meetings, how do the
architects document the meetings, what kinds of changes are made after the
meetings have concluded and their results are moved to implementation, and what
role do digital whiteboards plays? In studying these aspects, we identify 12
observations related to both technical aspects and social aspects of the
meetings. These insights have implications for further research, offer concrete
advice to practitioners, provide guidance for future tool design, and suggest
ways of educating future software architects.",http://arxiv.org/abs/2210.16089v1,arXiv
Software Engineering,Software Design and Architecture,Design and Development,"Collaborative Design and Planning of Software Architecture Changes via
  Software City Visualization","Developers usually use diagrams and source code to jointly discuss and plan
software architecture changes. With this poster, we present our on-going work
on a novel approach that enables developers to collaboratively use software
city visualization to design and plan software architecture changes.",http://arxiv.org/abs/2408.16777v1,arXiv
Software Engineering,Software Design and Architecture,Design and Development,Applying Slicing Technique to Software Architectures,"Software architecture is receiving increasingly attention as a critical
design level for software systems. As software architecture design resources
(in the form of architectural specifications) are going to be accumulated, the
development of techniques and tools to support architectural understanding,
testing, reengineering, maintenance, and reuse will become an important issue.
This paper introduces a new form of slicing, named architectural slicing, to
aid architectural understanding and reuse. In contrast to traditional slicing,
architectural slicing is designed to operate on the architectural specification
of a software system, rather than the source code of a program. Architectural
slicing provides knowledge about the high-level structure of a software system,
rather than the low-level implementation details of a program. In order to
compute an architectural slice, we present the architecture information flow
graph which can be used to represent information flows in a software
architecture. Based on the graph, we give a two-phase algorithm to compute an
architectural slice.",http://arxiv.org/abs/cs/0105008v1,arXiv
Software Engineering,Software Design and Architecture,Design and Development,Top Down Approach: SIMULINK Mixed Hardware / Software Design,"System-level design methodologies have been introduced as a solution to
handle the design complexity of mixed Hardware / Software systems. In this
paper we describe a system-level design flow starting from Simulink
specification, focusing on concurrent hardware and software design and
verification at four different abstraction levels: System Simulink model,
Transaction Simulink model, Macro architecture, and micro architecture. We used
the MP3 CodeC application, to validate our approach and methodology.",http://arxiv.org/abs/1207.3872v1,arXiv
Software Engineering,Software Design and Architecture,Design and Development,Using Dependence Analysis to Support Software Architecture Understanding,"Software architecture is receiving increasingly attention as a critical
design level for software systems. As software architecture design resources
(in the form of architectural descriptions) are going to be accumulated, the
development of techniques and tools to support architectural understanding,
testing, reengineering, maintaining, and reusing will become an important
issue. In this paper we introduce a new dependence analysis technique, named
architectural dependence analysis to support software architecture development.
In contrast to traditional dependence analysis, architectural dependence
analysis is designed to operate on an architectural description of a software
system, rather than the source code of a conventional program. Architectural
dependence analysis provides knowledge of dependences for the high-level
architecture of a software system, rather than the low-level implementation
details of a conventional program.",http://arxiv.org/abs/cs/0105009v1,arXiv
Software Engineering,Software Design and Architecture,Design and Development,"Selection of Architecture Styles using Analytic Network Process for the
  Optimization of Software Architecture","The continuing process of software systems enlargement in size and complexity
becomes system design extremely important for software production. In this way,
the role of software architecture is significantly important in software
development. It serves as an evaluation and implementation plan for software
development and software evaluation. Consequently, choosing the correct
architecture is a critical issue in software engineering domain.
Moreover,software architecture selection is a multicriteria decision-making
problem in which different goals and objectives must be taken into
consideration. In this paper, more precise and suitable decisions in selection
of architecture styles have been presented by using ANP inference to support
decisions of software architects in order to exploit properties of styles in
the best way to optimize the design of software architecture.",http://arxiv.org/abs/1005.4271v1,arXiv
Software Engineering,Software Design and Architecture,Theoretical / Conceptual,How do Software Ecosystems Co-Evolve? A view from OpenStack and beyond,"Much research that analyzes the evolution of a software ecosystem is confined
to its own boundaries. Evidence shows, however, that software ecosystems
co-evolve independently with other software ecosystems. In other words,
understanding the evolution of a software ecosystem requires an especially
astute awareness of its competitive landscape and much consideration for other
software ecosystems in related markets. A software ecosystem does not evolve in
insulation but with other software ecosystems. In this research, we analyzed
the OpenStack software ecosystem with a focal perspective that attempted to
understand its evolution as a function of other software ecosystems. We
attempted to understand and explain the evolution of OpenStack in relation to
other software ecosystems in the cloud computing market. Our findings add to
theoretical knowledge in software ecosystems by identifying and discussing
seven different mechanisms by which software ecosystems mutually influence each
other: sedimentation and embeddedness of business relationships, strategic
management of the portfolio of business relationships, firms values and
reputation as a partner, core technological architecture, design of the APIs,
competitive replication of functionality and multi-homing. Research addressing
the evolution of software ecosystem should, therefore, acknowledge that
software ecosystems entangle with other software ecosystems in multiple ways,
even with competing ones. A rigorous analysis of the evolution of a software
ecosystem should not be solely confined to its inner boundaries.",http://arxiv.org/abs/1808.06663v1,arXiv
Software Engineering,Software Design and Architecture,Theoretical / Conceptual,"Architectural Support for Software Performance in Continuous Software
  Engineering: A Systematic Mapping Study","The continuous software engineering paradigm is gaining popularity in modern
development practices, where the interleaving of design and runtime activities
is induced by the continuous evolution of software systems. In this context,
performance assessment is not easy, but recent studies have shown that
architectural models evolving with the software can support this goal. In this
paper, we present a mapping study aimed at classifying existing scientific
contributions that deal with the architectural support for performance-targeted
continuous software engineering. We have applied the systematic mapping
methodology to an initial set of 215 potentially relevant papers and selected
66 primary studies that we have analyzed to characterize and classify the
current state of research. This classification helps to focus on the main
aspects that are being considered in this domain and, mostly, on the emerging
findings and implications for future research",http://arxiv.org/abs/2304.02489v1,arXiv
Software Engineering,Software Design and Architecture,Theoretical / Conceptual,"A Systematic Mapping Study on Contract-based Software Design for
  Dependable Systems","Background: Contract-based Design (CbD) is a valuable methodology for
software design that allows annotation of code and architectural components
with contracts, thereby enhancing clarity and reliability in software
development. It establishes rules that outline the behaviour of software
components and their interfaces and interactions. This modular approach enables
the design process to be segmented into smaller, independently developed,
tested, and verified system components, ultimately leading to more robust and
dependable software. Aim: Despite the significance and well-established
theoretical background of CbD, there is a need for a comprehensive systematic
mapping study for reliable software systems. Our study provides an
evidence-based overview of a method and demonstrates its practical feasibility.
Method: To conduct this study, we systematically searched three different
databases using specially formulated queries, which initially yielded 1,221
primary studies. After voting, we focused on 288 primary studies for more
detailed analysis. Finally, a collaborative review allowed us to gather
relevant evidence and information to address our research questions. Results:
Our findings suggest potential avenues for future research trajectories in CbD,
emphasising its role in improving the dependability of software systems. We
highlight maturity levels across different domains and identify areas that may
benefit from further research. Conclusion: Although CbD is a well-established
software design approach, a more comprehensive literature review is needed to
clarify its theoretical state about dependable systems. Our study addresses
this gap by providing a detailed overview of CbD from various perspectives,
identifying key gaps, and suggesting future research directions.",http://arxiv.org/abs/2505.07542v1,arXiv
Software Engineering,Software Design and Architecture,Theoretical / Conceptual,Towards a Theory on Architecting for Continuous Deployment,"Context: As the adoption of continuous delivery practices increases in
software organizations, different scenarios struggle to make it scales for
their products in long-term evolution. This study looks at the concrete
software architecture as a relevant factor for successfully achieving
continuous delivery goals. Objective: This study aims to understand how the
design of software architectures impacts the continuous deployment of their
software product. Method: We conducted a systematic literature review to
identify proper evidence regarding the research objective. We analyzed the
selected sources adopting a synthesis and analysis approach based on Grounded
Theory. Results: We selected 14 primary sources. Through our analysis process,
we developed a theory that explains the phenomenon of Architecting for
Continuous Deployment. The theory describes three other phenomena that support
Architecting for Continuous Deployment: Supporting Operations, Continuous
Evolution, and Improving Deployability. Furthermore, the theory comprises the
following elements: contexts, actions and interactions, quality attributes,
principles, and effects. We instantiated these elements and identified their
interrelationships. The theory is supported by providing bi-directional
traceability from the selected sources to the elements and vice-versa.
Conclusions: Developing adequate architecture plays a crucial role in enabling
continuous delivery. Supporting operations becomes vital to increase the
deployability and monitorability of software architecture. These two outcomes
require that developers accept responsibility for maintaining the operations.
The continuous evolution of the architecture is essential, but it must consider
balanced management of technical debt. Finally, improving deployability
requires attention to the test strategy and how it affects downtime to enable
efficient pipelines.",http://arxiv.org/abs/2108.09571v1,arXiv
Software Engineering,Software Design and Architecture,Theoretical / Conceptual,Towards a “non-disposable” software infrastructure for participation,"<jats:p>For many years now our research team has been involved in an effort (both theoretical and technological) that can be labeled as an attempt to investigate the notion of cooperation from the ‘participation’ or ‘contribution’ perspective. From our perspective, it encompasses a set of situations in which different actors identified or unidentified, ratified or not, distributed in space and time, contribute to a sometimes ill-defined collective goal, using most of the time low-overhead web-based technologies. Doing so, people participate to a collective design that aims at generating a bunch of perpetually moving collective knowledge and decisions submitted to discussion, negotiation and sometimes dismissal. To avoid the design of services as a repeated ‘one-shot’ process we have gradually built a transverse software infrastructure that can be used in various projects aiming to design participatory services using complex knowledge and cooperations. Thanks to this “non-disposable” infrastructure, the participatory services designed take profit from the scientific outcomes of each previous project.</jats:p>",https://doi.org/10.55612/s-5002-018-005,CrossRef
Software Engineering,Software Testing and Quality Assurance,Quantitative,Quality assurance for TTCN‐3 test specifications,"<jats:title>Abstract</jats:title><jats:p>Comprehensive testing of modern communication systems often requires large and complex test suites, which have to be maintained throughout the system life cycle. Industrial experience, with those written using the standardized <jats:italic>Testing and Test Control Notation</jats:italic> (TTCN‐3), has shown that this maintenance is a non‐trivial task and its burden can be reduced by means of appropriate concepts and tool support. To this aim, Motorola has collaborated with the University of Göttingen to develop TRex, an open‐source TTCN‐3 development environment, which notably provides suitable metrics and refactorings to enable the assessment and automatic restructuring of test suites. This article presents concepts like metrics and refactoring for the quality assurance of TTCN‐3 test suites and their implementation provided by the TRex tool. These means make it far easier to construct and maintain TTCN‐3 tests that are concise and optimally balanced with respect to maintainability quality characteristics. Copyright © 2008 John Wiley &amp; Sons, Ltd.</jats:p>",https://doi.org/10.1002/stvr.379,CrossRef
Software Engineering,Software Testing and Quality Assurance,Quantitative,First step in development and evaluation simultaneous determination of mycotoxins in cereals by liquid chromatography - mass spectrometry,"<jats:p>Currently, solid phase extraction (SPE) with immunoaffinity columns is applied in&amp;nbsp;most standardized methods for mycotoxin determination to purify extracts and analysis by&amp;nbsp;HPLC-FLD, HPLC-UV/VIS or LC-MS/MS. Therefore, sample preparation and analysis by&amp;nbsp;instruments are time-consuming and high operating costs. The novel method allow&amp;nbsp;simultaneously identify nine mycotoxin compounds with selective, stable and accurate&amp;nbsp;results. The new method has been evaluated through three stages including validation as&amp;nbsp;requirements of CEN/TR 16059:2010 (phase 1), comparison with current standard methods&amp;nbsp;(phase 2), evaluate the method using an interlaboratory comparison program (phase 3). Cereal samples were extracted by QuEChERS and analyzed by LC-MS/MS. The limit&amp;nbsp;of quantitation (LOQ) was 0,5 &amp;mu;g/kg for each aflatoxin compound and 40 &amp;mu;g/kg, 25 &amp;mu;g/kg,&amp;nbsp;1 &amp;mu;g/kg, 75 &amp;mu;g/kg for&amp;nbsp;deoxynivalenol, zearalenone, ochratoxin A, each toxin fumonisin (B1&amp;amp;B2), respectively. The recovery is in the range of 70-120%, relative standard deviation&amp;nbsp;RSD &amp;lt; 20%. The novel method also gives the same results compared to the individual&amp;nbsp;standardized methods, using the immunoaffinity column in the extraction stage. At the&amp;nbsp;present, the method is being evaluated through an interlaboratory comparison program with&amp;nbsp;two rounds: round 1 (for survey) and round 2 (official round), which is expected to be&amp;nbsp;implemented in 2022. &amp;nbsp;    </jats:p>",https://doi.org/10.47866/2615-9252/vjfc.3959,CrossRef
Software Engineering,Software Testing and Quality Assurance,Quantitative,A Quasi-Experimental Evaluation of Teaching Software Testing in Software Quality Assurance Subject during a Post-Graduate Computer Science Course,"<jats:p>Software testing is regarded as a key activity in the software development cycle, as it helps information technology professionals to design good quality software. Thus, this is an essential activity for the software industry, although with all its nuances high priority is still not being given to learning about it at an academic level. The purpose of this work is to investigate a teaching strategy for software testing which involves acquiring academic skills within a curriculum based on active teaching methodologies. A teaching model was designed for this to coordinate the different areas of a subject, and then a controlled quasi-experiment was carried out in a post-graduate course to evaluate the application of this model. The results obtained demonstrate that there was a considerable learning gain in the experimental group that adopted the teaching approach when compared with the control group that relied on a traditional approach. The student t-test was employed to determine the learning efficiency.</jats:p>",https://doi.org/10.3991/ijet.v17i05.25673,CrossRef
Software Engineering,Software Testing and Quality Assurance,Quantitative,Assuring the Quality of Data Through Laboratory Quality Assurance,"<jats:title>Abstract</jats:title>
               <jats:p>Assuring the quality of data produced by analytical chemistry laboratories is important, not only because analytical chemists have a professional obligation to do so for those using their data, but also because of today's climate of regulations and high public concern over issues involving analytical laboratories. Assurance goes beyond the technical adequacy of methods used to make analyses and the control of the measurement process through calibration and statistical control. It concerns also overall control of operations through various administrative practices. The combination of technical and administrative practices constitutes a quality assurance program that will help assure the quality of data. Such a quality assurance program is described, which is based on nine elements of laboratory quality assurance derived from a nationally established standard on quality assurance. These nine elements are the basis for a new ASTM standard, Standard Guide for Establishing a Quality Assurance Program for Analytical Chemistry Laboratories within the Nuclear Industry (C 1009).</jats:p>",https://doi.org/10.1520/jte10726j,CrossRef
Software Engineering,Software Testing and Quality Assurance,Quantitative,Methodology for Quality Assurance of Educational Software,"<jats:p>Evaluating the quality of educational software is a priority given the number of educational systems currently being produced. The article carries out a documentary analysis to search for documents that have addressed these elements. The results address the main documents that have been obtained. Subsequently, a methodology containing a system of metrics to evaluate the quality of educational software is proposed.
</jats:p>",https://doi.org/10.32388/gictbh,CrossRef
Software Engineering,Software Testing and Quality Assurance,Qualitative,The value of a proper software quality assurance methodology,"<jats:p>This paper describes the experiences of a project development team during an attempt to ensure the quality of a new software product. This product was created by a team of software engineers at Digital Equipment Corporation, a mainframe manufacturer. As a result, the definition of “to ensure the quality of a software product” meant minimizing the maintenance costs of the new product. Ease of maintenance and a low bug rate after release to the customer were very important goals from the beginning of the project.</jats:p>
          <jats:p>This paper compares the degree of application and resultant effects of several software quality assurance methodologies upon different parts of the final product. Many of the product's subsystems were created using all of the discussed methodologies rigorously. Some subsystems were created with little or no use of the methodologies. Other subsystems used a mixture. The observed quality of the various subsystems when related to the methodology used to create them provides insights into the interactions between the methodologies. These observations also supply additional experience to reinforce established beliefs concerning the value of quality assurance methodologies.</jats:p>",https://doi.org/10.1145/953579.811118,CrossRef
Software Engineering,Software Testing and Quality Assurance,Qualitative,A Customized Quality Model for Software Quality Assurance in Agile Environment,"<p>The agile approach grew dramatically over traditional approaches. The methodology focuses more on rapid development, quick evaluation, quantifiable progress and continuous delivery satisfying the customer desire. In view of this, there is a need for measurement of the agile development process. In this respect, the present research work investigates the inter-relationships and inter-dependencies between the identified quality factors (QF), thereby outlining which of these QF have high driving power and dependence power, working indirectly towards the success of agile development process. This paper proposes a new agile quality model, utilizing an interpretive structural modeling (ISM) approach and the identified factors are classifies using Matriced' Impacts Croise's Multiplication Applique´e a UN Classement (MICMAC) approach. The research findings can significantly impact agile development process by understanding how these QF related to each other and how they can be adopted.</p>",https://doi.org/10.4018/ijitwe.2019070104,CrossRef
Software Engineering,Software Testing and Quality Assurance,Qualitative,"Risk Management, Quality Assurance, and Health Care Policy Dilemmas","<jats:p> Editor's Note: The following article, coauthored by the QAUR Case Study Editor, and Reginald F. Wells, Ph.D. (2.) is presented to demonstrate the utility of risk management when, as a component of a comprehensive QA initiative, it is applied to critical policy dilemmas facing health care providers today. The practical application of RM in formulating an AIDS policy for a public MRDD service agency is outlined as a strategic example-the ""case study"" if you will. Paper was presented by the authors at the 112th Annual Meeting of the AAMR, Washington Hilton Hotel, Washington, D. C. </jats:p>",https://doi.org/10.1177/0885713x8900400206,CrossRef
Software Engineering,Software Testing and Quality Assurance,Qualitative,A Performance Appraisal System for Quality Assurance and Utilization Review Fellows,"<jats:p> There is now increased interest in developing con tinuing education and career training progams in quality assurance and utilization review. These pro grams frequently involve on-site practicum experi ences for which the performance appraisal system described here was developed. Incorporating a new evaluation technology known as the rating grid, this system is discussed here in terms of its underlying rationale, its rating factors, and the observation and scoring factors used by the observer-evaluator. The system can be used by observers to give feedback to physicians and others undergoing training in quality assurance and utilization review. </jats:p>",https://doi.org/10.1177/0885713x8900400303,CrossRef
Software Engineering,Software Testing and Quality Assurance,Qualitative,Quality assurance and student work experience,"<jats:p>This article is based on the premise that owing to the substantial attention given to quality assurance and related initiatives in higher education in recent years, the management, and related practices involved, of student work experience will have improved. To investigate this hypothesis a comparative analysis of the findings of two major research projects, involving similar methods, into student work experience is undertaken. The two studies are discussed, criteria for comparative analysis are identified and key findings presented. This leads to the conclusion, that while there is evidence of substantial improvement in the probability of students gaining experience in the preparation of curricula vitae and of interview situations, little progress has been made in enhancing the realisation of the many other benefits attributable to student work experience. Recommendations to address the identified weaknesses in the system are proposed.</jats:p>",https://doi.org/10.1108/09684889910297712,CrossRef
Software Engineering,Software Testing and Quality Assurance,Mixed Methods,Software quality: A Historical and Synthetic Content Analysis,"Interconnected computers and software systems have become an indispensable
part of people's lives, therefore software quality research is becoming more
and more important. There have been multiple attempts to synthesize knowledge
gained in software quality research, however, they were focused mainly on
single aspects of software quality and not to structure the knowledge in a
holistic way. The aim of our study was to close this gap. The software quality
publications were harvested from the Scopus bibliographic database. The
metadata was exported first to CRexlporer, which was employed to identify
historical roots, and next to VOSViewer, which was used as a part of the
synthetic content analysis. In our study we defined synthetic context analysis
as a triangulation of bibliometrics and content analysis. Our search resulted
in 14451 publications. The performance bibliometric study showed that the
production of research publications relating to software quality is currently
following an exponential growth trend and that the software quality research
community is growing. The most productive country was the United States and the
most productive Institution The Florida Atlantic University. The synthetic
content analysis revealed that the published knowledge can be structured into
10 themes, the most important being the themes regarding software quality
improvement with enhancing software engineering, advanced software testing, and
improved defect and fault prediction with machine learning and data mining.
According to the analysis of the hot topics, it seems that future research will
be directed into developing and using a full specter of new artificial
intelligence tools (not just machine learning and data mining) and focusing on
how to assure software quality in agile development paradigms.",http://arxiv.org/abs/2106.14598v1,arXiv
Software Engineering,Software Testing and Quality Assurance,Mixed Methods,Product Backlog Rating: A Case Study On Measuring Test Quality In Scrum,"Agile software development methodologies focus on software projects which are
behind schedule or highly likely to have a problematic development phase. In
the last decade, Agile methods have transformed from cult techniques to
mainstream methodologies. Scrum, an Agile software development method, has been
widely adopted due to its adaptive nature.
  This paper presents a metric that measures the quality of the testing process
in a Scrum process. As product quality and process quality correlate, improved
test quality can ensure high quality products. Also, gaining experience from
eight years of successful Scrum implementation at SoftwarePeople, we describe
the Scrum process emphasizing the testing process. We propose a metric Product
Backlog Rating (PBR) to assess the testing process in Scrum. PBR considers the
complexity of the features to be developed in an iteration of Scrum, assesses
test ratings and offers a numerical score of the testing process. This metric
is able to provide a comprehensive overview of the testing process over the
development cycle of a product.
  We present a case study which shows how the metric is used at SoftwarePeople.
The case study explains some features that have been developed in a Sprint in
terms of feature complexity and potential test assessment difficulties and
shows how PBR is calculated during the Sprint. We propose a test process
assessment metric that provides insights into the Scrum testing process.
However, the metric needs further evaluation considering associated resources
(e.g., quality assurance engineers, the length of the Scrum cycle).",http://arxiv.org/abs/1310.2545v2,arXiv
Software Engineering,Software Testing and Quality Assurance,Mixed Methods,Quality Assurance Practices in Agile Methodology,"The complexity of software is increasing day by day the requirement and need
for a verity of softwareproducts increases, this necessitates the provision of
a strong tool that will make a balance betweenproduction and quality. The
practice of applying software metrics to the development process and to
asoftware product is a critical task and crucial enough that requires study and
discipline and whichbrings knowledge of the status of the process and/or
product of software in regards to the goals toachieve, this discipline is known
as quality assurance which is the key factor behind the success ofevery
software engineering project, the quality assurance activities are what result
in the qualitativeproduct as well as the process in both conventional software
development methodology and agilemethodology. However, agile methodology is now
becoming one of the dominant method adopted bymost of the software industries
because it allows developing of software with very limited requirementand
supports rapid changes in the requirement, the method may produce the product
very fast but wemight not guarantee the quality of the product unless we apply
the SQA activities to the process. Thisresearch paper aimed to study the
quality assurance activities practice in agile software developmentmethodology,
investigate the common problems and key drivers of quality in agile, and
propose asolution to improve the practice of SQA in agile methodology by
analyzing the parameters that assurequality in agile software.",http://arxiv.org/abs/2411.05134v1,arXiv
Software Engineering,Software Testing and Quality Assurance,Mixed Methods,A Hybrid Software Test Automation for Educational Portals,"Educational portal (EP) is a multi-function website that allows access to
activities such as public and private sections, data retrieval and submission,
personalized content and so on for the educational system. This study
investigated the specific requirement for the enhancement of quality and
behavior of EP with regards to time and cost using Obafemi Awolowo University
(OAU), Ile-Ife, Nigeria as a case study. A test automation framework was
designed using unified modelling language and implemented in Java programming
language. MySQL and Excel database were used to store test data. The framework
developed was evaluated using Test Time Performance (TTP), Performance Test
Efficiency (PTE) and Automation Scripting Productivity (ASP) metrics. The
results from the evaluation of the sample data provided showed that ASP
produced a tested outcome of 360 operations per hour, PTE yielded 80% and TTP
was just 4%. Based on the recorded performance, it is evident that the research
can provide quick and firsthand information to quality assurance analyst and
software testers, thereby reducing maintenance cost during software
development.",http://arxiv.org/abs/2111.00222v1,arXiv
Software Engineering,Software Testing and Quality Assurance,Mixed Methods,Optical quality assurance of GEM foils,"An analysis software was developed for the high aspect ratio optical scanning
system in the Detec- tor Laboratory of the University of Helsinki and the
Helsinki Institute of Physics. The system is used e.g. in the quality assurance
of the GEM-TPC detectors being developed for the beam diagnostics system of the
SuperFRS at future FAIR facility. The software was tested by analyzing five
CERN standard GEM foils scanned with the optical scanning system. The
measurement uncertainty of the diameter of the GEM holes and the pitch of the
hole pattern was found to be 0.5 {\mu}m and 0.3 {\mu}m, respectively. The
software design and the performance are discussed. The correlation between the
GEM hole size distribution and the corresponding gain variation was studied by
comparing them against a detailed gain mapping of a foil and a set of six lower
precision control measurements. It can be seen that a qualitative estimation of
the behavior of the local variation in gain across the GEM foil can be made
based on the measured sizes of the outer and inner holes.",http://arxiv.org/abs/1704.06691v1,arXiv
Software Engineering,Software Testing and Quality Assurance,Design and Development,Hybrid software testing model to improve software quality assurance,"<jats:p>Software testing is a very critical factor in determining the quality, robustness and reliability of applications. Testing methods have advanced to accommodate the various software development practices that are being churned out at frequent rates and representing complex systems. One such method is the hybrid software testing model, which combines the strengths of different testing techniques to achieve comprehensive testing coverage. This study provides an in-depth analysis of the hybrid software testing model, exploring its definition, benefits, challenges, and best practices. Furthermore, it discusses various testing techniques commonly used in hybrid testing and presents case studies showcasing the successful implementation of the hybrid model in real-world scenarios.</jats:p>",https://doi.org/10.30574/gjeta.2023.17.1.0206,CrossRef
Software Engineering,Software Testing and Quality Assurance,Design and Development,Study on CMM-Based Software Quality Assurance Process Improvement - A Case of the Educational Software Quality Assurance Model,"<jats:p>This paper mainly make a theoretical research and exploration on software quality assurance quality assurance improvement based on CMM process, with the educational software quality assurance model as an example. It elucidates the relationship between educational software process improvement and quality assurance, and explicits the importance of educational software development process improvement to the quality of educational software. Additionally, it discussed the establishment of educational software development model on the basis of the waterfall model of traditional software development, and construction of process quality management models and platforms based on CMM educational software process improvement.</jats:p>",https://doi.org/10.4028/www.scientific.net/amr.1049-1050.2032,CrossRef
Software Engineering,Software Testing and Quality Assurance,Design and Development,Generative AI for software testing: Harnessing large language models for automated and intelligent quality assurance,"<jats:p>Software testing is indispensable for ensuring that modern applications meet rigorous standards of functionality, reliability, and security. However, the complexity and pace of contemporary software development often overwhelm traditional and even AI-based testing approaches, leading to gaps in coverage, delayed feedback, and increased maintenance costs. Recent breakthroughs in Generative AI, particularly Large Language Models (LLMs), offer a new avenue for automating and optimizing testing processes. These models can dynamically generate test cases, predict system vulnerabilities, handle continuous software changes, and reduce the burden on human testers. This paper explores how Generative AI complements and advances established AI-driven testing frameworks, outlines the associated challenges of data preparation and governance, and proposes future directions for fully autonomous, trustworthy testing solutions.</jats:p>",https://doi.org/10.30574/ijsra.2025.14.1.0266,CrossRef
Software Engineering,Software Testing and Quality Assurance,Design and Development,Pattern‐based GUI testing: Bridging the gap between design and quality assurance,"<jats:title>Summary</jats:title><jats:p>Software systems with a graphical user interface (GUI) front end are typically designed using user interface (UI) Patterns, which describe generic solutions (with multiple possible implementations) for recurrent GUI design problems. However, existing testing techniques do not take advantage of this fact to test GUIs more efficiently. In this paper, we present a new pattern‐based GUI testing (PBGT) approach that formalizes the notion of UI Test Patterns, which are generic test strategies to test UI patterns over their different implementations. The PBGT approach is evaluated via 2 case studies. The first study involves 2 fielded Web application subjects; findings show that PBGT is both practical and useful, as testing teams were able to find real bugs in a reasonable time interval. The second study allows deeper analysis by studying software subjects seeded with artificial faults; the findings show that PBGT is more effective than a manual model‐based test case generation approach.</jats:p>",https://doi.org/10.1002/stvr.1629,CrossRef
Software Engineering,Software Testing and Quality Assurance,Design and Development,"A Multimodal Approach to Software Quality Assurance: Integrating Static Analysis, Dynamic Testing, and AI-based Anomaly Detection","<jats:p>: The combination of software architecture evolutions and cloud computing and cyber-physical systems
creates advanced complexity when ensuring software reliability and security and efficiency. The once typical software
quality assurance (SQA) practices using manual reviews and isolated testing methods fail to provide acceptable modern
results anymore. This study develops a multimodal software quality assurance enhancement approach which combines
static analysis together with dynamic testing and AI anomaly detection techniques. Software quality examines both
potential defects alongside security vulnerabilities through code-level static analysis before running the program while
dynamic testing evaluates real-time functionalities and security features. AI-based anomaly detection systems develop
through machine learning models which help software testing teams by predicting failures as well as detecting security
threats and adjusting testing strategies in real-time. When these technologies work together it reduces undetected
defects while attaining higher software security and quality levels and decreasing testing requirements. The paper
explores implementation barriers together with ethical matters and evolving AI-powered software testing patterns while
discussing the future trajectory of automated predictive and adaptive SQA methods</jats:p>",https://doi.org/10.15680/ijircce.2024.1202003,CrossRef
Software Engineering,Software Testing and Quality Assurance,Theoretical / Conceptual,"Adapting Quality Assurance to Adaptive Systems: The Scenario Coevolution
  Paradigm","From formal and practical analysis, we identify new challenges that
self-adaptive systems pose to the process of quality assurance. When tackling
these, the effort spent on various tasks in the process of software engineering
is naturally re-distributed. We claim that all steps related to testing need to
become self-adaptive to match the capabilities of the self-adaptive
system-under-test. Otherwise, the adaptive system's behavior might elude
traditional variants of quality assurance. We thus propose the paradigm of
scenario coevolution, which describes a pool of test cases and other
constraints on system behavior that evolves in parallel to the (in part
autonomous) development of behavior in the system-under-test. Scenario
coevolution offers a simple structure for the organization of adaptive testing
that allows for both human-controlled and autonomous intervention, supporting
software engineering for adaptive systems on a procedural as well as technical
level.",http://arxiv.org/abs/1902.04694v1,arXiv
Software Engineering,Software Testing and Quality Assurance,Theoretical / Conceptual,"Quality Assurance Challenges for Machine Learning Software Applications
  During Software Development Life Cycle Phases","In the past decades, the revolutionary advances of Machine Learning (ML) have
shown a rapid adoption of ML models into software systems of diverse types.
Such Machine Learning Software Applications (MLSAs) are gaining importance in
our daily lives. As such, the Quality Assurance (QA) of MLSAs is of paramount
importance. Several research efforts are dedicated to determining the specific
challenges we can face while adopting ML models into software systems. However,
we are aware of no research that offered a holistic view of the distribution of
those ML quality assurance challenges across the various phases of software
development life cycles (SDLC). This paper conducts an in-depth literature
review of a large volume of research papers that focused on the quality
assurance of ML models. We developed a taxonomy of MLSA quality assurance
issues by mapping the various ML adoption challenges across different phases of
SDLC. We provide recommendations and research opportunities to improve SDLC
practices based on the taxonomy. This mapping can help prioritize quality
assurance efforts of MLSAs where the adoption of ML models can be considered
crucial.",http://arxiv.org/abs/2105.01195v2,arXiv
Software Engineering,Software Testing and Quality Assurance,Theoretical / Conceptual,"A Roadmap for Software Testing in Open Collaborative Development
  Environments","Amidst the ever-expanding digital sphere, the evolution of the Internet has
not only fostered an atmosphere of information transparency and sharing but has
also sparked a revolution in software development practices. The distributed
nature of open collaborative development, along with its diverse contributors
and rapid iterations, presents new challenges for ensuring software quality.
This paper offers a comprehensive review and analysis of recent advancements in
software quality assurance within open collaborative development environments.
Our examination covers various aspects, including process management, personnel
dynamics, and technological advancements, providing valuable insights into
effective approaches for maintaining software quality in such collaborative
settings. Furthermore, we delve into the challenges and opportunities arising
from emerging technologies such as LLMs and the AI model-centric development
paradigm. By addressing these topics, our study contributes to a deeper
understanding of software quality assurance in open collaborative environments
and lays the groundwork for future exploration and innovation.",http://arxiv.org/abs/2406.05438v2,arXiv
Software Engineering,Software Testing and Quality Assurance,Theoretical / Conceptual,Reframing the Test Pyramid for Digitally Transformed Organizations,"The test pyramid is a conceptual model that describes how quality checks can
be organized to ensure coverage of all components of a system, at all scales.
Originally conceived to help aerospace engineers plan tests to determine how
material changes impact system integrity, the concept was gradually introduced
into software engineering. Today, the test pyramid is typically used to
illustrate that the majority of tests should be performed at the lowest (unit
test) level, with fewer integration tests, and even fewer acceptance tests
(which are the most expensive to produce, and the slowest to execute). Although
the value of acceptance tests and integration tests increasingly depends on the
integrity of the underlying data, models, and pipelines, software development
and data management organizations have traditionally been siloed and quality
assurance practice is not as mature in data operations as it is for software.
Companies that close this gap by developing cross-organizational systems will
create new competitive advantage and differentiation. By taking a more holistic
view of testing that crosses these boundaries, practitioners can help their
organizations close the gap.",http://arxiv.org/abs/2011.00655v1,arXiv
Software Engineering,Software Testing and Quality Assurance,Theoretical / Conceptual,Ontology Reuse: the Real Test of Ontological Design,"Reusing ontologies in practice is still very challenging, especially when
multiple ontologies are (jointly) involved. Moreover, despite recent advances,
the realization of systematic ontology quality assurance remains a difficult
problem. In this work, the quality of thirty biomedical ontologies, and the
Computer Science Ontology are investigated, from the perspective of a practical
use case. Special scrutiny is given to cross-ontology references, which are
vital for combining ontologies. Diverse methods to detect potential issues are
proposed, including natural language processing and network analysis. Moreover,
several suggestions for improving ontologies and their quality assurance
processes are presented. It is argued that while the advancing automatic tools
for ontology quality assurance are crucial for ontology improvement, they will
not solve the problem entirely. It is ontology reuse that is the ultimate
method for continuously verifying and improving ontology quality, as well as
for guiding its future development. Specifically, multiple issues can be found
and fixed primarily through practical and diverse ontology reuse scenarios.",http://arxiv.org/abs/2205.02892v2,arXiv
Software Engineering,Requirements Engineering,Quantitative,Analysing the Assumed Benefits of Software Requirements,"Often during the requirements engineering (RE) process, the value of a
requirement is assessed, e.g., in requirement prioritisation, release planning,
and trade-off analysis. In order to support these activities, this research
evaluates Goal Oriented Requirements Engineering (GORE) methods for the
description of a requirement's value. Specifically, we investigate the
goal-to-goal contribution relationship for its ability to demonstrate the value
of a requirement, and propose that it is enriched with concepts such as
correlation, confidence, and utility.",http://arxiv.org/abs/1305.3853v2,arXiv
Software Engineering,Requirements Engineering,Quantitative,"A holistic look at requirements engineering practices in the gaming
  industry","In this work we present an account of the status of requirements engineering
in the gaming industry. Recent papers in the area were surveyed.
Characterizations of the gaming industry were deliberated upon by portraying
its relations with the market industry. Some research directions in the area of
requirements engineering in the gaming industry were also mentioned.",http://arxiv.org/abs/1811.03482v1,arXiv
Software Engineering,Requirements Engineering,Quantitative,Emotions in Requirements Engineering: A Systematic Mapping Study,"The purpose of requirements engineering (RE) is to make sure that the
expectations and needs of the stakeholders of a software system are met.
Emotional needs can be captured as emotional requirements that represent how
the end user should feel when using the system. Differently from functional and
quality (non-functional) requirements, emotional requirements have received
relatively less attention from the RE community. This study is motivated by the
need to explore and map the literature on emotional requirements. The study
applies the systematic mapping study technique for surveying and analyzing the
available literature to identify the most relevant publications on emotional
requirements. We identified 34 publications that address a wide spectrum of
practices concerned with engineering emotional requirements. The identified
publications were analyzed with respect to the application domains, instruments
used for eliciting and artefacts used for representing emotional requirements,
and the state of the practice in emotion-related requirements engineering. This
analysis serves to identify research gaps and research directions in
engineering emotional requirements. To the best of the knowledge by the
authors, no other similar study has been conducted on emotional requirements.",http://arxiv.org/abs/2305.16091v1,arXiv
Software Engineering,Requirements Engineering,Quantitative,"Requirements engineering current practice and capability in small and
  medium software development enterprises in New Zealand","This paper presents research on current industry practices with respect to
requirements engineering as implemented within software development companies
in New Zealand. A survey instrument is designed and deployed. The results are
analysed and compared against what is internationally considered ""best
practice"" and previous New Zealand and Australian studies. An attempt is made
to assess the requirements engineering capability of New Zealand companies
using both formal and informal frameworks.",http://arxiv.org/abs/1407.6102v1,arXiv
Software Engineering,Requirements Engineering,Quantitative,"Quality Requirements for Code: On the Untapped Potential in
  Maintainability Specifications","Quality requirements are critical for successful software engineering, with
maintainability being a key internal quality. Despite significant attention in
software metrics research, maintainability has attracted surprisingly little
focus in the Requirements Engineering (RE) community. This position paper
proposes a synergistic approach, combining code-oriented research with RE
expertise, to create meaningful industrial impact. We introduce six
illustrative use cases and propose three future research directions.
Preliminary findings indicate that the established QUPER model, designed for
setting quality targets, does not adequately address the unique aspects of
maintainability.",http://arxiv.org/abs/2401.10833v1,arXiv
Software Engineering,Requirements Engineering,Qualitative,Theory of Regulatory Compliance for Requirements Engineering,"Regulatory compliance is increasingly being addressed in the practice of
requirements engineering as a main stream concern. This paper points out a gap
in the theoretical foundations of regulatory compliance, and presents a theory
that states (i) what it means for requirements to be compliant, (ii) the
compliance problem, i.e., the problem that the engineer should resolve in order
to verify whether requirements are compliant, and (iii) testable hypotheses
(predictions) about how compliance of requirements is verified. The theory is
instantiated by presenting a requirements engineering framework that implements
its principles, and is exemplified on a real-world case study.",http://arxiv.org/abs/1002.3711v1,arXiv
Software Engineering,Requirements Engineering,Qualitative,"Aspects of Modelling Requirements in Very-Large Agile Systems
  Engineering","Using models for requirements engineering (RE) is uncommon in systems
engineering, despite the widespread use of model-based engineering in general.
One reason for this lack of use is that formal models do not match well the
trend to move towards agile developing methods. While there exists work that
investigates challenges in the adoption of requirements modeling and agile
methods in systems engineering, there is a lack of work studying successful
approaches of using requirements modelling in agile systems engineering. To
address this gap, we conducted a case study investigating the application of
requirements models at Ericsson AB, a Swedish telecommunications company. We
studied a department using requirements models to bridge agile development and
plan-driven development aspects. We find that models are used to understand how
requirements relate to each other, and to keep track with the product's
evolution. To cope with the effort to maintain models over time, study
participants suggest to rely on text-based notations that bring the models
closer to developers and allow integration into existing software development
workflows. This results in tool trade-offs, e.g., losing the possibility to
control diagram layout.",http://arxiv.org/abs/2209.01993v1,arXiv
Software Engineering,Requirements Engineering,Qualitative,ReXCL: A Tool for Requirement Document Extraction and Classification,"This paper presents the ReXCL tool, which automates the extraction and
classification processes in requirement engineering, enhancing the software
development lifecycle. The tool features two main modules: Extraction, which
processes raw requirement documents into a predefined schema using heuristics
and predictive modeling, and Classification, which assigns class labels to
requirements using adaptive fine-tuning of encoder-based models. The final
output can be exported to external requirement engineering tools. Performance
evaluations indicate that ReXCL significantly improves efficiency and accuracy
in managing requirements, marking a novel approach to automating the
schematization of semi-structured requirement documents.",http://arxiv.org/abs/2504.07562v1,arXiv
Software Engineering,Requirements Engineering,Qualitative,"Identifying relevant Factors of Requirements Quality: an industrial Case
  Study","[Context and Motivation]: The quality of requirements specifications impacts
subsequent, dependent software engineering activities. Requirements quality
defects like ambiguous statements can result in incomplete or wrong features
and even lead to budget overrun or project failure. [Problem]: Attempts at
measuring the impact of requirements quality have been held back by the vast
amount of interacting factors. Requirements quality research lacks an
understanding of which factors are relevant in practice. [Principal Ideas and
Results]: We conduct a case study considering data from both interview
transcripts and issue reports to identify relevant factors of requirements
quality. The results include 17 factors and 11 interaction effects relevant to
the case company. [Contribution]: The results contribute empirical evidence
that (1) strengthens existing requirements engineering theories and (2)
advances industry-relevant requirements quality research.",http://arxiv.org/abs/2402.00594v2,arXiv
Software Engineering,Requirements Engineering,Qualitative,"A multi-case study of agile requirements engineering and the use of test
  cases as requirements","Context: It is an enigma that agile projects can succeed 'without
requirements' when weak requirements engineering is a known cause for project
failures. While agile development projects often manage well without extensive
requirements test cases are commonly viewed as requirements and detailed
requirements are documented as test cases. Objective: We have investigated this
agile practice of using test cases as requirements to understand how test cases
can support the main requirements activities, and how this practice varies.
Method: We performed an iterative case study at three companies and collected
data through 14 interviews and two focus groups. Results: The use of test cases
as requirements poses both benefits and challenges when eliciting, validating,
verifying, and managing requirements, and when used as a documented agreement.
We have identified five variants of the test-cases-as-requirements practice,
namely de facto, behaviour-driven, story-test driven, stand-alone strict and
stand-alone manual for which the application of the practice varies concerning
the time frame of requirements documentation, the requirements format, the
extent to which the test cases are a machine executable specification and the
use of tools which provide specific support for the practice of using test
cases as requirements. Conclusions: The findings provide empirical insight into
how agile development projects manage and communicate requirements. The
identified variants of the practice of using test cases as requirements can be
used to perform in-depth investigations into agile requirements engineering.
Practitioners can use the provided recommendations as a guide in designing and
improving their agile requirements practices based on project characteristics
such as number of stakeholders and rate of change.",http://arxiv.org/abs/2308.11747v1,arXiv
Software Engineering,Requirements Engineering,Mixed Methods,Crowd-based requirements elicitation via pull feedback: method and case studies,"<jats:title>Abstract</jats:title><jats:p>Crowd-based Requirements Engineering (CrowdRE) promotes the active involvement of a large number of stakeholders in RE activities. A prominent strand of CrowdRE research concerns the creation and use of online platforms for a crowd of stakeholders to formulate ideas, which serve as an additional input for requirements elicitation. Most of the reported case studies are of small size, and they analyze the size of the crowd, rather than the quality of the collected ideas. By means of an iterative design that includes three case studies conducted at two organizations, we present the CREUS method for crowd-based elicitation via user stories. Besides reporting the details of these case studies and quantitative results on the number of participants, ideas, votes, etc., a key contribution of this paper is a qualitative analysis of the elicited ideas. To analyze the quality of the user stories, we apply criteria from the Quality User Story framework, we calculate automated text readability metrics, and we check for the presence of vague words. We also study whether the user stories can be linked to software qualities, and the specificity of the ideas. Based on the results, we distill six key findings regarding CREUS and, more generally, for CrowdRE via pull feedback.</jats:p>",https://doi.org/10.1007/s00766-022-00384-6,CrossRef
Software Engineering,Requirements Engineering,Mixed Methods,"Causality in requirements artifacts: prevalence, detection, and impact","<jats:title>Abstract</jats:title><jats:p>Causal relations in natural language (NL) requirements convey strong, semantic information. Automatically extracting such causal information enables multiple use cases, such as test case generation, but it also requires to reliably detect causal relations in the first place. Currently, this is still a cumbersome task as causality in NL requirements is still barely understood and, thus, barely detectable. In our empirically informed research, we aim at better understanding the notion of causality and supporting the automatic extraction of causal relations in NL requirements. In a first case study, we investigate 14.983 sentences from 53 requirements documents to understand the extent and form in which causality occurs. Second, we present and evaluate a tool-supported approach, called CiRA, for causality detection. We conclude with a second case study where we demonstrate the applicability of our tool and investigate the impact of causality on NL requirements. The first case study shows that causality constitutes around 28 % of all NL requirements sentences. We then demonstrate that our detection tool achieves a macro-<jats:inline-formula><jats:alternatives><jats:tex-math>$$\hbox {F}_{1}$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:msub><mml:mtext>F</mml:mtext><mml:mn>1</mml:mn></mml:msub></mml:math></jats:alternatives></jats:inline-formula>score of 82 % on real-world data and that it outperforms related approaches with an average gain of 11.06 % in macro-Recall and 11.43 % in macro-Precision. Finally, our second case study corroborates the positive correlations of causality with features of NL requirements. The results strengthen our confidence in the eligibility of causal relations for downstream reuse, while our tool and publicly available data constitute a first step in the ongoing endeavors of utilizing causality in RE and beyond.</jats:p>",https://doi.org/10.1007/s00766-022-00371-x,CrossRef
Software Engineering,Requirements Engineering,Mixed Methods,Requirements and software engineering for automotive perception systems: an interview study,"<jats:title>Abstract</jats:title><jats:p>Driving automation systems, including autonomous driving and advanced driver assistance, are an important safety-critical domain. Such systems often incorporate perception systems that use machine learning to analyze the vehicle environment. We explore new or differing topics and challenges experienced by practitioners in this domain, which relate to requirements engineering (RE), quality, and systems and software engineering. We have conducted a semi-structured interview study with 19 participants across five companies and performed thematic analysis of the transcriptions. Practitioners have difficulty specifying upfront requirements and often rely on scenarios and operational design domains (ODDs) as RE artifacts. RE challenges relate to ODD detection and ODD exit detection, realistic scenarios, edge case specification, breaking down requirements, traceability, creating specifications for data and annotations, and quantifying quality requirements. Practitioners consider performance, reliability, robustness, user comfort, and—most importantly—safety as important quality attributes. Quality is assessed using statistical analysis of key metrics, and quality assurance is complicated by the addition of ML, simulation realism, and evolving standards. Systems are developed using a mix of methods, but these methods may not be sufficient for the needs of ML. Data quality methods must be a part of development methods. ML also requires a data-intensive verification and validation process, introducing data, analysis, and simulation challenges. Our findings contribute to understanding RE, safety engineering, and development methodologies for perception systems. This understanding and the collected challenges can drive future research for driving automation and other ML systems.</jats:p>",https://doi.org/10.1007/s00766-023-00410-1,CrossRef
Software Engineering,Requirements Engineering,Mixed Methods,Effects of common requirements engineering techniques on requirements engineering success in agile environments: A case study from Sri Lanka,"<jats:p>Requirements engineering is the process of exploring, analyzing, documenting and handling the needs to be facilitated through a computer-based system that is being developed. Though software solutions fail due to many reasons, Flawed Requirements Engineering (RE) is one of the main causes leading to failure of software projects especially in agile environments where requirements and solutions evolve through collaboration between self-organizing cross-functional teams. Therefore, succeeding in RE is an essential factor which influences success of the entire software project. In the light of achieving project success, various methods have been introduced to deal with RE. Main aim of this study is to explore the relationship between commonly used techniques and successful agile requirements engineering in agile environments with a special emphasis to Sri Lankan context. A secondary objective was to suggest innovative recommendations to software solution developers to achieve RE success. Five research questions were formulated to explore the relationship between mostly common techniques of requirements management and success of agile requirements engineering. Through key readings related to agile project development, authors have identified five key success factors as having an influence on successful project completion in an agile environment and are being tested in this research. Authors have designed the study taking an epistemological standpoint of positivism, using quantitative methods and case study strategy to derive conclusions. The entire population 130 employees of the case organization been considered for this study. The quantitative analysis of was conducted based on 105 valid responses gained through a five-point Likert scaled questionnaire from the case organization. It has been discovered that these common requirement engineering techniques are correlated with project success in agile environments. Further, scrutiny of each requirements engineering phase, led to finding a set of most effective techniques to be utilized in an agile environment contributing to software development project success.&#x0D;
Keywords: Requirements engineering, agile software development, project failures</jats:p>",https://doi.org/10.31357/icbm.v17.5193,CrossRef
Software Engineering,Requirements Engineering,Mixed Methods,Non-functional requirements for machine learning: understanding current use and challenges among practitioners,"<jats:title>Abstract</jats:title><jats:p>Systems that rely on Machine Learning (ML systems) have differing demands on quality—known as non-functional requirements (NFRs)—from traditional systems. NFRs for ML systems may differ in their definition, measurement, scope, and comparative importance. Despite the importance of NFRs in ensuring the quality ML systems, our understanding of all of these aspects is lacking compared to our understanding of NFRs in traditional domains. We have conducted interviews and a survey to understand how NFRs for ML systems are perceived among practitioners from both industry and academia. We have identified the degree of importance that practitioners place on different NFRs, including cases where practitioners are in agreement or have differences of opinion. We explore how NFRs are defined and measured over different aspects of a ML system (i.e., model, data, or whole system). We also identify challenges associated with NFR definition and measurement. Finally, we explore differences in perspective between practitioners in industry, academia, or a blended context. This knowledge illustrates how NFRs for ML systems are treated in current practice, and helps to guide future RE for ML efforts.</jats:p>",https://doi.org/10.1007/s00766-022-00395-3,CrossRef
Software Engineering,Requirements Engineering,Design and Development,"What If People Learn Requirements Over Time? A Rough Introduction to
  Requirements Economics","The overall objective of Requirements Engineering is to specify, in a
systematic way, a system that satisfies the expectations of its stakeholders.
Despite tremendous effort in the field, recent studies demonstrate this is
objective is not always achieved. In this paper, we discuss one particularly
challenging factor to Requirements Engineering projects, namely the change of
requirements. We proposes a rough discussion of how learning and time explain
requirements changes, how it can be introduced as a key variable in the
formulation of the Requirements Engineering Problem, and how this induces costs
for a requirements engineering project. This leads to a new discipline of
requirements economics.",http://arxiv.org/abs/1711.09092v1,arXiv
Software Engineering,Requirements Engineering,Design and Development,"Requirements Engineering Methods: A Classification Framework and
  Research Challenges","Requirements Engineering Methods (REMs) support Requirements Engineering (RE)
tasks, from elicitation, through modeling and analysis, to validation and
evolution of requirements. Despite the growing interest to design, validate and
teach REMs, it remains unclear what components REMs should have. A
classification framework for REMs is proposed. It distinguishes REMs based on
the domain-independent properties of their components. The classification
framework is intended to facilitate (i) analysis, teaching and extension of
existing REMs, (ii) engineering and validation of new REMs, and (iii)
identifying research challenges in REM design. The framework should help
clarify further the relations between REM and other concepts of interest in and
to RE, including Requirements Problem and Solution, Requirements Modeling
Language, and Formal Method.",http://arxiv.org/abs/1203.1717v1,arXiv
Software Engineering,Requirements Engineering,Design and Development,Refinement-Based Specification: Requirements and Architecture,"This paper presents the methodology for the system requirements and
architecture w.r.t. their decomposition and refinement. It also introduces
ideas of refinement layers and of refinement-based verification.",http://arxiv.org/abs/1404.7260v1,arXiv
Software Engineering,Requirements Engineering,Design and Development,A Framework for Aspectual Requirements Validation: An Experimental Study,"Requirements engineering is a discipline of software engineering that is
concerned with the identification and handling of user and system requirements.
Aspect-Oriented Requirements Engineering (AORE) extends the existing
requirements engineering approaches to cope with the issue of tangling and
scattering resulted from crosscutting concerns. Crosscutting concerns are
considered as potential aspects and can lead to the phenomena tyranny of the
dominant decomposition. Requirements-level aspects are responsible for
producing scattered and tangled descriptions of requirements in the
requirements document. Validation of requirements artefacts is an essential
task in software development. This task ensures that requirements are correct
and valid in terms of completeness and consistency, hence, reducing the
development cost, maintenance and establish an approximately correct estimate
of effort and completion time of the project. In this paper, we present a
validation framework to validate the aspectual requirements and the
crosscutting relationship of concerns that are resulted from the requirements
engineering phase. The proposed framework comprises a high-level and low-level
validation to implement on software requirements specification (SRS). The
high-level validation validates the concerns with stakeholders, whereas the
low-level validation validates the aspectual requirement by requirements
engineers and analysts using a checklist. The approach has been evaluated using
an experimental study on two AORE approaches. The approaches are
viewpoint-based called AORE with ArCaDe and lexical analysis based on Theme/Doc
approach. The results obtained from the study demonstrate that the proposed
framework is an effective validation model for AORE artefacts.",http://arxiv.org/abs/2110.03952v1,arXiv
Software Engineering,Requirements Engineering,Design and Development,"Applying Agile Requirements Engineering Approach for Re-engineering &
  Changes in existing Brownfield Adaptive Systems","Requirements Engineering (RE) is a key activity in the development of
software systems and is concerned with the identification of the goals of
stakeholders and their elaboration into precise statements of desired services
and behavior. The research describes an Agile Requirements Engineering approach
for re-engineering & changes in existing Brownfield adaptive system. The
approach has few modifications that can be used as a part of SCRUM development
process for re-engineering & changes. The approach illustrates the
re-engineering & changes requirements through introduction of GAP analysis &
requirements structuring & prioritization by creating AS-IS & TO-BE models with
80 / 20 rule. An attempt to close the gap between requirements engineering &
agile methods in form of this approach is provided for practical
implementation.",http://arxiv.org/abs/1410.6902v1,arXiv
Software Engineering,Requirements Engineering,Theoretical / Conceptual,"Teaching Model-based Requirements Engineering to Industry Professionals:
  An Experience Report","The use of conceptual models to foster requirements engineering has been
proposed and evaluated as beneficial for several decades. For instance,
goal-oriented requirements engineering or the specification of scenarios are
commonly done using conceptual models. Bringing such model-based requirements
engineering approaches into industrial practice typically requires industrial
training. In this paper, we report lessons learned from a training program for
teaching industry professionals model-based requirements engineering.
Particularly, we as educators and learners report experiences from designing
the training program, conducting the actual training, and applying the
instructed material in our day-to-day work. From these findings we provide
guidelines for educators designing requirements engineering courses for
industry professionals.",http://arxiv.org/abs/2103.04433v1,arXiv
Software Engineering,Requirements Engineering,Theoretical / Conceptual,"A Theoretical and Empirical Evaluation of Software Component Search
  Engines, Semantic Search Engines and Google Search Engine in the Context of
  COTS-Based Development","COTS-based development is a component reuse approach promising to reduce
costs and risks, and ensure higher quality. The growing availability of COTS
components on the Web has concretized the possibility of achieving these
objectives. In this multitude, a recurrent problem is the identification of the
COTS components that best satisfy the user requirements. Finding an adequate
COTS component implies searching among heterogeneous descriptions of the
components within a broad search space. Thus, the use of search engines is
required to make more efficient the COTS components identification. In this
paper, we investigate, theoretically and empirically, the COTS component search
performance of eight software component search engines, nine semantic search
engines and a conventional search engine (Google). Our empirical evaluation is
conducted with respect to precision and normalized recall. We defined ten
queries for the assessed search engines. These queries were carefully selected
to evaluate the capability of each search engine for handling COTS component
identification.",http://arxiv.org/abs/1204.2079v1,arXiv
Software Engineering,Requirements Engineering,Theoretical / Conceptual,"A Noble Methodology for Users Work Process Driven Software Requirements
  for Smart Handheld Devices","Requirement engineering is a key ingredient for software development to be
effective. Apart from the traditional software requirement which is not much
appropriate for new emerging software such as smart handheld device based
software. In many perspectives of requirement engineering, traditional and new
emerging software are not similar. Whereas requirement engineering of
traditional software needs more research, it is obvious that new emerging
software needs methodically and in-depth research for improved productivity,
quality, risk management and validity. In particular, the result of this paper
shows that how effective requirement engineering can improve in project
negotiation, project planning, managing feature creep, testing, defect, rework
and product quality. This paper also shows a new methodology which is focused
on users work process applicable for eliciting the requirement of traditional
software and any new type software of smart handheld device such as iPad. As an
example, the paper shows how the methodology will be applied as a software
requirement of iPad-based software for play-group students.",http://arxiv.org/abs/1408.2687v1,arXiv
Software Engineering,Requirements Engineering,Theoretical / Conceptual,"Analysis of Software Engineering Practices in General Software and
  Machine Learning Startups","Context: On top of the inherent challenges startup software companies face
applying proper software engineering practices, the non-deterministic nature of
machine learning techniques makes it even more difficult for machine learning
(ML) startups.
  Objective: Therefore, the objective of our study is to understand the whole
picture of software engineering practices followed by ML startups and identify
additional needs.
  Method: To achieve our goal, we conducted a systematic literature review
study on 37 papers published in the last 21 years. We selected papers on both
general software startups and ML startups. We collected data to understand
software engineering (SE) practices in five phases of the software development
life-cycle: requirement engineering, design, development, quality assurance,
and deployment.
  Results: We find some interesting differences in software engineering
practices in ML startups and general software startups. The data management and
model learning phases are the most prominent among them.
  Conclusion: While ML startups face many similar challenges to general
software startups, the additional difficulties of using stochastic ML models
require different strategies in using software engineering practices to produce
high-quality products.",http://arxiv.org/abs/2304.01523v1,arXiv
Software Engineering,Requirements Engineering,Theoretical / Conceptual,"Ontologies for Privacy Requirements Engineering: A Systematic Literature
  Review","Privacy has been frequently identified as a main concern for system
developers while dealing with/managing personal information. Despite this, most
existing work on privacy requirements deals with them as a special case of
security requirements. Therefore, key aspects of privacy are, usually,
overlooked. In this context, wrong design decisions might be made due to
insufficient understanding of privacy concerns. In this paper, we address this
problem with a systematic literature review whose main purpose is to identify
the main concepts/relations for capturing privacy requirements. In addition,
the identified concepts/relations are further analyzed to propose a novel
privacy ontology to be used by software engineers when dealing with privacy
requirements.",http://arxiv.org/abs/1611.10097v1,arXiv
Software Engineering,Project Management,Quantitative,Project Risk Management Model Based on PRINCE2 and Scrum Frameworks,"There is a lack of formal risk management techniques in agile software
development methods Scrum. The need to manage risks in agile project management
is also identified by various authors. Authors of this paper conducted a survey
to find out the current practices in agile project management. Furthermore
authors discuss the new integrated framework of Scrum and PRINCE2 with focus on
risk management. Enrichment of Scrum with selected practices from the
heavy-weight project management framework PRINCE2 promises better results in
delivering software products especially in global development projects.",http://arxiv.org/abs/1502.03595v1,arXiv
Software Engineering,Project Management,Quantitative,Complexity in the Context of Systems Approach to Project Management,"Complexity is an inherent attribute of any project. The purpose of defining
and documenting complexity is to have an early warning tool allowing a project
team to focus on certain areas and aspects of the project in order to prevent
and alleviate future risks and issues caused by this complexity. The main
contribution of this paper is to present a systematic view of complexity in
project management by identifying its key attributes and classifying complexity
by these attributes. A ""complexity taxonomy"", based on a survey of the existing
complexity literature, is developed and discussed including the product,
project, and external environment dimensions. We show how complexity types are
described through simple real life examples and business cases. Then we develop
a framework (tool) for applying the notion of complexity as an early warning
tool for a project manager in order to timely foresee future risks and
problems. The paper is intended for researchers in complexity, project
management, information systems, technology solutions and business management,
and also for information specialists, project managers, program managers,
financial staff and technology directors.",http://arxiv.org/abs/1412.1027v1,arXiv
Software Engineering,Project Management,Quantitative,"Successful Management of Cloud Based Global Software Development
  Projects: A Multivocal Study","Context: Software industry is continuously exploring better ways to develop
applications. A new phenomenon to achieve this is Cloud based Global Software
Development (CGSD), which refers to the adoption of cloud computing services by
organizations to support global software development projects. The CGSD
approach affects the strategic and operational aspects of the way projects are
managed. Objective: The objective of the study is to identify the success
factors which contribute to management of CGSD projects. Methods: We carried
out a Multivocal Literature Review (MLR) to identify the success factors from
the state-of-the-art and the state-of-the-practice in project management of
CGSD projects. We identified 32 success factors that contribute to the
management of CGSD projects. Results: The findings of MLR indicate that time to
market, continuous development, financial restructuring, scalability Moreover,
the findings of the study show that there is a positive correlation between the
success factors reported in both formal literature and industry based grey
literature. Conclusion: The findings of this study can assist the practitioners
to develop the strategies needed for effective project management of CGSD
projects.",http://arxiv.org/abs/2208.08743v1,arXiv
Software Engineering,Project Management,Quantitative,"An Analytical Approach for Project Managers in Effective Defect
  Management in Software Process","Defect estimation and prediction are some of the main modulating factors for
the success of software projects in any software industry. Maturity and
competency of a project manager in efficient prediction and estimation of
resource capabilities are one of the strategic driving forces towards the
generation of high quality software. Currently, there are no estimation
techniques developed through empirical analysis to evaluate the decision
capability of a project manager towards resource allocation for effective
defect management. This paper brings out an empirical study carried out in a
product based software organization. Our deep investigation on several projects
throws light on the impact of decision capability of project manager towards
accomplishment of an aforementioned objective. The paper enables project
managers to gain further awareness towards the significance of predictive
positioning in resource allocation in order to develop high quality defect-free
software products. It also enhances the maturity level of the company and its
persistence in the competitive atmosphere.",http://arxiv.org/abs/1203.6439v1,arXiv
Software Engineering,Project Management,Quantitative,"An Study of The Role of Software Project Manger in the Outcome of the
  Project","This paper describes an in depth analysis of successful and unsuccessful
software Projects and the Role of Software Project Mangers in that success. One
of the main reason in software project success is manager. Software houses are
investing too much in this regard but the average ratio of software project
failure is on the high side. Project managers experience, technical knowledge,
and skills are not good enough for success in general. In this paper we have
conducted a survey related to the approached used by different project
managers, their methods and techniques, and the success ratio of their
projects, and the steps they took during their projects. We will explore the
core reasons of software project success and then will suggest key steps to be
taken by the software project managers to deliver a successful software
project.",http://arxiv.org/abs/2009.13869v1,arXiv
Software Engineering,Project Management,Qualitative,Risk Analysis in the Selection of Project Managers Based on ANP and FMEA,"Project managers play a crucial role in the success of projects. The
selection of an appropriate project manager is a primary concern for senior
managers in firms. Typically, this process involves candidate interviews and
assessments of their abilities. There are various criteria for selecting a
project manager, and the importance of each criterion depends on the project
type, its conditions, and the risks associated with their absence in the chosen
candidate. Often, senior managers in engineering companies lack awareness of
the significance of these criteria and the potential risks linked to their
absence. This research aims to identify these risks in selecting project
managers for civil engineering projects, utilizing a combined ANP-FMEA
approach. Through a comprehensive literature review, five risk categories have
been identified: individual skills, power-related issues, knowledge and
expertise, experience, and personality traits. Subsequently, these risks, along
with their respective sub-criteria and internal relationships, were analysed
using the combined ANP-FMEA technique. The results highlighted that the lack of
political influence, absence of construction experience, and deficiency in
project management expertise represent the most substantial risks in selecting
a project manager. Moreover, upon comparison with the traditional FMEA
approach, this study demonstrates the superior ability of the ANP-FMEA model in
differentiating risks and pinpointing factors with elevated risk levels.",http://arxiv.org/abs/2311.03224v1,arXiv
Software Engineering,Project Management,Qualitative,"IT Project Governance in Project Delivery: Key Processes, Activities,
  Roles and Responsibilities","The general objective of this work was to contribute to the general body of
knowledge and research work in the area of managing IT projects successfully.
The office of Government commerce of the UK Government in conjunction with the
National Audit came out with a guideline in 2007 which list out eight causes of
project failures. Six out of the eight causes were attributed to governance
issues (Aon, 2011). This shows clearly that governance activities in project
management are really important, and failure to place premium on them can
result in failure of projects. The research work was therefore intended to come
out with IT governance frameworks that can help resolve the problem of failure
of IT projects resulting from governance issues. Four IT program managers and
eight IT project managers were talked to on a variety of IT project governance
issues. The frameworks were developed based on a combination of literature,
experts (IT project and program managers of the telecom industry in Ghana)
input, observation, and so on. The governance frameworks depicts project
management processes, project activities, project governance activities, roles
and responsibilities of key stakeholders, key milestones, approval bodies,
signatures, and so on, to ensure successful delivery of IT projects.",http://arxiv.org/abs/1912.13295v1,arXiv
Software Engineering,Project Management,Qualitative,"Managing Security Issues in Software Containers: From Practitioners
  Perspective","Software development industries are increasingly adopting containers to
enhance the scalability and flexibility of software applications. Security in
containerized projects is a critical challenge that can lead to data breaches
and performance degradation, thereby directly affecting the reliability and
operations of the container services. Despite the ongoing effort to manage the
security issues in containerized projects in software engineering (SE)
research, more focused investigations are needed to explore the human
perspective of security management and the technical approaches to security
management in containerized projects. This research aims to explore security
management in containerized projects by exploring how SE practitioners perceive
the security issues in containerized software projects and their approach to
managing such issues. A clear understanding of security management in
containerized projects will enable industries to develop robust security
strategies that enhance software reliability and trust. To achieve this, we
conducted two separate semi-structured interview studies to examine how
practitioners approach security management. The first study focused on
practitioners perceptions of security challenges in containerized environments,
where we interviewed 15 participants between December 2022 and October 2023.
The second study explored how to enhance container security, with 20
participants interviewed between October 2024 and December 2024. Analyzing the
data from both studies reveals how SE practitioners address the various
security challenges in containerized projects. Our analysis also identified the
technical and non-technical enablers that can be utilized to enhance security.",http://arxiv.org/abs/2504.07707v1,arXiv
Software Engineering,Project Management,Qualitative,"Automated Security Findings Management: A Case Study in Industrial
  DevOps","In recent years, DevOps, the unification of development and operation
workflows, has become a trend for the industrial software development
lifecycle. Security activities turned into an essential field of application
for DevOps principles as they are a fundamental part of secure software
development in the industry. A common practice arising from this trend is the
automation of security tests that analyze a software product from several
perspectives. To effectively improve the security of the analyzed product, the
identified security findings must be managed and looped back to the project
team for stakeholders to take action. This management must cope with several
challenges ranging from low data quality to a consistent prioritization of
findings while following DevOps aims. To manage security findings with the same
efficiency as other activities in DevOps projects, a methodology for the
management of industrial security findings minding DevOps principles is
essential.
  In this paper, we propose a methodology for the management of security
findings in industrial DevOps projects, summarizing our research in this domain
and presenting the resulting artifact. As an instance of the methodology, we
developed the Security Flama, a semantic knowledge base for the automated
management of security findings. To analyze the impact of our methodology on
industrial practice, we performed a case study on two DevOps projects of a
multinational industrial enterprise. The results emphasize the importance of
using such an automated methodology in industrial DevOps projects, confirm our
approach's usefulness and positive impact on the studied projects, and identify
the communication strategy as a crucial factor for usability in practice.",http://arxiv.org/abs/2401.06602v1,arXiv
Software Engineering,Project Management,Qualitative,"Towards an Understanding of Why and How ICT Projects Are Initiated:
  Analysis via Repertory Grid","Contemporary business innovation relies increasingly on information and
communications technology (ICT) solutions. As ICT initiatives are generally
implemented via projects the management of ICT projects has come under
increasing scrutiny. ICT projects continue to fail; as a result, while research
in ICT project management has indeed increased, many challenges for research
and practice remain. Many studies have addressed the execution and management
of ICT projects and the many factors that might relate to project outcomes.
Very few, however, have considered ICT project initiation and the crucial
decisions made at that very early, pre-life cycle stage. The primary intent of
this research is therefore to investigate ICT projects with a particular focus
on their initiation. In doing so we wished to understand why ICT projects are
started, and how they are moved from idea or proposal to supported reality. A
combination of semi-structured interviews and the repertory grid data
collection and analysis method was employed to investigate and validate the
motivating factors that influence individual IT Managers' project initiation
decisions and the methods they use to transition from idea to enacted project.
Our results showed that there are indeed multiple underlying reasons for the
decisions made at this early stage and that there are some especially common
decision drivers. Some were expected, in the sense that they mapped to
recommended best practice. For instance, most projects are motivated by a
desire to achieve efficiencies or cost savings, and their potential tends to be
assessed using cost benefit analysis. Other results were more surprising -
competitor pressure was not a common driver for ICT project initiation in our
analysis. Unsurprisingly, formal evaluation methods are more frequently used to
assess project proposals when those projects are larger and higher profile.
(Abridged)",http://arxiv.org/abs/2103.10570v1,arXiv
Software Engineering,Project Management,Mixed Methods,"Transient Information Adaptation of Artificial Intelligence: Towards
  Sustainable Data Processes in Complex Projects","Large scale projects increasingly operate in complicated settings whilst
drawing on an array of complex data-points, which require precise analysis for
accurate control and interventions to mitigate possible project failure.
Coupled with a growing tendency to rely on new information systems and
processes in change projects, 90% of megaprojects globally fail to achieve
their planned objectives. Renewed interest in the concept of Artificial
Intelligence (AI) against a backdrop of disruptive technological innovations,
seeks to enhance project managers cognitive capacity through the project
lifecycle and enhance project excellence. However, despite growing interest
there remains limited empirical insights on project managers ability to
leverage AI for cognitive load enhancement in complex settings. As such this
research adopts an exploratory sequential linear mixed methods approach to
address unresolved empirical issues on transient adaptations of AI in complex
projects, and the impact on cognitive load enhancement. Initial thematic
findings from semi-structured interviews with domain experts, suggest that in
order to leverage AI technologies and processes for sustainable cognitive load
enhancement with complex data over time, project managers require improved
knowledge and access to relevant technologies that mediate data processes in
complex projects, but equally reflect application across different project
phases. These initial findings support further hypothesis testing through a
larger quantitative study incorporating structural equation modelling to
examine the relationship between artificial intelligence and project managers
cognitive load with project data in complex contexts.",http://arxiv.org/abs/2104.04067v2,arXiv
Software Engineering,Project Management,Mixed Methods,"Enhancing Project Performance Forecasting using Machine Learning
  Techniques","Accurate forecasting of project performance metrics is crucial for
successfully managing and delivering urban road reconstruction projects.
Traditional methods often rely on static baseline plans and fail to consider
the dynamic nature of project progress and external factors. This research
proposes a machine learning-based approach to forecast project performance
metrics, such as cost variance and earned value, for each Work Breakdown
Structure (WBS) category in an urban road reconstruction project. The proposed
model utilizes time series forecasting techniques, including Autoregressive
Integrated Moving Average (ARIMA) and Long Short-Term Memory (LSTM) networks,
to predict future performance based on historical data and project progress.
The model also incorporates external factors, such as weather patterns and
resource availability, as features to enhance the accuracy of forecasts. By
applying the predictive power of machine learning, the performance forecasting
model enables proactive identification of potential deviations from the
baseline plan, which allows project managers to take timely corrective actions.
The research aims to validate the effectiveness of the proposed approach using
a case study of an urban road reconstruction project, comparing the model's
forecasts with actual project performance data. The findings of this research
contribute to the advancement of project management practices in the
construction industry, offering a data-driven solution for improving project
performance monitoring and control.",http://arxiv.org/abs/2411.17914v1,arXiv
Software Engineering,Project Management,Mixed Methods,"Using statistical control charts to monitor duration-based performance
  of project","Monitoring of project performance is a crucial task of project managers that
significantly affect the project success or failure. Earned Value Management
(EVM) is a well-known tool to evaluate project performance and effective
technique for identifying delays and proposing appropriate corrective actions.
The original EVM analysis is a monetary-based method and it can be misleading
in the evaluation of the project schedule performance and estimation of the
project duration. Earned Duration Management (EDM) is a more recent method
which introduces metrics for the project schedule performance evaluation and
improves EVM analysis. In this paper, we apply statistical control charts on
EDM indices to better investigate the variations of project schedule
performance. Control charts are decision support tools to detect the out of
control performance. Usually project performance measurements are
auto-correlated and not following the normal distribution. Hence, in this
paper, a two-step adjustment framework is proposed to make the control charts
applicable to non-normal and auto-correlated measurements. The case study
project illustrates how the new method can be implemented in practice. The
numerical results conclude that that employing control chart method along with
analyzing the actual values of EDM indices increase the capability of project
management teams to detect cost and schedule problems on time",http://arxiv.org/abs/1902.02270v1,arXiv
Software Engineering,Project Management,Mixed Methods,"SLI, a New Metric to determine Success of a Software Project","Project Management process plays a critical role in managing factors such as
cost, time, technology and personnel towards achieving the success of a project
and henceforth the sustainability of the company in the industrial market. This
paper emphasizes empirical study of several projects developed over a period of
time in a product and service based CMMI Level 5 Software Company. The
investigation shows impact analysis of resources such as cost, time, and number
of developers towards the successful completion of the project as allocated by
the project manager during the developmental process. The analysis has further
led to the introduction of a new qualitative metric, Success Level Index Metric
(SLI) whose index value varies from 0 to 1. SLI acts as a maturity indicator
that indicates the degree of maturity of the company in terms of success of
their projects based on which the company can choose their desired level of
success for their projects.",http://arxiv.org/abs/1407.8377v1,arXiv
Software Engineering,Project Management,Mixed Methods,"ERP projects Internal Stakeholder network and how it influences the
  projects outcome","So far little effort has been put into researching the importance of internal
ERP project stakeholders mutual interactions,realizing the projects
complexity,influence on the whole organization, and high risk for a useful
final outcome. This research analyzes the stakeholders interactions and
positions in the project network, their criticality, potential bottlenecks and
conflicts. The main methods used are Social Network Analysis, and the
elicitation of drivers for the individual players. Information was collected
from several stakeholders from three large ERP projects all in global companies
headquartered in Finland, together with representatives from two different ERP
vendors, and with two experienced ERP consultants. The analysis gives
quantitative as well as qualitative characterization of stakeholder criticality
(mostly the Project Manager(s), the Business Owner(s) and the Process
Owner(s)), degree of centrality, closeness, mediating or bottleneck roles,
relational ties and conflicts (individual, besides those between business and
project organizations), and clique formations. A generic internal stakeholder
network model is established as well as the criticality of the project phases.
The results are summarized in the form of a list of recommendations for future
ERP projects to address the internal stakeholder impacts .Project management
should utilize the latest technology to provide tools to increase the
interaction between the stakeholders and to monitor the strength of these
relations. Social network analysis tools could be used in the projects to
visualize the stakeholder relations in order to better understand the possible
risks related to the relations (or lack of them).",http://arxiv.org/abs/1308.2938v1,arXiv
Software Engineering,Project Management,Design and Development,"Embedding Sustainability in Complex Projects: A Pedagogic Practice
  Simulation Approach","Sustainability is focussed on avoiding the long-term depletion of natural
resources. Under the terms of a government plan to tackle climate change, a
driver for improved sustainability is the cut of greenhouse gas emissions in
the UK to almost zero by 2050. With this type of change, new themes are
continuously being developed which drive complex projects, such as the
development of new power generation methods, which encompass challenging lead
times and demanding requirements. Consideration of the implementation of
strategies and key concepts, which may engender sustainability within complex
projects therefore presents an opportunity for further critical debate, review,
and application through a project management lens. Sustainability incorporation
in project management has been documented in academic literature, with this
emerging field providing new challenges. For example, project management
education can provide a holistic base for the inculcation of sustainability
factors to a range of industries, including complex projects. Likewise,
practitioner interest and approaches to sustainability in project management
are being driven by the recently Chartered Association for Project Management
(APM). Whilst this body makes a significant contribution to the UK economy
across many sectors, it also addresses ongoing sustainability challenges.
Therefore, by drawing on research and practitioner developments, the authors
argue that by connecting with the next generation through practice simulation
approaches, and embedding sustainability issues within project management tools
and methods, improved focus on sustainability in complex project management may
be achieved.",http://arxiv.org/abs/2104.04068v2,arXiv
Software Engineering,Project Management,Design and Development,"Service Oriented Architecture A Revolution For Comprehensive Web Based
  Project Management Software","Service Oriented Architecture A Revolution for Project Management Software
has changed the way projects today are moving on the fly with the help of web
services booming the industry. Service oriented architecture improves
performance and the communication between the distributed and remote teams. Web
Services to Provide Project Management software the visibility and control of
the application development lifecycle-giving a better control over the entire
development process, from the management stage through development. The goal of
Service Oriented Architecture for Project Management Software is to produce a
product that is delivered on time, within the allocated budget, and with the
capabilities expected by the customer. Web Services in Project management
Project management software is basically a properly managed project and has a
clear, communicated, and managed set of goals and objectives, whose progress is
quantifiable and controlled. Resources are used effectively and efficiently to
produce the desired product. With the help of service oriented architecture we
can move into the future without abandoning the past. A project usually has a
communicated set of processes that cover the daily activities of the project,
forming the project framework. As a result, every team member understands their
roles, responsibilities and how they fit into the big picture thus promoting
the efficient use of resources.",http://arxiv.org/abs/1207.4966v1,arXiv
Software Engineering,Project Management,Design and Development,"SoaDssPm: A new Service-Oriented Architecture of the decision support
  system for the Project Management","This paper presents an architecture for the Project Management, which is
defined using the concepts behind ServiceOriented and Decision Support System.
The framework described, denominated as SoaDssPm, represents the following: a
coherent solution to the problem of control Project Management the existing gap
between the real execution of Project Management by describing the business
process and relationships required by a SOA solution, and its objectives
representation, in which the decisional aspects determine the final shape of
the system, providing decision support to the identified business processes and
constraints.",http://arxiv.org/abs/1401.5433v1,arXiv
Software Engineering,Project Management,Design and Development,Towards effective AI-powered agile project management,"The rise of Artificial intelligence (AI) has the potential to significantly
transform the practice of project management. Project management has a large
socio-technical element with many uncertainties arising from variability in
human aspects e.g., customers' needs, developers' performance and team
dynamics. AI can assist project managers and team members by automating
repetitive, high-volume tasks to enable project analytics for estimation and
risk prediction, providing actionable recommendations, and even making
decisions. AI is potentially a game changer for project management in helping
to accelerate productivity and increase project success rates. In this paper,
we propose a framework where AI technologies can be leveraged to offer support
for managing agile projects, which have become increasingly popular in the
industry.",http://arxiv.org/abs/1812.10578v1,arXiv
Software Engineering,Project Management,Design and Development,Anywhere: A Web Crawler Automation Management Interface,"Web crawling projects or design is significant in the current information
age. Using the web spider or crawler can automatically search and collect a
huge amount of internet information. As one of the most popular web crawler
frameworks, Scrapy is robust in abundant functions but weak in easy operation.
In this paper, we provide a framework Anywhere, for optimising the usage
feeling and improving the use efficiency of the web crawling management of
Scrapy. We analyse the whole workflow of a web crawling project of Scrapy and
design two main functions in Anywhere, one is quickly generating a Scrapy
project with the preset temperatures, the other is repeatable configuration
function for the Scrapy project setting. Beside, with Anywhere, users can
easily directly manage multiple Scrapy projects with a file folders
architecture. Compared with normal Scrapy project interactive coding
development, we test Anywhere with enough experiments that show Anywhere can
improve the development efficiency of Scrapy projects to about 200\%. For the
multiple project management in code interaction level, the developing
efficiency is improved to about 300\%. We simplify the procedure to quickly
generate a simple spider project with Scrapy. Anywhere can assist the
development of Scrapy is useful for the design of large batch concurrent
projects at coding level.",http://arxiv.org/abs/2407.00025v1,arXiv
Software Engineering,Project Management,Theoretical / Conceptual,A Perspective-Based Understanding of Project Success,"Answering the call for alternative approaches to researching project
management, we explore the evaluation of project success from a subjectivist
perspective. An in-depth, longitudinal case study of information systems
development in a large manufacturing company was used to investigate how
various project stakeholders subjectively perceived the project outcome and
what evaluation criteria they drew on in doing so. A conceptual framework is
developed for understanding and analyzing evaluations of project success, both
formal and informal. The framework highlights how different stakeholder
perspectives influence the perceived outcome(s) of a project, and how project
evaluations may differ between stakeholders and across time.",http://arxiv.org/abs/2101.05425v1,arXiv
Software Engineering,Project Management,Theoretical / Conceptual,From Nobel Prize to Project Management: Getting Risks Right,"A major source of risk in project management is inaccurate forecasts of
project costs, demand, and other impacts. The paper presents a promising new
approach to mitigating such risk, based on theories of decision making under
uncertainty which won the 2002 Nobel prize in economics. First, the paper
documents inaccuracy and risk in project management. Second, it explains
inaccuracy in terms of optimism bias and strategic misrepresentation. Third,
the theoretical basis is presented for a promising new method called ""reference
class forecasting,"" which achieves accuracy by basing forecasts on actual
performance in a reference class of comparable projects and thereby bypassing
both optimism bias and strategic misrepresentation. Fourth, the paper presents
the first instance of practical reference class forecasting, which concerns
cost forecasts for large transportation infrastructure projects. Finally,
potentials for and barriers to reference class forecasting are assessed.",http://arxiv.org/abs/1302.3642v1,arXiv
Software Engineering,Project Management,Theoretical / Conceptual,"Quality Control and Due Diligence in Project Management: Getting
  Decisions Right by Taking the Outside View","This paper explores how theories of the planning fallacy and the outside view
may be used to conduct quality control and due diligence in project management.
First, a much-neglected issue in project management is identified, namely that
the front-end estimates of costs and benefits--used in the business cases,
cost-benefit analyses, and social and environmental impact assessments that
typically support decisions on projects--are typically significantly different
from actual ex post costs and benefits, and are therefore poor predictors of
the actual value and viability of projects. Second, it is discussed how
Kahneman and Tversky's theories of the planning fallacy and the outside view
may help explain and remedy this situation through quality control of
decisions. Third, it is described what quality control and due diligence are in
the context of project management, and an eight-step procedure is outlined for
due diligence based on the outside view. Fourth, the procedure is tested on a
real-life, multibillion-dollar project, organized as a public-private
partnership. Finally, Akerlof and Shiller's recent discussion in economics of
""firing the forecaster"" is discussed together with its relevance to project
management. In sum, the paper demonstrates the need, the theoretical basis, a
practical methodology, and a real-life example for how to de-bias project
management using quality control and due diligence based on the outside view.",http://arxiv.org/abs/1302.2544v1,arXiv
Software Engineering,Project Management,Theoretical / Conceptual,"OntoMaven: Maven-based Ontology Development and Management of
  Distributed Ontology Repositories","In collaborative agile ontology development projects support for modular
reuse of ontologies from large existing remote repositories, ontology project
life cycle management, and transitive dependency management are important
needs. The Apache Maven approach has proven its success in distributed
collaborative Software Engineering by its widespread adoption. The contribution
of this paper is a new design artifact called OntoMaven. OntoMaven adopts the
Maven-based development methodology and adapts its concepts to knowledge
engineering for Maven-based ontology development and management of ontology
artifacts in distributed ontology repositories.",http://arxiv.org/abs/1309.7341v1,arXiv
Software Engineering,Project Management,Theoretical / Conceptual,"Exploring Data Management Challenges and Solutions in Agile Software
  Development: A Literature Review and Practitioner Survey","Context: Managing data related to a software product and its development
poses significant challenges for software projects and agile development teams.
These include integrating data from diverse sources and ensuring data quality
amidst continuous change and adaptation. Objective: The paper systematically
explores data management challenges and potential solutions in agile projects,
aiming to provide insights into data management challenges and solutions for
both researchers and practitioners. Method: We employed a mixed-methods
approach, including a systematic literature review (SLR) to understand the
state-of-research followed by a survey with practitioners to reflect on the
state-of-practice. The SLR reviewed 45 studies, identifying and categorizing
data management aspects along with their associated challenges and solutions.
The practitioner survey captured practical experiences and solutions from 32
industry practitioners who were significantly involved in data management to
complement the findings from the SLR. Results: Our findings identified major
data management challenges in practice, such as managing data integration
processes, capturing diverse data, automating data collection, and meeting
real-time analysis requirements. To address the challenges, solutions such as
automation tools, decentralized data management practices, and ontology-based
approaches have been identified. The solutions enhance data integration,
improve data quality, and enable real-time decision-making by providing
flexible frameworks tailored to agile project needs. Conclusion: The study
pinpointed significant challenges and actionable solutions in data management
for agile software development. Our findings provide practical implications for
practitioners and researchers, emphasizing the development of effective data
management practices and tools to address those challenges and improve project
success.",http://arxiv.org/abs/2402.00462v4,arXiv
Information Systems,Enterprise Systems,Quantitative,"Enterprise resource planning systems, strategic enterprise management systems and management accounting","<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>The purpose of this paper is to contribute to the body of knowledge about to what extent integrated information systems, such as ERP and SEM systems, affect the ability to solve different management accounting tasks.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>The relationship between IIS and management accounting practices was investigated quantitatively. A total of 349 responses were collected using a survey, and the data were analysed using linear regression models.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>Analyses indicate that ERP systems support the data collection and the organisational breadth of management accounting better than SEM systems. SEM systems, on the other hand, seem to be better at supporting reporting and analysis. In addition, modern management accounting techniques involving the use of non‐financial data are better supported by an SEM system. This indicates that different management accounting tasks are supported by different parts of the IIS.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Research limitations/implications</jats:title><jats:p>The study applies the methods of quantitative research. Thus, the internal validity is threatened. Conducting in‐depth studies might be able to reduce this possible shortcoming.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p>On the basis of the findings, there is a need to consider the potential of closer integration of ERP and SEM systems in order to solve management accounting tasks.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>This paper adds to the limited body of knowledge about the relationship between IIS and management accounting practices.</jats:p></jats:sec>",https://doi.org/10.1108/17410390610636878,CrossRef
Information Systems,Enterprise Systems,Quantitative,Is it Really so 'Strategic'?,"<p>This paper presents empirical research on motivational factors for investing in Enterprise Systems (ES), based on the survey conducted among project leaders. The results show that enterprises make investments in ES mostly to increase operational efficiency, provide managers with more accurate information and, which is interesting, to be able to continue the operations on the current level. Almost one third of examined enterprises indicated the replacement of an inefficient IT infrastructure with a new one enabling smooth operation of current business processes as the most important motivational factor for investments. The results of the research presented in this paper may help to understand the productivity paradox as they prove that many enterprises treat IT as a commodity rather than a strategic asset that generates significant business gains.</p>",https://doi.org/10.4018/ijeis.2011100102,CrossRef
Information Systems,Enterprise Systems,Quantitative,Modern problems of anomaly identification in Enterprise Systems,"<jats:p>The article addresses modern challenges in anomaly detection within enterprise systems us-ing memory dump analysis. As the complexity of enterprise systems grows, the number of potential issues affecting their stability and performance also increases. Anomalies, such as software failures or unexpected deviations from normal behavior, can lead to serious consequences, including data loss, reduced performance, or even complete system shutdown. Detecting and resolving these anomalies is a critical task for maintaining uninterrupted operation in enterprise environments. The primary method discussed in this article is memory dump analysis, which provides de-tailed information about the system's state at the time of an anomaly. This method is effective for identifying root causes of failures, such as memory leaks or other resource-intensive operations. However, due to the large volumes of data and the complexity of modern software systems, memory dump analysis faces several challenges, such as the need for precise data collection during inci-dents and the requirement for powerful computational resources to process such data. The article thoroughly analyzes algorithms and tools used for detecting problems in enter-prise systems. Specifically, statistical methods, machine learning algorithms, and tools for memory dump analysis are reviewed. Machine learning techniques enable the creation of models represent-ing normal system behavior and automatically detect deviations from these models, facilitating timely identification of potential issues. Additionally, optimization methods aimed at improving sys-tem performance, including techniques such as parallelization, caching, and code profiling, are ex-plored. One of the main challenges discussed in the article is the limitations of existing methods and tools for software analysis. High-load systems often face difficulties in real-time profiling and monitoring, complicating the identification of root causes. The article also examines limitations re-lated to the accuracy of data collection and the complexity of diagnosing issues in distributed sys-tems. Based on the analysis, the article suggests future prospects for improving modern methods of anomaly detection in enterprise systems. Key areas for further research include enhancing machine learning algorithms for memory dump analysis, developing more efficient optimization methods, and improving monitoring tools to increase the accuracy and speed of problem detection. The arti-cle also highlights the importance of integrating these technologies into real-world enterprise envi-ronments to ensure stability and reliability.</jats:p>",https://doi.org/10.34185/1562-9945-5-154-2024-15,CrossRef
Information Systems,Enterprise Systems,Quantitative,Enterprise systems implementation and accounting benefits,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>The aim of this paper is to examine the accounting benefits involved in adopting enterprise systems, but also to comprehend the underlying causes that motivate their adoption.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>This paper presents evidence from a survey of 73 companies which implemented ES on the related benefits to their accounting information provision and practices.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>Empirical evidence confirms a number of benefits derived from the application of ES that focus on the following dimensions: organisation, operations, management and IT infrastructure. These benefits are related both to the reasons leading to the implementation of ES and to the relevant selection of ES modules.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>The corresponding results from this survey can then be used as a basis for establishing the best approach in order to fully exploit the potential of future ES applications in accounting practice.</jats:p></jats:sec>",https://doi.org/10.1108/17410390610636887,CrossRef
Information Systems,Enterprise Systems,Quantitative,Improving Internal Supply Chain Collaboration by Increasing Information-Processing Capacity,"<p>This study examines the role of enterprise information systems in fostering collaboration between the Purchasing, Production, and Outbound Logistics functions of manufacturing facilities. These forms of integrative information technology are considered key facilitators of supply chain management. The research model embeds these systems within a broader model of facilitators of interfunctional integration. The model is tested by path analysis, using a survey sample of 120 manufacturing facilities located in the United States. The results suggest that integrative information technology has a significant effect on collaboration, and that the level of demand uncertainty moderates the effect on collaboration.</p>",https://doi.org/10.4018/jeis.2011100101,CrossRef
Information Systems,Enterprise Systems,Qualitative,"Exploring the Use of Enterprise 2.0 and Its Impact on Social Capital
  within a Large Organisation","Despite the rampant adoption of Enterprise 2.0, there is lack of empirical
evidence of how Enterprise 2.0 is aptly supporting the business objectives.
Social capital theory will be used as a theoretical lens to understand the
impact and implications of individual use of Enterprise 2.0. To ascertain the
impact from the use of Enterprise 2.0 on the various dimensions of social
capital, a single in-depth qualitative case study was conducted with a large
professional services organisation. The findings unfold the different areas of
impacts based on actual individual use and experience. The research concludes
with a framework delineating the intertwined relationship between each social
capital dimensions.",http://arxiv.org/abs/1606.02486v1,arXiv
Information Systems,Enterprise Systems,Qualitative,"From Full-fledged ERP Systems Towards Process-centric Business Process
  Platforms","Enterprise Resource Planning (ERP) systems are critical to the success of
enterprises, facilitating business operations through standardized digital
processes. However, existing ERP systems are unsuitable for startups and small
and medium-sized enterprises that grow quickly and require adaptable solutions
with low barriers to entry. Drawing upon 15 explorative interviews with
industry experts, we examine the challenges of current ERP systems using the
task technology fit theory across companies of varying sizes. We describe high
entry barriers, high costs of implementing implicit processes, and insufficient
interoperability of already employed tools. We present a vision of a future
business process platform based on three enablers: Business processes as
first-class entities, semantic data and processes, and cloud-native elasticity
and high availability. We discuss how these enablers address current ERP
systems' challenges and how they may be used for research on the next
generation of business software for tomorrow's enterprises.",http://arxiv.org/abs/2306.02995v1,arXiv
Information Systems,Enterprise Systems,Qualitative,"Towards Energy-Proportional Computing Using Subsystem-Level Power
  Management","Massive data centers housing thousands of computing nodes have become
commonplace in enterprise computing, and the power consumption of such data
centers is growing at an unprecedented rate. Adding to the problem is the
inability of the servers to exhibit energy proportionality, i.e., provide
energy-efficient execution under all levels of utilization, which diminishes
the overall energy efficiency of the data center. It is imperative that we
realize effective strategies to control the power consumption of the server and
improve the energy efficiency of data centers. With the advent of Intel Sandy
Bridge processors, we have the ability to specify a limit on power consumption
during runtime, which creates opportunities to design new power-management
techniques for enterprise workloads and make the systems that they run on more
energy-proportional.
  In this paper, we investigate whether it is possible to achieve energy
proportionality for enterprise-class server workloads, namely SPECpower_ssj2008
and SPECweb2009 benchmarks, by using Intel's Running Average Power Limit (RAPL)
interfaces. First, we analyze the average power consumption of the full system
as well as the subsystems and describe the energy proportionality of these
components. We then characterize the instantaneous power profile of these
benchmarks within different subsystems using the on-chip energy meters exposed
via the RAPL interfaces. Finally, we present the effects of power limiting on
the energy proportionality, performance, power and energy efficiency of
enterprise-class server workloads. Our observations and results shed light on
the efficacy of the RAPL interfaces and provide guidance for designing
power-management techniques for enterprise-class workloads.",http://arxiv.org/abs/1501.02724v1,arXiv
Information Systems,Enterprise Systems,Qualitative,"Cloud Migration: A Case Study of Migrating an Enterprise IT System to
  IaaS","This case study illustrates the potential benefits and risks associated with
the migration of an IT system in the oil & gas industry from an in-house data
center to Amazon EC2 from a broad variety of stakeholder perspectives across
the enterprise, thus transcending the typical, yet narrow, financial and
technical analysis offered by providers. Our results show that the system
infrastructure in the case study would have cost 37% less over 5 years on EC2,
and using cloud computing could have potentially eliminated 21% of the support
calls for this system. These findings seem significant enough to call for a
migration of the system to the cloud but our stakeholder impact analysis
revealed that there are significant risks associated with this. Whilst the
benefits of using the cloud are attractive, we argue that it is important that
enterprise decision-makers consider the overall organizational implications of
the changes brought about with cloud computing to avoid implementing local
optimizations at the cost of organization-wide performance.",http://arxiv.org/abs/1002.3492v1,arXiv
Information Systems,Enterprise Systems,Qualitative,"Fostering Enterprise Conversations Around Data on Collaboration
  Platforms","In enterprise organizations, data-driven decision making processes include
the use of business intelligence dashboards and collaborative deliberation on
communication platforms such as Slack. However, apart from those in data
analyst roles, there is shallow engagement with dashboard content due to
insufficient context, poor representation choices, or a lack of access and
guidance. Data analysts often need to retarget their dashboard content for
those with limited engagement, and this retargeting process often involves
switching between different tools. To inform the design of systems that
streamline this work process, we conducted a co-design study with nine
enterprise professionals who use dashboard content to communicate with their
colleagues. We consolidate our findings from the co-design study into a
comprehensive demonstration scenario. Using this scenario as a design probe, we
interviewed 14 data workers to further develop our design recommendations.",http://arxiv.org/abs/2310.04315v3,arXiv
Information Systems,Enterprise Systems,Mixed Methods,A Serverless Distributed Ledger for Enterprises,"Enterprises have been attracted by the capability of blockchains to provide a
single source of truth for workloads that span companies, geographies, and
clouds while retaining the independence of each party's IT operations. However,
so far production applications have remained rare, stymied by technical
limitations of existing blockchain technologies and challenges with their
integration into enterprises' IT systems. In this paper, we collect
enterprises' requirements on distributed ledgers for data sharing and
integration from a technical perspective, argue that they are not sufficiently
addressed by available blockchain frameworks, and propose a novel distributed
ledger design that is ""serverless"", i.e., built on cloud-native resources. We
evaluate its qualitative and quantitative properties and give evidence that
enterprises already heavily reliant on cloud service providers would consider
such an approach acceptable, particularly if it offers ease of deployment, low
transactional cost structure, and a combination of latency and scalability
aligned with real-time IT application needs.",http://arxiv.org/abs/2110.09221v1,arXiv
Information Systems,Enterprise Systems,Mixed Methods,Multi-variable Adversarial Time-Series Forecast Model,"Short-term industrial enterprises power system forecasting is an important
issue for both load control and machine protection. Scientists focus on load
forecasting but ignore other valuable electric-meters which should provide
guidance of power system protection. We propose a new framework, multi-variable
adversarial time-series forecasting model, which regularizes Long Short-term
Memory (LSTM) models via an adversarial process. The novel model forecasts all
variables (may in different type, such as continue variables, category
variables, etc.) in power system at the same time and helps trade-off process
between forecasting accuracy of single variable and variable-variable
relations. Experiments demonstrate the potential of the framework through
qualitative and quantitative evaluation of the generated samples. The predict
results of electricity consumption of industrial enterprises by multi-variable
adversarial time-series forecasting model show that the proposed approach is
able to achieve better prediction accuracy. We also applied this model to real
industrial enterprises power system data we gathered from several large
industrial enterprises via advanced power monitors, and got impressed
forecasting results.",http://arxiv.org/abs/2406.00596v1,arXiv
Information Systems,Enterprise Systems,Mixed Methods,Key Success Factors of Enterprise Risk Management Systems: Listed Polish Companies,"<jats:p>Purpose: The main purpose of the article is to determine the key success factors of enterprise risk management systems, understood as the characteristics of these systems that have the greatest impact on the effectiveness of their functioning.  Methodology: Bearing in mind the most accurate determination of key success factors of enterprise risk management systems, I used several research methods. The conducted research was divided into four stages: (1) literature review, (2) financial statements analysis, (3) individual in-depth interviews, and (4) anonymous surveys. The research embraced enterprises operating in Poland.  Findings: Based on the literature analysis, in-depth interviews, and conducted surveys, a list of risk management systems’ success factors was created and sorted in the order from the most important ones – that have the greatest impact on the success of risk management – to the least important ones.  Additional analysis of financial statements of all WSE-listed companies allowed me to discern that few of the surveyed companies use mature modern ERM systems, and it enabled me to identify a group of companies that qualified to participate in the survey. Moreover, a statistically significant positive correlation appeared between the degree of key success factors’ implementation and the overall ERM implementation’s impact on the organization, while a statistically significant negative correlation emerged between the overall impact of ERM implementation on the organization and the degree of ERM implementation goals’ achievement, but also between the degree of the implementation of key success factors and the degree of the ERM implementation objectives’ implementation. Moreover, I noted that the fact of using individual features has no significant impact on the assessment of a given feature by respondents.  Implications: All factors included in the study are success factors of risk management systems. However, the surveys’ results suggest a different level of individual factors’ significance, thus a different degree of these factors impact on risk management success.  Originality/Value: The article presents an original set of key success factors in risk management systems, created based on my own research. The obtained research results can be used by managers willing to implement or develop risk management systems in their organizations.</jats:p>",https://doi.org/10.7206/cemj.2658-0845.71,CrossRef
Information Systems,Enterprise Systems,Mixed Methods,Enterprise Systems Outsourcing “Behind the Curtain”,"<p>Outsourcing is now a feasible means for enterprise systems (ES) cost savings, but does however increase the complexity of coordination substantially when many organizations are involved. We set out to study ES outsourcing in a large Scandinavian high-tech organization, SCANDI, a case setting with many inter-organizational partners, trying to answer the question: Why does SCANDI engage in these very complex outsourcing arrangements? To answer this question we have analyzed documents, observed meetings and gathered data from interviews in four parts of SCANDI. The first data analysis found just the rational front stage cost-saving explanation; but then, with a more careful analysis focusing on institutional factors, other backstage explanations “behind the curtain” were uncovered, such as management consultants with a “best practice” agenda, people promoting outsourcing, thereby being promoted themselves, and a belief in outsourcing as a “silver bullet”: a recipe to success, solving everything</p>",https://doi.org/10.4018/jeis.2011010101,CrossRef
Information Systems,Enterprise Systems,Mixed Methods,"Assessing the Impact of System Quality, Information Quality, and Service Quality on Enterprise Resource Planning (ERP) Systems","<p>Globally, governments are taking steps to help them increase their income generation margin by implementing tax administrative ERP systems. However, the impacts on the internal system users of these ERP system quality features have not drawn the attention needed. This study, therefore, examines the relationship between the information systems' (IS) quality and individual impact using the theoretical foundation of the DeLone and McLean IS success model and, secondly, addresses the interrelationships between the quality constructs of information systems (IS). The authors also used the structural equation modeling technique of partial least squares to evaluate and analyze the data. The results show that system quality, the information quality, and the service quality characteristics of the tax administrative ERP system have a strong positive impact on the success of the IS at the individual level. There is also a positive relationship between the information systems' (IS) quality construction. The results provide additional empirical observations and consequences for management.</p>",https://doi.org/10.4018/ijeis.2021100104,CrossRef
Information Systems,Enterprise Systems,Design and Development,Comparative image filter-ing using monotonic morphological operators,"<jats:p>In a previous work, we proposed a Comparative Morphology (CM) construction scheme that generalized Pytyev's Morphological Image Analysis approach onto a wider range of practical image comparison applications. Within a Guided Contrasting framework, a filtering procedure and a related change detection algorithm were developed. In this work, we propose a class of CM filtering in which Mathematical Morphology operators introduced by Serra are used as smoothing operators that offer monotonically non-increasing (non-decreasing) filtering, in contrast to linear diffusion filtering and non-linear median filtering. The results of experiments on change detection based on the new CM filtering are discussed in comparison with other morphological procedures.</jats:p>",https://doi.org/10.18287/2412-6179-2018-42-2-306-311,CrossRef
Information Systems,Enterprise Systems,Design and Development,"Enterprise Architecture, Enterprise Information Systems and Enterprise Integration: A Review Based on Systems Theory Perspective","<jats:p>Systems theory is one of the most important and well-used concept to explain the phenomenon in social sciences. Therefore, systems science plays an important role in explaining many of the phenomena in information systems research. Enterprise Systems (ES), Enterprise Information Systems (EIS) and Enterprise Architecture (EA) are three such emerging technologies in which systems’ perspective plays an important role in explaining the growth and development of these technologies. However, there is lack of literature that illustrates the development and the impact of systems science in these three technologies. This research carefully collects and studies 106 existing literature in the field of ES, EA and EIS, and a summary review of all the latest developments in the ways systems theory has been implemented to these three fields as well as different areas of these three technologies. In the conclusion, three future trends are concluded from the review.</jats:p>",https://doi.org/10.1142/s2424862219500015,CrossRef
Information Systems,Enterprise Systems,Design and Development,Continuous Computing Technologies for Improving Performances of Enterprise Information Systems,"<p>Business computing has evolved into an organizational engine that drives business and provides a powerful source for competitive advantage. In order to achieve higher levels of competitiveness, business has to be continuous from a data availability perspective and agile with regard to data access. Simply put, system and application downtime are not an option in modern business since each hour, or even each minute, of downtime may generate negative financial effects. An enterprise information system (EIS) can be qualified as “high-quality” in terms of its architecture, application platform and information it can provide to users but if that information is unavailable when it is needed by customer, manager or any other end user, the value of that EIS simply becomes “zeroed” from end users’ point of view. The paper presents a framework for implementation of continuous computing technologies (CCT) for improving performances of enterprise information systems from business continuance perspective. It identifies high system availability and agile data access as two critical attributes (measures of performances) in evaluating performances of enterprise information systems. The framework is based on a MS/OR-based definition of a system given by Churchman (1968, 1971). In addition, it proposes a set of IT drivers for enhancing the performances of enterprise information systems from business continuity and business agility perspectives.</p>",https://doi.org/10.4018/jeis.2005100105,CrossRef
Information Systems,Enterprise Systems,Design and Development,A Complex Adaptive Systems-Based Enterprise Knowledge Sharing Model,<p>This article describes a complex adaptive system (CAS)-based enterprise knowledge-sharing (KnS) model. The CAS-based enterprise KnS model consists of a CAS-based KnS framework and a multi-agent simulation model. Enterprise knowledge sharing is modeled as the emergent behavior of knowledge workers interacting with the KnS environment and other knowledge workers. The CAS-based enterprise KnS model is developed to aid knowledge management (KM) leadership and other KnS researchers in gaining an enhanced understanding of KnS behavior and its influences. A premise of this research is that a better understanding of KnS influences can result in enhanced decision-making of KnS interventions that can result in improvements in KnS behavior.</p>,https://doi.org/10.4018/jeis.2009040102,CrossRef
Information Systems,Enterprise Systems,Design and Development,Detecting image differences based on reference EMD-filters,"<jats:p>In our previous works dealing with detecting differences in images in the case of substantial variations in brightness and geometry of an object, we proposed a morphological scheme for image analysis based on diffusion and reference mosaic filters. The filters were defined through the heat kernels of the similarity of image fragments. In this paper, we propose an implementation of this morphological scheme using original reference Earth mover's distance (EMD) filters, in which the optimal matrices of the mutual similarity of mosaic forms are calculated through a linear programming method.</jats:p>",https://doi.org/10.18287/2412-6179-2018-42-2-291-296,CrossRef
Information Systems,Enterprise Systems,Theoretical / Conceptual,People-Oriented Enterprise Information Systems,"<p>Current notations and languages do not emphasize the participation of users in business processes and consider them essentially as service providers. Moreover, they follow a centralized approach as all the interactions originate from or end in a business process; direct interactions between users cannot be represented. What is missing from this approach is that human work is cooperative and cooperation takes place through structured interactions called conversations; the notion of conversation is at the center of the language/action perspective. However, the problem of effectively integrating conversations and business processes is still open and this chapter proposes a notation called POBPN (People-Oriented Business Process Notation) and a perspective, referred to as conversation-oriented perspective, for its solution.</p>",https://doi.org/10.4018/jeis.2009090202,CrossRef
Information Systems,Enterprise Systems,Theoretical / Conceptual,Authority and Its Implementation in Enterprise Information Systems,"<p>The concept of power is inherent in human organizations of any type. As power relations have important consequences for organizational viability and productivity, they should be explicitly represented in enterprise information systems (EISs). Although organization theory provides a rich and very diverse theoretical basis on organizational power, still most of the definitions for power-related concepts are too abstract, often vague and ambiguous to be directly implemented in EISs. To create a bridge between informal organization theories and automated EISs, this article proposes a formal logic-based specification language for representing power (in particular authority) relations. The use of the language is illustrated by considering authority structures of organizations of different types. Moreover, the article demonstrates how the formalized authority relations can be integrated into an EIS.</p>",https://doi.org/10.4018/jeis.2008070105,CrossRef
Information Systems,Enterprise Systems,Theoretical / Conceptual,Ontology-Based Knowledge Management for Enterprise Systems,"<p>Companies face the challenges of expanding their markets, improving products, services and processes, and exploiting intellectual capital in a dynamic network. Therefore, more companies are turning to an Enterprise System (ES). Knowledge management (KM) has also received considerable attention and is continuously gaining the interest of industry, enterprises, and academia. For ES, KM can provide support across the entire lifecycle, from selection and implementation to use. In addition, it is also recognised that an ontology is an appropriate methodology to accomplish a common consensus of communication, as well as to support a diversity of KM activities, such as knowledge repository, retrieval, sharing, and dissemination. This paper examines the role of ontology-based KM for ES (OKES) and investigates the possible integration of ontology-based KM and ES. The authors develop a taxonomy as a framework for understanding OKES research. In order to achieve the objective of this study, a systematic review of existing research was conducted. Based on a theoretical framework of the ES lifecycle, KM, KM for ES, ontology, and ontology-based KM, guided by the framework of study, a taxonomy for OKES is established.</p>",https://doi.org/10.4018/jeis.2011100104,CrossRef
Information Systems,Enterprise Systems,Theoretical / Conceptual,Using Institutional Theory in Enterprise Systems Research,"<p>This paper examines the use of institutional theory as a conceptually rich lens to study social issues of enterprise systems (ES) research. More precisely, the purpose is to categorize current ES research using institutional theory to develop a conceptual model that advances ES research. Key institutional features are presented such as isomorphism, rationalized myths, and bridging macro and micro structures, and institutional logics and their implications for ES research are discussed. Through a literature review of 181 articles, of which 18 papers are selected, the author’s built a conceptual model that advocates multi-level and multi-theory approaches and applies newer institutional aspects such as institutional logics. The findings show that institutional theory in ES research is in its infancy and adopts mainly traditional institutional aspects like isomorphism, with the organization as the level of analysis, and in several cases it is complemented by structuration theory and other theories.</p>",https://doi.org/10.4018/jeis.2013010101,CrossRef
Information Systems,Enterprise Systems,Theoretical / Conceptual,The Impact of Enterprise Systems on Organizational Control and Drift,"<p>Enterprise systems (ES) are widespread in current organizations, and seen as integrating organizational procedures across functional divisions. An enterprise system (also known as enterprise resource planning – ERP system), once installed, seems to enable or constrain certain actions by managers and users, which have an impact on organizational operations. Those actions may result in increased organizational control, or may lead to organizational drift. The processes that give rise to such outcomes are investigated in this paper, which is based on a field study of five companies. By drawing on the theoretical concepts of human and machine agencies, as well as the embedding and disembedding of managerial and user actions in the system, this paper agues that control and drift arising from the use of an enterprise system are outcomes of the processes of embedding and disembedding human actions, which are afforded (enabled or constrained) by the enterprise system.</p>",https://doi.org/10.4018/jeis.2007070103,CrossRef
Information Systems,Decision Support Systems,Quantitative,"Decision Support Systems in Fisheries and Aquaculture: A systematic
  review","Decision support systems help decision makers make better decisions in the
face of complex decision problems (e.g. investment or policy decisions).
Fisheries and Aquaculture is a domain where decision makers face such decisions
since they involve factors from many different scientific fields. No systematic
overview of literature describing decision support systems and their
application in fisheries and aquaculture has been conducted. This paper
summarizes scientific literature that describes decision support systems
applied to the domain of Fisheries and Aquaculture. We use an established
systematic mapping survey method to conduct our literature mapping. Our
research questions are: What decision support systems for fisheries and
aquaculture exists? What are the most investigated fishery and aquaculture
decision support systems topics and how have these changed over time? Do any
current DSS for fisheries provide real- time analytics? Do DSSes in Fisheries
and Aquaculture build their models using machine learning done on captured and
grounded data? The paper then detail how we employ the systematic mapping
method in answering these questions. This results in 27 papers being identified
as relevant and gives an exposition on the primary methods concluded in the
study for designing a decision support system. We provide an analysis of the
research done in the studies collected. We discovered that most literature does
not consider multiple aspects for multiple stakeholders in their work. In
addition we observed that little or no work has been done with real-time
analysis in these decision support systems.",http://arxiv.org/abs/1611.08374v1,arXiv
Information Systems,Decision Support Systems,Quantitative,Towards Decision Support Technology Platform for Modular Systems,"The survey methodological paper addresses a glance to a general decision
support platform technology for modular systems (modular/composite
alterantives/solutions) in various applied domains. The decision support
platform consists of seven basic combinatorial engineering frameworks (system
synthesis, system modeling, evaluation, detection of bottleneck,
improvement/extension, multistage design, combinatorial evolution and
forecasting). The decision support platform is based on decision support
procedures (e.g., multicriteria selection/sorting, clustering), combinatorial
optimization problems (e.g., knapsack, multiple choice problem, clique,
assignment/allocation, covering, spanning trees), and their combinations. The
following is described: (1) general scheme of the decision support platform
technology; (2) brief descriptions of modular (composite) systems (or composite
alternatives); (3) trends in moving from chocie/selection of alternatives to
processing of composite alternatives which correspond to hierarchical modular
products/systems; (4) scheme of resource requirements (i.e., human,
information-computer); and (5) basic combinatorial engineering frameworks and
their applications in various domains.",http://arxiv.org/abs/1408.5492v1,arXiv
Information Systems,Decision Support Systems,Quantitative,Revealing Sub-Optimality Conditions of Strategic Decisions,"Conceptual view of fitness and fitness measurement of strategic decisions on
information systems, technological systems and innovation are becoming more
important in recent years. This paper determines some dynamics of fitness
landscape which are lead to termination of decision makers' research before
reaching the global maximum in strategic decisions. These dynamics are
specified according to management decision making models and supported with
simulation results. This article determines simulation results by means of
""Fitness Value"" and ""Probability of Optimality"". Correlation between these two
concepts may be remarkable according to revealing optimal values in innovative
and research-based decision making approaches beside sub-optimal results of
traditional decision making approaches.",http://arxiv.org/abs/1107.0202v1,arXiv
Information Systems,Decision Support Systems,Quantitative,"An Example for BeSpaceD and its Use for Decision Support in Industrial
  Automation","We describe our formal methods-based spatial reasoning framework BeSpaceD and
its application in decision support for industrial automation. In particular we
are supporting analysis and decisions based on formal models for industrial
plant and mining operations. BeSpaceD is a framework for deciding geometric and
topological properties of spatio-temporal models. We present an example and
report on our ongoing experience with applications in different projects around
software and cyber-physical systems engineering. The example features
abstracted aspects of a production plant model. Using the example we motivate
the use of our framework in the context of an existing software platform
supporting monitoring, incident handling and maintenance of industrial
automation facilities in remote locations.",http://arxiv.org/abs/1512.04656v1,arXiv
Information Systems,Decision Support Systems,Quantitative,Note on Thompson sampling for large decision problems,"There is increasing interest in using streaming data to inform decision
making across a wide range of application domains including mobile health, food
safety, security, and resource management. A decision support system formalizes
online decision making as a map from up-to-date information to a recommended
decision. Online estimation of an optimal decision strategy from streaming data
requires simultaneous estimation of components of the underlying system
dynamics as well as the optimal decision strategy given these dynamics; thus,
there is an inherent trade-off between choosing decisions that lead to improved
estimates and choosing decisions that appear to be optimal based on current
estimates. Thompson (1933) was among the first to formalize this trade-off in
the context of choosing between two treatments for a stream of patients; he
proposed a simple heuristic wherein a treatment is selected randomly at each
time point with selection probability proportional to the posterior probability
that it is optimal. We consider a variant of Thompson sampling that is simple
to implement and can be applied to large and complex decision problems. We show
that the proposed Thompson sampling estimator is consistent for the optimal
decision support system and provide rates of convergence and finite sample
error bounds. The proposed algorithm is illustrated using an agent-based model
of the spread of influenza on a network and management of mallard populations
in the United States.",http://arxiv.org/abs/1905.04735v1,arXiv
Information Systems,Decision Support Systems,Qualitative,"Beyond Recommendations: From Backward to Forward AI Support of Pilots'
  Decision-Making Process","AI is anticipated to enhance human decision-making in high-stakes domains
like aviation, but adoption is often hindered by challenges such as
inappropriate reliance and poor alignment with users' decision-making. Recent
research suggests that a core underlying issue is the recommendation-centric
design of many AI systems, i.e., they give end-to-end recommendations and
ignore the rest of the decision-making process. Alternative support paradigms
are rare, and it remains unclear how the few that do exist compare to
recommendation-centric support. In this work, we aimed to empirically compare
recommendation-centric support to an alternative paradigm, continuous support,
in the context of diversions in aviation. We conducted a mixed-methods study
with 32 professional pilots in a realistic setting. To ensure the quality of
our study scenarios, we conducted a focus group with four additional pilots
prior to the study. We found that continuous support can support pilots'
decision-making in a forward direction, allowing them to think more beyond the
limits of the system and make faster decisions when combined with
recommendations, though the forward support can be disrupted. Participants'
statements further suggest a shift in design goal away from providing
recommendations, to supporting quick information gathering. Our results show
ways to design more helpful and effective AI decision support that goes beyond
end-to-end recommendations.",http://arxiv.org/abs/2406.08959v3,arXiv
Information Systems,Decision Support Systems,Qualitative,"Exploring the Requirements of Clinicians for Explainable AI Decision
  Support Systems in Intensive Care","There is a growing need to understand how digital systems can support
clinical decision-making, particularly as artificial intelligence (AI) models
become increasingly complex and less human-interpretable. This complexity
raises concerns about trustworthiness, impacting safe and effective adoption of
such technologies. Improved understanding of decision-making processes and
requirements for explanations coming from decision support tools is a vital
component in providing effective explainable solutions. This is particularly
relevant in the data-intensive, fast-paced environments of intensive care units
(ICUs). To explore these issues, group interviews were conducted with seven ICU
clinicians, representing various roles and experience levels. Thematic analysis
revealed three core themes: (T1) ICU decision-making relies on a wide range of
factors, (T2) the complexity of patient state is challenging for shared
decision-making, and (T3) requirements and capabilities of AI decision support
systems. We include design recommendations from clinical input, providing
insights to inform future AI systems for intensive care.",http://arxiv.org/abs/2411.11774v1,arXiv
Information Systems,Decision Support Systems,Qualitative,A Typology of Decision-Making Tasks for Visualization,"Despite decision-making being a vital goal of data visualization, little work
has been done to differentiate decision-making tasks within the field. While
visualization task taxonomies and typologies exist, they often focus on more
granular analytical tasks that are too low-level to describe large complex
decisions, which can make it difficult to reason about and design
decision-support tools. In this paper, we contribute a typology of
decision-making tasks that were iteratively refined from a list of design goals
distilled from a literature review. Our typology is concise and consists of
only three tasks: CHOOSE, ACTIVATE, and CREATE. Although decision types
originating in other disciplines exist, we provide definitions for these tasks
that are suitable for the visualization community. Our proposed typology offers
two benefits. First, the ability to compose and hierarchically organize the
tasks enables flexible and clear descriptions of decisions with varying levels
of complexities. Second, the typology encourages productive discourse between
visualization designers and domain experts by abstracting the intricacies of
data, thereby promoting clarity and rigorous analysis of decision-making
processes. We demonstrate the benefits of our typology through four case
studies, and present an evaluation of the typology from semi-structured
interviews with experienced members of the visualization community who have
contributed to developing or publishing decision support systems for domain
experts. Our interviewees used our typology to delineate the decision-making
processes supported by their systems, demonstrating its descriptive capacity
and effectiveness. Finally, we present preliminary findings on the usefulness
of our typology for visualization design.",http://arxiv.org/abs/2404.08812v3,arXiv
Information Systems,Decision Support Systems,Qualitative,"""If I Had All the Time in the World"": Ophthalmologists' Perceptions of
  Anchoring Bias Mitigation in Clinical AI Support","Clinical needs and technological advances have resulted in increased use of
Artificial Intelligence (AI) in clinical decision support. However, such
support can introduce new and amplify existing cognitive biases. Through
contextual inquiry and interviews, we set out to understand the use of an
existing AI support system by ophthalmologists. We identified concerns
regarding anchoring bias and a misunderstanding of the AI's capabilities.
Following, we evaluated clinicians' perceptions of three bias mitigation
strategies as integrated into their existing decision support system. While
clinicians recognised the danger of anchoring bias, we identified a concern
around the impact of bias mitigation on procedure time. Our participants were
divided in their expectations of any positive impact on diagnostic accuracy,
stemming from varying reliance on the decision support. Our results provide
insights into the challenges of integrating bias mitigation into AI decision
support.",http://arxiv.org/abs/2303.03981v1,arXiv
Information Systems,Decision Support Systems,Qualitative,"Fiduciary Responsibility: Facilitating Public Trust in Automated
  Decision Making","Automated decision-making systems are being increasingly deployed and affect
the public in a multitude of positive and negative ways. Governmental and
private institutions use these systems to process information according to
certain human-devised rules in order to address social problems or
organizational challenges. Both research and real-world experience indicate
that the public lacks trust in automated decision-making systems and the
institutions that deploy them. The recreancy theorem argues that the public is
more likely to trust and support decisions made or influenced by automated
decision-making systems if the institutions that administer them meet their
fiduciary responsibility. However, often the public is never informed of how
these systems operate and resultant institutional decisions are made. A ``black
box'' effect of automated decision-making systems reduces the public's
perceptions of integrity and trustworthiness. The result is that the public
loses the capacity to identify, challenge, and rectify unfairness or the costs
associated with the loss of public goods or benefits.
  The current position paper defines and explains the role of fiduciary
responsibility within an automated decision-making system. We formulate an
automated decision-making system as a data science lifecycle (DSL) and examine
the implications of fiduciary responsibility within the context of the DSL.
Fiduciary responsibility within DSLs provides a methodology for addressing the
public's lack of trust in automated decision-making systems and the
institutions that employ them to make decisions affecting the public. We posit
that fiduciary responsibility manifests in several contexts of a DSL, each of
which requires its own mitigation of sources of mistrust. To instantiate
fiduciary responsibility, a Los Angeles Police Department (LAPD) predictive
policing case study is examined.",http://arxiv.org/abs/2301.10001v1,arXiv
Information Systems,Decision Support Systems,Mixed Methods,"Adaptive questionnaires for facilitating patient data entry in clinical
  decision support systems: Methods and application to STOPP/START v2","Clinical decision support systems are software tools that help clinicians to
make medical decisions. However, their acceptance by clinicians is usually
rather low. A known problem is that they often require clinicians to manually
enter lots of patient data, which is long and tedious. Existing solutions, such
as the automatic data extraction from electronic health record, are not fully
satisfying, because of low data quality and availability. In practice, many
systems still include long questionnaire for data entry.
  In this paper, we propose an original solution to simplify patient data
entry, using an adaptive questionnaire, i.e. a questionnaire that evolves
during user interaction, showing or hiding questions dynamically. Considering a
rule-based decision support systems, we designed methods for translating the
system's clinical rules into display rules that determine the items to show in
the questionnaire, and methods for determining the optimal order of priority
among the items in the questionnaire. We applied this approach to a decision
support system implementing STOPP/START v2, a guideline for managing
polypharmacy. We show that it permits reducing by about two thirds the number
of clinical conditions displayed in the questionnaire. Presented to clinicians
during focus group sessions, the adaptive questionnaire was found ""pretty easy
to use"". In the future, this approach could be applied to other guidelines, and
adapted for data entry by patients.",http://arxiv.org/abs/2309.10398v1,arXiv
Information Systems,Decision Support Systems,Mixed Methods,PERFEX: Classifier Performance Explanations for Trustworthy AI Systems,"Explainability of a classification model is crucial when deployed in
real-world decision support systems. Explanations make predictions actionable
to the user and should inform about the capabilities and limitations of the
system. Existing explanation methods, however, typically only provide
explanations for individual predictions. Information about conditions under
which the classifier is able to support the decision maker is not available,
while for instance information about when the system is not able to
differentiate classes can be very helpful. In the development phase it can
support the search for new features or combining models, and in the operational
phase it supports decision makers in deciding e.g. not to use the system. This
paper presents a method to explain the qualities of a trained base classifier,
called PERFormance EXplainer (PERFEX). Our method consists of a meta tree
learning algorithm that is able to predict and explain under which conditions
the base classifier has a high or low error or any other classification
performance metric. We evaluate PERFEX using several classifiers and datasets,
including a case study with urban mobility data. It turns out that PERFEX
typically has high meta prediction performance even if the base classifier is
hardly able to differentiate classes, while giving compact performance
explanations.",http://arxiv.org/abs/2212.06045v1,arXiv
Information Systems,Decision Support Systems,Mixed Methods,"Towards Next-Generation Urban Decision Support Systems through
  AI-Powered Construction of Scientific Ontology using Large Language Models --
  A Case in Optimizing Intermodal Freight Transportation","The incorporation of Artificial Intelligence (AI) models into various
optimization systems is on the rise. Yet, addressing complex urban and
environmental management problems normally requires in-depth domain science and
informatics expertise. This expertise is essential for deriving data and
simulation-driven for informed decision support. In this context, we
investigate the potential of leveraging the pre-trained Large Language Models
(LLMs). By adopting ChatGPT API as the reasoning core, we outline an integrated
workflow that encompasses natural language processing, methontology-based
prompt tuning, and transformers. This workflow automates the creation of
scenario-based ontology using existing research articles and technical manuals
of urban datasets and simulations. The outcomes of our methodology are
knowledge graphs in widely adopted ontology languages (e.g., OWL, RDF, SPARQL).
These facilitate the development of urban decision support systems by enhancing
the data and metadata modeling, the integration of complex datasets, the
coupling of multi-domain simulation models, and the formulation of
decision-making metrics and workflow. The feasibility of our methodology is
evaluated through a comparative analysis that juxtaposes our AI-generated
ontology with the well-known Pizza Ontology employed in tutorials for popular
ontology software (e.g., prot\'eg\'e). We close with a real-world case study of
optimizing the complex urban system of multi-modal freight transportation by
generating anthologies of various domain data and simulations to support
informed decision-making.",http://arxiv.org/abs/2405.19255v3,arXiv
Information Systems,Decision Support Systems,Mixed Methods,"Do Expressions Change Decisions? Exploring the Impact of AI's
  Explanation Tone on Decision-Making","Explanatory information helps users to evaluate the suggestions offered by
AI-driven decision support systems. With large language models, adjusting
explanation expressions has become much easier. However, how these expressions
influence human decision-making remains largely unexplored. This study
investigated the effect of explanation tone (e.g., formal or humorous) on
decision-making, focusing on AI roles and user attributes. We conducted user
experiments across three scenarios depending on AI roles (assistant,
second-opinion provider, and expert) using datasets designed with varying
tones. The results revealed that tone significantly influenced decision-making
regardless of user attributes in the second-opinion scenario, whereas its
impact varied by user attributes in the assistant and expert scenarios. In
addition, older users were more influenced by tone, and highly extroverted
users exhibited discrepancies between their perceptions and decisions.
Furthermore, open-ended questionnaires highlighted that users expect tone
adjustments to enhance their experience while emphasizing the importance of
tone consistency and ethical considerations. Our findings provide crucial
insights into the design of explanation expressions.",http://arxiv.org/abs/2502.19730v1,arXiv
Information Systems,Decision Support Systems,Mixed Methods,"OCEP: An Ontology-Based Complex Event Processing Framework for
  Healthcare Decision Support in Big Data Analytics","The exponential expansion of real-time data streams across multiple domains
needs the development of effective event detection, correlation, and
decision-making systems. However, classic Complex Event Processing (CEP)
systems struggle with semantic heterogeneity, data interoperability, and
knowledge driven event reasoning in Big Data environments. To solve these
challenges, this research work presents an Ontology based Complex Event
Processing (OCEP) framework, which utilizes semantic reasoning and Big Data
Analytics to improve event driven decision support. The proposed OCEP
architecture utilizes ontologies to support reasoning to event streams. It
ensures compatibility with different data sources and lets you find the events
based on the context. The Resource Description Framework (RDF) organizes event
data, and SPARQL query enables rapid event reasoning and retrieval. The
approach is implemented within the Hadoop environment, which consists of Hadoop
Distributed File System (HDFS) for scalable storage and Apache Kafka for
real-time CEP based event execution. We perform a real-time healthcare analysis
and case study to validate the model, utilizing IoT sensor data for illness
monitoring and emergency responses. This OCEP framework successfully integrates
several event streams, leading to improved early disease detection and aiding
doctors in decision-making. The result shows that OCEP predicts event detection
with an accuracy of 85%. This research work utilizes an OCEP to solve the
problems with semantic interoperability and correlation of complex events in
Big Data analytics. The proposed architecture presents an intelligent, scalable
and knowledge driven event processing framework for healthcare based decision
support.",http://arxiv.org/abs/2503.21453v1,arXiv
Information Systems,Decision Support Systems,Design and Development,"Towards a Reliable Framework of Uncertainty-Based Group Decision Support
  System","This study proposes a framework of Uncertainty-based Group Decision Support
System (UGDSS). It provides a platform for multiple criteria decision analysis
in six aspects including (1) decision environment, (2) decision problem, (3)
decision group, (4) decision conflict, (5) decision schemes and (6) group
negotiation. Based on multiple artificial intelligent technologies, this
framework provides reliable support for the comprehensive manipulation of
applications and advanced decision approaches through the design of an
integrated multi-agents architecture.",http://arxiv.org/abs/1107.0089v1,arXiv
Information Systems,Decision Support Systems,Design and Development,"Stakeholder-in-the-Loop Fair Decisions: A Framework to Design Decision
  Support Systems in Public and Private Organizations","Due to the opacity of machine learning technology, there is a need for
explainability and fairness in the decision support systems used in public or
private organizations. Although the criteria for appropriate explanations and
fair decisions change depending on the values of those who are affected by the
decisions, there is a lack of discussion framework to consider the appropriate
outputs for each stakeholder. In this paper, we propose a discussion framework
that we call ""stakeholder-in-the-loop fair decisions."" This is proposed to
consider the requirements for appropriate explanations and fair decisions. We
identified four stakeholders that need to be considered to design accountable
decision support systems and discussed how to consider the appropriate outputs
for each stakeholder by referring to our works. By clarifying the
characteristics of specific stakeholders in each application domain and
integrating the stakeholders' values into outputs that all stakeholders agree
upon, decision support systems can be designed as systems that ensure
accountable decision makings.",http://arxiv.org/abs/2308.01163v1,arXiv
Information Systems,Decision Support Systems,Design and Development,An Ontology-driven Framework for Supporting Complex Decision Process,"The study proposes a framework of ONTOlogy-based Group Decision Support
System (ONTOGDSS) for decision process which exhibits the complex structure of
decision-problem and decision-group. It is capable of reducing the complexity
of problem structure and group relations. The system allows decision makers to
participate in group decision-making through the web environment, via the
ontology relation. It facilitates the management of decision process as a
whole, from criteria generation, alternative evaluation, and opinion
interaction to decision aggregation. The embedded ontology structure in
ONTOGDSS provides the important formal description features to facilitate
decision analysis and verification. It examines the software architecture, the
selection methods, the decision path, etc. Finally, the ontology application of
this system is illustrated with specific real case to demonstrate its
potentials towards decision-making development.",http://arxiv.org/abs/1107.2997v1,arXiv
Information Systems,Decision Support Systems,Design and Development,Agile System Development Lifecycle for AI Systems: Decision Architecture,"Agile system development life cycle (SDLC) focuses on typical functional and
non-functional system requirements for developing traditional software systems.
However, Artificial Intelligent (AI) systems are different in nature and have
distinct attributes such as (1) autonomy, (2) adaptiveness, (3) content
generation, (4) decision-making, (5) predictability and (6) recommendation.
Agile SDLC needs to be enhanced to support the AI system development and
ongoing post-deployment adaptation. The challenge is: how can agile SDLC be
enhanced to support AI systems? The scope of this paper is limited to AI system
enabled decision automation. Thus, this paper proposes the use of decision
science to enhance the agile SDLC to support the AI system development.
Decision science is the study of decision-making, which seems useful to
identify, analyse and describe decisions and their architecture subject to
automation via AI systems. Specifically, this paper discusses the decision
architecture in detail within the overall context of agile SDLC for AI systems.
The application of the proposed approach is demonstrated with the help of an
example scenario of insurance claim processing. This initial work indicated the
usability of a decision science to enhancing the agile SDLC for designing and
implementing the AI systems for decision-automation. This work provides an
initial foundation for further work in this new area of decision architecture
and agile SDLC for AI systems.",http://arxiv.org/abs/2501.09434v2,arXiv
Information Systems,Decision Support Systems,Design and Development,A hybrid decision support system : application on healthcare,"Many systems based on knowledge, especially expert systems for medical
decision support have been developed. Only systems are based on production
rules, and cannot learn and evolve only by updating them. In addition, taking
into account several criteria induces an exorbitant number of rules to be
injected into the system. It becomes difficult to translate medical knowledge
or a support decision as a simple rule. Moreover, reasoning based on generic
cases became classic and can even reduce the range of possible solutions. To
remedy that, we propose an approach based on using a multi-criteria decision
guided by a case-based reasoning (CBR) approach.",http://arxiv.org/abs/1311.4086v1,arXiv
Information Systems,Decision Support Systems,Theoretical / Conceptual,Introduction to Multi-Agent Simulation,"When designing systems that are complex, dynamic and stochastic in nature,
simulation is generally recognised as one of the best design support
technologies, and a valuable aid in the strategic and tactical decision making
process. A simulation model consists of a set of rules that define how a system
changes over time, given its current state. Unlike analytical models, a
simulation model is not solved but is run and the changes of system states can
be observed at any point in time. This provides an insight into system dynamics
rather than just predicting the output of a system based on specific inputs.
Simulation is not a decision making tool but a decision support tool, allowing
better informed decisions to be made. Due to the complexity of the real world,
a simulation model can only be an approximation of the target system. The
essence of the art of simulation modelling is abstraction and simplification.
Only those characteristics that are important for the study and analysis of the
target system should be included in the simulation model.",http://arxiv.org/abs/0803.3905v1,arXiv
Information Systems,Decision Support Systems,Theoretical / Conceptual,Symbolic Decision Theory and Autonomous Systems,"The ability to reason under uncertainty and with incomplete information is a
fundamental requirement of decision support technology. In this paper we argue
that the concentration on theoretical techniques for the evaluation and
selection of decision options has distracted attention from many of the wider
issues in decision making. Although numerical methods of reasoning under
uncertainty have strong theoretical foundations, they are representationally
weak and only deal with a small part of the decision process. Knowledge based
systems, on the other hand, offer greater flexibility but have not been
accompanied by a clear decision theory. We describe here work which is under
way towards providing a theoretical framework for symbolic decision procedures.
A central proposal is an extended form of inference which we call
argumentation; reasoning for and against decision options from generalised
domain theories. The approach has been successfully used in several decision
support applications, but it is argued that a comprehensive decision theory
must cover autonomous decision making, where the agent can formulate questions
as well as take decisions. A major theoretical challenge for this theory is to
capture the idea of reflection to permit decision agents to reason about their
goals, what they believe and why, and what they need to know or do in order to
achieve their goals.",http://arxiv.org/abs/1303.5716v1,arXiv
Information Systems,Decision Support Systems,Theoretical / Conceptual,A Social Network for Societal-Scale Decision-Making Systems,"In societal-scale decision-making systems the collective is faced with the
problem of ensuring that the derived group decision is in accord with the
collective's intention. In modern systems, political institutions have
instatiated representative forms of decision-making to ensure that every
individual in the society has a participatory voice in the decision-making
behavior of the whole--even if only indirectly through representation. An
agent-based simulation demonstrates that in modern representative systems, as
the ratio of representatives increases, there exists an exponential decrease in
the ability for the group to behave in accord with the desires of the whole. To
remedy this issue, this paper provides a novel representative power structure
for decision-making that utilizes a social network and power distribution
algorithm to maintain the collective's perspective over varying degrees of
participation and/or ratios of representation. This work shows promise for the
future development of policy-making systems that are supported by the computer
and network infrastructure of our society.",http://arxiv.org/abs/cs/0412047v1,arXiv
Information Systems,Decision Support Systems,Theoretical / Conceptual,"Extracting Process-Aware Decision Models from Object-Centric Process
  Data","Organizations execute decisions within business processes on a daily basis
whilst having to take into account multiple stakeholders who might require
multiple point of views of the same process. Moreover, the complexity of the
information systems running these business processes is generally high as they
are linked to databases storing all the relevant data and aspects of the
processes. Given the presence of multiple objects within an information system
which support the processes in their enactment, decisions are naturally
influenced by both these perspectives, logged in object-centric process logs.
However, the discovery of such decisions from object-centric process logs is
not straightforward as it requires to correctly link the involved objects
whilst considering the sequential constraints that business processes impose as
well as correctly discovering what a decision actually does. This paper
proposes the first object-centric decision-mining algorithm called Integrated
Object-centric Decision Discovery Algorithm (IODDA). IODDA is able to discover
how a decision is structured as well as how a decision is made. Moreover, IODDA
is able to discover which activities and object types are involved in the
decision-making process. Next, IODDA is demonstrated with the first artificial
knowledge-intensive process logs whose log generators are provided to the
research community.",http://arxiv.org/abs/2401.14847v1,arXiv
Information Systems,Decision Support Systems,Theoretical / Conceptual,Explanation of Probabilistic Inference for Decision Support Systems,"An automated explanation facility for Bayesian conditioning aimed at
improving user acceptance of probability-based decision support systems has
been developed. The domain-independent facility is based on an information
processing perspective on reasoning about conditional evidence that accounts
both for biased and normative inferences. Experimental results indicate that
the facility is both acceptable to naive users and effective in improving
understanding.",http://arxiv.org/abs/1304.2756v1,arXiv
Information Systems,Knowledge Management,Quantitative,Knowledge management in banking: A bibliometric literature review,"<jats:p>This bibliometric study examines publication trends, influential works, authorship networks, conceptual themes, and future directions in knowledge management research within the banking sector over the past three decades. Data were collected from a total of 443 scholarly publications written in English and indexed in Scopus. The data collection period spanned from 1994 to 2023. Quantitative bibliometric analysis techniques were employed, which involved the use of temporal visualization to examine publication and citation patterns, mapping co-authorship networks, and clustering high-frequency keywords. The results suggest a consistent rise in research interest and output as time progresses. Influential publications have proposed various models and frameworks for knowledge management, risk analysis, and organizational change. The majority of prolific authors were primarily of European descent, and it is worth noting that the Journal of Knowledge Management held the position as the most influential publication venue. The core research focus encompassed various areas such as knowledge for competitive advantage, intellectual capital measurement, knowledge-performance links, customer relationship management, and knowledge management technologies. The research has evolved to encompass digital transformation, sustainability, automation, and analytics. Proposed future directions include an examination of the role of knowledge management in ensuring continuity during crises, the facilitation of risk management through knowledge systems, and the development of decision support based on knowledge. This study offers valuable insights into the development of knowledge management research in the banking sector, although it is limited to English sources in Scopus.
Acknowledgment Expressing gratitude towards those who have contributed to the successful completion of a study has significant importance. In particular, it is crucial to extend appreciation to the individuals affiliated with the Ho Chi Minh University of Banking for their valuable support and assistance during the research endeavor.</jats:p>",https://doi.org/10.21511/kpm.08(1).2024.01,CrossRef
Information Systems,Knowledge Management,Quantitative,Knowledge‐enabled customer relationship management: integrating customer relationship management and knowledge management concepts[1],"<jats:p>The concepts of customer relationship management (CRM) and knowledge management (KM) both focus on allocating resources to supportive business activities in order to gain competitive advantages. CRM focuses on managing the relationship between a company and its current and prospective customer base as a key to success, while KM recognizes the knowledge available to a company as a major success factor. From a business process manager’s perspective both the CRM and KM approaches promise a positive impact on cost structures and revenue streams in return for the allocation of resources. However, investments in CRM and KM projects are not without risk, as demonstrated by many failed projects. In this paper we show that the benefit of using CRM and KM can be enhanced and the risk of failure reduced by integrating both approaches into a customer knowledge management (CKM) model. In this regard, managing relationships requires managing customer knowledge – knowledge about as well as from and for customers. In CKM, KM plays the role of a service provider, managing the four knowledge aspects: content, competence, collaboration and composition. Our findings are based on a literature analysis and six years of action research, supplemented by case studies and surveys.</jats:p>",https://doi.org/10.1108/13673270310505421,CrossRef
Information Systems,Knowledge Management,Quantitative,Integrating knowledge management with smart technologies in public pharmaceutical organizations,"<jats:p>This study investigates the impact of Knowledge Management (KM) practices, enhanced by smart technologies, on organizational performance within public pharmaceutical organizations in Cairo Governorate, Egypt. Using a descriptive-analytical approach, the study targeted employees from five public pharmaceutical companies in Cairo Governorate, including Memphis Pharmaceuticals, Arab Pharmaceuticals, Cairo Pharmaceuticals, Nile Pharmaceuticals, and EIPICO. These companies were selected based on their public listing and accessible workforce data. Respondents included administrative and technical staff, ensuring a representative sample of the sector. The sample size of 372 was calculated using a 95% confidence level and a 5% margin of error, proportionally distributed across organizations and roles. The results of the study reveal that KM practices significantly enhance operational efficiency and foster innovation, with quantitative evidence showing that KM positively influences operational efficiency (β = 0.42, p &amp;amp;lt; 0.01) and innovation (β = 0.35, p &amp;amp;lt; 0.05). The analysis also indicates that strategic leadership plays a moderating role in the relationship between KM practices and organizational performance. Specifically, the moderation effect of leadership strengthens the impact of KM on operational efficiency (interaction term: β = 0.18, p &amp;amp;lt; 0.05) and innovation (interaction term: β = 0.21, p &amp;amp;lt; 0.05). These findings underscore the critical role of leadership in aligning KM practices with strategic goals, highlighting the potential for public pharmaceutical organizations to achieve higher efficiency and innovation. Organizations operating in highly regulated sectors can drive continuous improvement and achieve sustainable performance outcomes by integrating KM frameworks with advanced technologies and strategic leadership.
Acknowledgment  The authors are thankful to the Deanship of Graduate Studies and Scientific Research at the University of Bisha for supporting this work through the Fast-Track Research Support Program.</jats:p>",https://doi.org/10.21511/kpm.09(1).2025.03,CrossRef
Information Systems,Knowledge Management,Quantitative,Relationship between transformational leadership and knowledge management: The moderating effect of organizational culture,"<jats:p>In the context of Peruvian public universities, knowledge management is a key tool for academic advancement and social progress. This study investigates how transformational leadership impacts knowledge management, with a special focus on the moderating effect of organizational culture. Data were collected from 370 managerial staff members, both teaching and administrative, across various public universities in Peru, through online surveys. The methodology employed was the analysis of regression models with interaction terms. The results demonstrate that transformational leadership has a significant positive effect on knowledge management (β = 0.7092; p &amp;lt; 0.01), especially highlighting the influence of charisma (β = 0.5315; p &amp;lt; 0.01). Organizational culture proved to be a significant moderator in this relationship. This was reflected in the significance of the interaction terms between the dimensions of organizational culture and transformational leadership. Participation has a moderating effect (β = 0.4507; p &amp;lt; 0.01), consistency is a significant moderator (β = 0.5356; p &amp;lt; 0.01), adaptability has a moderating influence in the relationship between leadership and knowledge management (β = 0.4890; p &amp;lt; 0.01), as does mission (β = 0.3846; p &amp;lt; 0.01). This suggests that in contexts where organizational culture is robust and focused on learning and collaboration, transformational leadership effectively enhances knowledge management. These results provide a deeper understanding of the role of transformational leadership and organizational culture in knowledge management practices in the academic field, offering valuable insights for future research and administrative practices in the educational sector.</jats:p>",https://doi.org/10.21511/kpm.07(1).2023.11,CrossRef
Information Systems,Knowledge Management,Quantitative,"Does knowledge management really matter? Linking knowledge management practices, competitiveness and economic performance","<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p><jats:italic>While nowadays an extensive literature promoting knowledge management (KM) exists, there is a worrying shortage of empirical studies demonstrating an actual connection between KM activities and organizational outcomes. To bridge this gap, this paper aims to examine the link between KM practices, firm competitiveness and economic performance.</jats:italic></jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p><jats:italic>This paper proposes a framework of KM practices consisting of human resource management (HRM) and information communication technology (ICT). These both are hypothesized to impact competitiveness and economic performance of the firm. Hypotheses are then tested with structural equation modeling by using a survey dataset of 234 companies.</jats:italic></jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p><jats:italic>The results show that HRM and ICT practices for managing knowledge are quite strongly correlated and have a statistically significant influence on both financial performance and competitiveness of the firm. The findings also indicate that ICT practices improve financial performance only when they are coupled with HRM practices.</jats:italic></jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Research limitations/implications</jats:title><jats:p><jats:italic>The data are limited to companies from Finland, Russia and China.</jats:italic></jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p><jats:italic>The paper contributes to managerial practice by pointing out the importance of utilizing a combination of both social and technical means for KM and illustrating that they do matter for the company bottom line.</jats:italic></jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p><jats:italic>This paper contributes to the literature on knowledge‐based organizing by empirically analyzing the performance impact of various areas of KM. It thereby tests the proposition put forth in many previous theoretical and case‐based studies that KM promotes high organizational performance. It also addresses the interaction of social and technical KM practices in producing organizational outcomes.</jats:italic></jats:p></jats:sec>",https://doi.org/10.1108/13673271211246185,CrossRef
Information Systems,Knowledge Management,Qualitative,An Introduction to Knowledge Management,"Knowledge has been lately recognized as one of the most important assets of
organizations. Managing knowledge has grown to be imperative for the success of
a company. This paper presents an overview of Knowledge Management and various
aspects of secure knowledge management. A case study of knowledge management
activities at Tata Steel is also discussed",http://arxiv.org/abs/0812.0438v1,arXiv
Information Systems,Knowledge Management,Qualitative,"Pitfalls in Effective Knowledge Management: Insights from an
  International Information Technology Organization","Knowledge is considered an essential resource for organizations. For
organizations to benefit from their possessed knowledge, knowledge needs to be
managed effectively. Despite knowledge sharing and management being viewed as
important by practitioners, organizations fail to benefit from their knowledge,
leading to issues in cooperation and the loss of valuable knowledge with
departing employees. This study aims to identify hindering factors that prevent
individuals from effectively sharing and managing knowledge and understand how
to eliminate these factors. Empirical data were collected through
semi-structured group interviews from 50 individuals working in an
international large IT organization. This study confirms the existence of a gap
between the perceived importance of knowledge management and how little this
importance is reflected in practice. Several hindering factors were identified,
grouped into personal social topics, organizational social topics, technical
topics, environmental topics, and interrelated social and technical topics. The
presented recommendations for mitigating these hindering factors are focused on
improving employees' actions, such as offering training and guidelines to
follow. The findings of this study have implications for organizations in
knowledge-intensive fields, as they can use this knowledge to create knowledge
sharing and management strategies to improve their overall performance.",http://arxiv.org/abs/2304.07737v3,arXiv
Information Systems,Knowledge Management,Qualitative,Knowledge Engineering Technique for Cluster Development,"After the concept of industry cluster was tangibly applied in many countries,
SMEs trended to link to each other to maintain their competitiveness in the
market. The major key success factors of the cluster are knowledge sharing and
collaboration between partners. This knowledge is collected in form of tacit
and explicit knowledge from experts and institutions within the cluster. The
objective of this study is about enhancing the industry cluster with knowledge
management by using knowledge engineering which is one of the most important
method for managing knowledge. This work analyzed three well known knowledge
engineering methods, i.e. MOKA, SPEDE and CommonKADS, and compares the
capability to be implemented in the cluster context. Then, we selected one
method and proposed the adapted methodology. At the end of this paper, we
validated and demonstrated the proposed methodology with some primary result by
using case study of handicraft cluster in Thailand.",http://arxiv.org/abs/0712.1994v1,arXiv
Information Systems,Knowledge Management,Qualitative,An Ontology-based Knowledge Management System for Industry Clusters,"Knowledge-based economy forces companies in the nation to group together as a
cluster in order to maintain their competitiveness in the world market. The
cluster development relies on two key success factors which are knowledge
sharing and collaboration between the actors in the cluster. Thus, our study
tries to propose knowledge management system to support knowledge management
activities within the cluster. To achieve the objectives of this study,
ontology takes a very important role in knowledge management process in various
ways; such as building reusable and faster knowledge-bases, better way for
representing the knowledge explicitly. However, creating and representing
ontology create difficulties to organization due to the ambiguity and
unstructured of source of knowledge. Therefore, the objectives of this paper
are to propose the methodology to create and represent ontology for the
organization development by using knowledge engineering approach. The
handicraft cluster in Thailand is used as a case study to illustrate our
proposed methodology.",http://arxiv.org/abs/0806.0526v1,arXiv
Information Systems,Knowledge Management,Qualitative,Knowledge Management in the Companion Cognitive Architecture,"One of the fundamental aspects of cognitive architectures is their ability to
encode and manipulate knowledge. Without a consistent, well-designed, and
scalable knowledge management scheme, an architecture will be unable to move
past toy problems and tackle the broader problems of cognition. In this paper,
we document some of the challenges we have faced in developing the knowledge
stack for the Companion cognitive architecture and discuss the tools,
representations, and practices we have developed to overcome them. We also lay
out a series of potential next steps that will allow Companion agents to play a
greater role in managing their own knowledge. It is our hope that these
observations will prove useful to other cognitive architecture developers
facing similar challenges.",http://arxiv.org/abs/2407.06401v1,arXiv
Information Systems,Knowledge Management,Mixed Methods,"Impact of knowledge management, knowledge sharing, and mental accounting on farmer performance in Sasi culture in Maluku Islands, Indonesia","<jats:p>The welfare of farmers and agricultural productivity are significantly influenced by challenges in business and financial management. This study investigates how knowledge management, knowledge sharing, and mental accounting impact farmer performance within the unique context of Sasi culture in Maluku Islands, Indonesia. Knowledge management provides farmers with the tools to acquire, utilize, and apply agricultural insights, while mental accounting shapes their financial decision-making and resource allocation. Using a mixed-method approach that combines WarpPLS and ethnomethodology, data were gathered through questionnaires distributed to 65 respondents and in-depth interviews with selected participants. The analysis revealed that knowledge management significantly impacts farmer performance with a path coefficient of 0.717 (p &amp;lt; 0.001), while mental accounting also has a positive effect with a coefficient of 0.164 (p = 0.050). However, knowledge sharing did not significantly affect performance (coefficient = 0.372, p = 0.382). The results suggest that Sasi culture, deeply rooted in local wisdom, helps integrate knowledge management and mental accounting to improve farmer welfare and agricultural income. Despite the ineffectiveness of formal knowledge sharing, the cultural practice of Sasi inherently promotes the sharing of knowledge within the community, enhancing the overall management of agricultural practices. This study emphasizes the role of local wisdom in creating sustainable agricultural practices and highlights the potential of Sasi culture to synergize modern knowledge management with traditional financial behaviors.</jats:p>",https://doi.org/10.21511/kpm.09(1).2025.05,CrossRef
Information Systems,Knowledge Management,Mixed Methods,"Knowledge management, adaptability and business process reengineering performance in microfinance institutions","<jats:p>The purpose of this paper is to provide theoretical explanation of business process reengineering performance using emerging themes of adaptability and knowledge management in the context of developing economies. The study used a narrative cross-sectional survey conducted using qualitative data collection technique, specifically the appreciative inquiry. The study used operations managers and senior executive managers to gather qualitative data from Uganda’s reengineered microfinance institutions to provide indepth explanation of business process reengineering performance. The authors find that adaptability, knowledge creation and knowledge sharing explain business process reengineering performance. The results suggest that business process reengineering be made mandatory to ensure sustainable competitiveness of the financial sector. The study provides novel insights of business process reengineering performance using a theory of change and a complexity theory. Methodological, theoretical, managerial and policy implications herein play pivotal role in bridging the knowledge gap that exists in Microfinance institutions of developing economies.</jats:p>",https://doi.org/10.21511/kpm.02(1).2018.06,CrossRef
Information Systems,Knowledge Management,Mixed Methods,Knowledge Management Implementation Strategy for Knowledge Management Systems in Two Mobile Telecommunication Companies Namibia,"<jats:p>Rationale of Study – This article presents the findings of a study on the knowledge management (KM) implementation strategy for two mobile telecommunications (MT) companies in Namibia.Methodology – The case study used a mixed-methods approach via convergent parallel design. This permitted the concurrent gathering of quantitative and qualitative data for the study. The study used simple random sampling via probability sampling to identify 309 respondents. An online survey distributed 329 questionnaires, and 200 were received with a 60.79% response rate. A purposive sampling technique was employed for the qualitative phase; 11 participants were interviewed out of the planned 20.Findings – The study found that neither KM implementation strategies nor a department or section dedicated to organisational KM exist, necessitating a KM implementation strategy for KMS for effective KM practices in two MT companies in Namibia. The study also identified potential barriers to KMS, such as the complexity of employee attitudes, the dearth of use of specific KMS, and the organisational KM corporate work culture.Implications – This study's findings could expand academicians', KM researchers', and organisations' understanding of the importance of organisational KM implementation strategy for KMS to be effective and efficient in MT companies in Namibia.Originality – This study is the first on KM implementation strategies for KMS to influence knowledge management practices in Namibia.</jats:p>",https://doi.org/10.70759/zm1kk411,CrossRef
Information Systems,Knowledge Management,Mixed Methods,The role of knowledge management in institutional strategy development and competitiveness at leading African universities,"<jats:p>The role of knowledge management as a strategic intervention in higher education in developing economies has not been studied extensively. Higher education plays a central role in a country’s economy through knowledge creation and dissemination to its stakeholders. The main purpose of this article was to examine the role and influence of knowledge management in decision-making and strategy formulation at leading universities in Africa and to establish if knowledge management was adding value and competitiveness to the institutions. A survey across 20 leading African universities was conducted in 2014. A mixed method of quantitative and qualitative approaches was adopted. The results show that knowledge management does have the potential to positively influence institutional strategy formulation, but should ideally be represented at executive level for its potential to be fully realized. More knowledge management practice is needed in the areas of academic teaching and learning, and research. There was a lack of sophisticated and powerful knowledge management Information Systems in most of Africa’s leading institutions. Those institutions that utilized KM more strategically, inclusive of specialized KM Information Systems were the higher ranked institutions. This suggests that knowledge management could play a crucial role in a University’s success and competitiveness.</jats:p>",https://doi.org/10.21511/kpm.03(1).2019.03,CrossRef
Information Systems,Knowledge Management,Mixed Methods,Strategic analysis of knowledge firms: the links between knowledge management and leadership,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p><jats:italic>The purpose of this paper is to explore and explain the links between knowledge management (KM) and leadership in knowledge‐intensive firms.</jats:italic></jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p><jats:italic>This study employs an instrumental case‐based study on four knowledge‐based firms to explore KM and leadership approaches, and the links between them. Data were primarily collected through qualitative interviews with firm managers and direct observations, as well as quantitative data by questionnaire from the firm employees.</jats:italic></jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p><jats:italic>The study identified two combinations of KM and leadership systems. These combinations are personalization‐distribution and codification‐centralization; which are explained within the theoretical framework of this paper. Other theoretically possible combinations were discussed and argued to be non‐viable or non‐economical.</jats:italic></jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Research limitations/implications</jats:title><jats:p><jats:italic>As with most qualitative case‐based research papers, this research was focused on study of a small number of cases; a limitation that does not allow the authors to claim a statistical generalization but nevertheless allows analytical generalization to be made. Limitations of this paper include the fact that all cases were located in one country and all were more or less involved with the field of information technology.</jats:italic></jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p><jats:italic>Practical implications of this paper for managers and company strategists involve alignment of their KM strategy with a relevant leadership system.</jats:italic></jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p><jats:italic>There has been little research aimed at finding links between KM and leadership in firms, and how this link may lead to increased knowledge exploitation capability for the firm. The present study addresses this issue and presents an evidenced and theoretically supported explanation for this link.</jats:italic></jats:p></jats:sec>",https://doi.org/10.1108/13673271311300697,CrossRef
Information Systems,Knowledge Management,Design and Development,Knowledge Management,"This paper discusses the important process of knowledge and its management,
and differences between tacit and explicit knowledge and understanding the
culture as a key issue for the successful implementation of knowledge
management, in addition to, this paper is concerned with the four-stage model
for the evolution of information technology (IT) support for knowledge
management in law firms.",http://arxiv.org/abs/1003.1807v1,arXiv
Information Systems,Knowledge Management,Design and Development,Review of Knowledge Management Systems As Socio-Technical System,"Knowledge Management Systems as socio-technical systemperspectives has
recognized for decades. Practitioners and scholars belief Knowledge Management
is best carried out throught the optimization both technological and
social-aspect.Lacking of understand and consider both aspects could lead
organizations in misinterpretation while developing andimplementing Knowledge
Management System. There is a need for practical guidance how Knowledge
Management System should implement in organizations. We propose a framework
that could use by practitioner and manager as guidance in developing and
implementing Knowledge Management System as Socio-Technical Systems. The
framework developed base on Pan and Scarborough view of Knowledge Management as
Socio-Technical system. Our framework consists of: Infrastructure(technology),
Info structure (organizational structure) and Info culture (organizational
culture). This concept would lead practitioners get clear understand aspect
contribute to Knowledge Management System success as Socio-Technical System.",http://arxiv.org/abs/1212.0387v1,arXiv
Information Systems,Knowledge Management,Design and Development,On the Convergence of Collaboration and Knowledge Management,"Collaboration technology typically focuses on collaboration and group
processes (cooperation, communication, coordination and coproduction).
Knowledge Management (KM) technology typically focuses on content (creation,
storage, sharing and use of data, information and knowledge). Yet, to achieve
their common goals, teams and organizations need both KM and collaboration
technology to make that more effective and efficient. This paper is interested
in knowledge management and collaboration regarding their convergence and their
integration. First, it contributes to a better understanding of the knowledge
management and collaboration concepts. Second, it focuses on KM and
collaboration convergence by presenting the different interpretation of this
convergence. Third, this paper proposes a generic framework of collaborative
knowledge management.",http://arxiv.org/abs/1202.6104v1,arXiv
Information Systems,Knowledge Management,Design and Development,"Penerapan Knowledge Management System Sales and Customer Care pada PT
  Satria Medikantara Palembang","PT Satria Medikantara Palembang has a great desire to apply knowledge
management system, therefore the documentation of knowledge and its utilization
needs to be well managed in the context of performance improvement. The
implementation of knowledge management in PT Satria Medikantara Palembang is
considered very good and can have a positive impact for the quality of
employees. Where every employee can store and document and share knowledge
owned, so that other employees can access, even learn and discuss with other
employees based on the knowledge posted. Then when needed a knowledge, it is
very easy to find in the database searching feature in the web based knowledge
management system at PT Satria Medikantara Palembang. The methodology used in
this study refers to the methodology of knowledge management system life cycle
developed by Awad and Ghaziri (2010), and this system will be based on web
using PHP programming language.",http://arxiv.org/abs/1911.01277v1,arXiv
Information Systems,Knowledge Management,Design and Development,"Dynamic Capitalization and Visualization Strategy in Collaborative
  Knowledge Management System for EI Process","Knowledge is attributed to human whose problem-solving behavior is subjective
and complex. In today's knowledge economy, the need to manage knowledge
produced by a community of actors cannot be overemphasized. This is due to the
fact that actors possess some level of tacit knowledge which is generally
difficult to articulate. Problem-solving requires searching and sharing of
knowledge among a group of actors in a particular context. Knowledge expressed
within the context of a problem resolution must be capitalized for future
reuse. In this paper, an approach that permits dynamic capitalization of
relevant and reliable actors' knowledge in solving decision problem following
Economic Intelligence process is proposed. Knowledge annotation method and
temporal attributes are used for handling the complexity in the communication
among actors and in contextualizing expressed knowledge. A prototype is built
to demonstrate the functionalities of a collaborative Knowledge Management
system based on this approach. It is tested with sample cases and the result
showed that dynamic capitalization leads to knowledge validation hence
increasing reliability of captured knowledge for reuse. The system can be
adapted to various domains",http://arxiv.org/abs/1012.3312v1,arXiv
Information Systems,Knowledge Management,Theoretical / Conceptual,Choosing a Knowledge Dissemination Approach,"Knowledge management has been described as getting the right knowledge to the
right people in the right place at the right time. Knowledge dissemination is a
crucial part of knowledge management because it ensures knowledge is available
to those who need it.
  This paper reviews four well-known knowledge dissemination techniques. Each
technique is classified according to a recently proposed classification scheme,
and advice is given regarding when it is appropriate to use each technique.",http://arxiv.org/abs/1809.05761v1,arXiv
Information Systems,Knowledge Management,Theoretical / Conceptual,"Knowledge Management Competence and ISD Vendor Innovativeness in
  Turbulent Markets","Continuous changes in the technology and the business landscape place high
strain on managing knowledge in organisations. Prior researchers highlight a
positive connotation with knowledge management competence and organisational
innovativeness in a turbulent environment. However, the rapid changes in the
market and technology landscape may exert an additional pressure on the
employees and such pressures may ultimately hinder organisational
innovativeness. Drawing on knowledge management and innovation literature, this
research conceptualises a model that investigates this tenacious relationship
between knowledge management competence and innovativeness specifically in
turbulent dynamic markets, considering information systems development
(ISD)-outsourcing as the context. Following a mixed method approach, this
research expects to provide guidance for ISD-outsourcing vendors to manage
innovation expectations, knowledge management process and performance of the
employees in dynamic market conditions.",http://arxiv.org/abs/2011.09840v1,arXiv
Information Systems,Knowledge Management,Theoretical / Conceptual,"Evident: a Development Methodology and a Knowledge Base Topology for
  Data Mining, Machine Learning and General Knowledge Management","Software has been developed for knowledge discovery, prediction and
management for over 30 years. However, there are still unresolved pain points
when using existing project development and artifact management methodologies.
Historically, there has been a lack of applicable methodologies. Further,
methodologies that have been applied, such as Agile, have several limitations
including scientific unfalsifiability that reduce their applicability. Evident,
a development methodology rooted in the philosophy of logical reasoning and
EKB, a knowledge base topology, are proposed. Many pain points in data mining,
machine learning and general knowledge management are alleviated conceptually.
Evident can be extended potentially to accelerate philosophical exploration,
science discovery, education as well as knowledge sharing & retention across
the globe. EKB offers one solution of storing information as knowledge, a
granular level above data. Related topics in computer history, software
engineering, database, sensor, philosophy, and project & organization &
military managements are also discussed.",http://arxiv.org/abs/2211.10291v1,arXiv
Information Systems,Knowledge Management,Theoretical / Conceptual,"OntoMath Digital Ecosystem: Ontologies, Mathematical Knowledge Analytics
  and Management","In this article we consider the basic ideas, approaches and results of
developing of mathematical knowledge management technologies based on
ontologies. These solutions form the basis of a specialized digital ecosystem
OntoMath which consists of the ontology of the logical structure of
mathematical documents Mocassin and ontology of mathematical knowledge
OntoMathPRO, tools of text analysis, recommender system and other applications
to manage mathematical knowledge. The studies are in according to the ideas of
creating a distributed system of interconnected repositories of digitized
versions of mathematical documents and project to create a World Digital
Mathematical Library.",http://arxiv.org/abs/1702.05112v1,arXiv
Information Systems,Knowledge Management,Theoretical / Conceptual,"Imperfect Knowledge Management -- A Case Study in a Chilean
  Manufacturing Company","To conceptualize living systems based on the processes that create them,
rather than their interactions with the environment, as in systems theory.
Maturana and Varela (1969) at the University of Chile introduced the term
autopoiesis (from Greek self and production). This concept emphasizes autonomy
as the defining feature of living systems. It describes them as self-sustaining
entities that preserve their identity through continuous self-renewal to
preserve their unity. Furthermore, these systems can only be understood in
reference to themselves, as all internal activities are inherently
self-determined by self-production and self-referentiality. This thesis
introduces the Fuzzy Autopoietic Knowledge Management (FAKM) model, which
integrates the system theory of living systems, the cybernetic theory of viable
systems, and the autopoiesis theory of autopoietic systems. The goal is to move
beyond traditional knowledge management models that rely on Cartesian dualism
(cognition/action) where knowledge is treated as symbolic information
processing. Instead, the FAKM model adopts a dualism of organization/structure
to define an autopoietic system within a sociotechnical approach. The model is
experimentally applied to a manufacturing company in the Maule Region, south of
Santiago, Chile.",http://arxiv.org/abs/2502.01656v1,arXiv
Information Systems,Information Security,Quantitative,Information Security Games: A Survey,"We introduce some preliminaries about game theory and information security.
Then surveying a subset of the literature, we identify opportunities for future
research.",http://arxiv.org/abs/2103.12520v1,arXiv
Information Systems,Information Security,Quantitative,"Quantitative Information Flow Control by Construction for
  Component-Based Systems","Secure software architecture is increasingly important in a data-driven
world. When security is neglected sensitive information might leak through
unauthorized access. To mitigate this software architects needs tools and
methods to quantify security risks in complex systems. This paper presents
doctoral research in its early stages concerned with creating constructive
methods for building secure component-based systems from a quantitative
information flow specification. This research aim at developing a method that
allows software architects to develop secure systems from a repository of
secure components. Planned contributions are refinement rules for secure
development of components from a specification and well-formedness rules for
secure composition of said components.",http://arxiv.org/abs/2401.07677v1,arXiv
Information Systems,Information Security,Quantitative,On Converse Results for Secure Index Coding,"In this work, we study the secure index coding problem where there are
security constraints on both legitimate receivers and eavesdroppers. We develop
two performance bounds (i.e., converse results) on the symmetric secure
capacity. The first one is an extended version of the basic acyclic chain bound
(Liu and Sadeghi, 2019) that takes security constraints into account. The
second converse result is a novel information-theoretic lower bound on the
symmetric secure capacity, which is interesting as all the existing converse
results in the literature for secure index coding give upper bounds on the
capacity.",http://arxiv.org/abs/2105.07211v1,arXiv
Information Systems,Information Security,Quantitative,"The need for effective information security awareness practices in Oman
  higher educational institutions","The revolution of internet technology and its usage have led a significant
increase in the number of online transactions and electronic data transfer,
parallely increased the number of cybercrime incidents around the world. Steady
economic growth in the Sultanate of Oman accelerated the volume of online
utilization for e-commerce, banking, communication, education and so forth.
Normally attackers target the users who ignore security practices due to the
lack of information security awareness. Unawareness of information security
practices, user negligence, lack of awareness programs and trainings are the
root cause for information security threats. Earlier studies reveal there is a
considerable and continuous cybercrime incident in Oman which compromises the
security policy of the organizations, affecting the business continuity and the
economic growth. In this study, a survey was performed among the educational
institutions in Oman to investigate the level of information security awareness
and based on the study, a security awareness model is proposed to enable
information security practices in the educational institutions.",http://arxiv.org/abs/1602.06510v1,arXiv
Information Systems,Information Security,Quantitative,"Individual and Contextual Variables of Cyber Security Behaviour -- An
  empirical analysis of national culture, industry, organisation, and
  individual variables of (in)secure human behaviour","Cyber security incidents are increasing and humans play an important role in
reducing their likelihood and impact. We identify a skewed focus towards
technical aspects of cyber security in the literature, whereas factors
influencing the secure behaviour of individuals require additional research.
These factors span across both the individual level and the contextual level in
which the people are situated. We analyse two datasets of a total of 37,075
records from a) self-reported security behaviours across the EU, and b)
observed phishing-related behaviours from the industry security awareness
training programmes. We identify that national culture, industry type, and
organisational security culture play are influential Variables (antecedents) of
individuals' security behaviour at contextual level. Whereas, demographics
(age, gender, and level or urbanisation) and security-specific factors
(security awareness, security knowledge, and prior experience with security
incidents) are found to be influential variables of security behaviour at
individual level. Our findings have implications for both research and practice
as they fill a gap in the literature and provide concrete statistical evidence
on the variables which influence security behaviour. Moreover, findings
provides practical insights for organisations regarding the susceptibility of
groups of people to insecure behaviour. Consequently, organisations can tailor
their security training and awareness efforts (e.g., through behaviour change
interventions and/or appropriate employee group profiles), adapt their
communications (e.g., of information security policies), and customise their
interventions according to national culture characteristics to improve security
behaviour.",http://arxiv.org/abs/2405.16215v2,arXiv
Information Systems,Information Security,Qualitative,"Secure Synthesis of Distributed Cryptographic Applications (Technical
  Report)","Developing secure distributed systems is difficult, and even harder when
advanced cryptography must be used to achieve security goals. Following prior
work, we advocate using secure program partitioning to synthesize cryptographic
applications: instead of implementing a system of communicating processes, the
programmer implements a centralized, sequential program, which is automatically
compiled into a secure distributed version that uses cryptography.
  While this approach is promising, formal results for the security of such
compilers are limited in scope. In particular, no security proof yet
simultaneously addresses subtleties essential for robust, efficient
applications: multiple cryptographic mechanisms, malicious corruption, and
asynchronous communication.
  In this work, we develop a compiler security proof that handles these
subtleties. Our proof relies on a novel unification of simulation-based
security, information-flow control, choreographic programming, and
sequentialization techniques for concurrent programs. While our proof targets
hybrid protocols, which abstract cryptographic mechanisms as idealized
functionalities, our approach offers a clear path toward leveraging Universal
Composability to obtain end-to-end, modular security results with fully
instantiated cryptographic mechanisms.
  Finally, following prior observations about simulation-based security, we
prove that our result guarantees robust hyperproperty preservation, an
important criterion for compiler correctness that preserves all source-level
security properties in target programs.",http://arxiv.org/abs/2401.04131v1,arXiv
Information Systems,Information Security,Qualitative,"Performance Measurement of Security Academic Information System using
  Maturity Level","This study aims to information security in academic information systems to
provide recommendations for improvements in information security management by
the expected maturity level based on ISO/IEC 27002:2013. By using a qualitative
descriptive approach, data collection and validation techniques with
triangulation techniques are interviews, observation, and documentation. The
data were analyzed by using gap analysis and to measure the maturity level
determined 15 objective control and 45 security controls scattered in 5
clauses, the result of the research found that the performance of academic
information system maturity level at level 2. That is, the current level of
maturity is below the expected maturity level, so it needs to be increased to
the expected level.",http://arxiv.org/abs/2204.09511v1,arXiv
Information Systems,Information Security,Qualitative,Security policies for distributed systems,"A security policy specifies a security property as the maximal information
flow. A distributed system composed of interacting processes implicitly defines
an intransitive security policy by repudiating direct information flow between
processes that do not exchange messages directly. We show that implicitly
defined security policies in distributed systems are enforced, provided that
processes run in separation, and possible process communication on a technical
platform is restricted to specified message paths of the system. Furthermore,
we propose to further restrict the allowable information flow by adding filter
functions for controlling which messages may be transmitted between processes,
and we prove that locally checking filter functions is sufficient for ensuring
global security policies. Altogether, global intransitive security policies are
established by means of local verification conditions for the (trusted)
processes of the distributed system. Moreover, security policies may be
implemented securely on distributed integration platforms which ensure
partitioning. We illustrate our results with a smart grid case study, where we
use CTL model checking for discharging local verification conditions for each
process under consideration.",http://arxiv.org/abs/1310.3723v1,arXiv
Information Systems,Information Security,Qualitative,"Getting Users Smart Quick about Security: Results from 90 Minutes of
  Using a Persuasive Toolkit for Facilitating Information Security Problem
  Solving by Non-Professionals","There is a conflict between the need for security compliance by users and the
fact that commonly they cannot afford to dedicate much of their time and energy
to that security. A balanced level of user engagement in security is difficult
to achieve due to difference of priorities between the business perspective and
the security perspective. We sought to find a way to engage users minimally,
yet efficiently, so that they would both improve their security awareness and
provide necessary feedback for improvement purposes to security designers. We
have developed a persuasive software toolkit to engage users in structured
discussions about security vulnerabilities in their company and potential
interventions addressing these. In the toolkit we have adapted and integrated
an established framework from conventional crime prevention. In the research
reported here we examine how non-professionals perceived security problems
through a short-term use of the toolkit. We present perceptions from a pilot
lab study in which randomly recruited participants had to analyze a crafted
insider threat problem using the toolkit. Results demonstrate that study
participants were able to successfully identify causes, propose interventions
and engage in providing feedback on proposed interventions. Subsequent
interviews show that participants have developed greater awareness of
information security issues and the framework to address these, which in a real
setting would lead ultimately to significant benefits for the organization.
These results indicate that when well-structured such short-term engagement is
sufficient for users to meaningfully take part in complex security discussions
and develop in-depth understanding of theoretical principles of security.",http://arxiv.org/abs/2209.02420v1,arXiv
Information Systems,Information Security,Qualitative,"Efficient Three-party Computation: An Information-theoretic Approach
  from Cut-and-Choose","As far as we know, the literature on secure computation from cut-and-choose
has focused on achieving computational security against malicious adversaries.
It is unclear whether the idea of cut-and-choose can be adapted to secure
computation with information-theoretic security. In this work we explore the
possibility of using cut-and-choose in information theoretic setting for secure
three-party computation (3PC). Previous work on 3PC has mainly focus on the
semi-honest case, and is motivated by the observation that real-word
deployments of multi-party computation (MPC) seem to involve few parties. We
propose a new protocol for information-theoretically secure 3PC tolerating one
malicious party with cheating probability $2^{-s}$ using $s$ runs of circuit
computation in the cut-and-choose paradigm. The computational cost of our
protocol is essentially only a small constant worse than that of
state-of-the-art 3PC protocols against a semi-honest corruption, while its
communication round is greatly reduced compared to other maliciously secure 3PC
protocols in information-theoretic setting.",http://arxiv.org/abs/1908.03718v1,arXiv
Information Systems,Information Security,Mixed Methods,"Chaotic iterations versus Spread-spectrum: topological-security and
  stego-security","A new framework for information hiding security, called topological-security,
has been proposed in a previous study. It is based on the evaluation of
unpredictability of the scheme, whereas existing notions of security, as
stego-security, are more linked to information leaks. It has been proven that
spread-spectrum techniques, a well-known stego-secure scheme, are
topologically-secure too. In this paper, the links between the two notions of
security is deepened and the usability of topological-security is clarified, by
presenting a novel data hiding scheme that is twice stego and
topological-secure. This last scheme has better scores than spread-spectrum
when evaluating qualitative and quantitative topological-security properties.
Incidentally, this result shows that the new framework for security tends to
improve the ability to compare data hiding scheme.",http://arxiv.org/abs/1112.3874v1,arXiv
Information Systems,Information Security,Mixed Methods,Composite Metrics for Network Security Analysis,"Security metrics present the security level of a system or a network in both
qualitative and quantitative ways. In general, security metrics are used to
assess the security level of a system and to achieve security goals. There are
a lot of security metrics for security analysis, but there is no systematic
classification of security metrics that are based on network reachability
information. To address this, we propose a systematic classification of
existing security metrics based on network reachability information. Mainly, we
classify the security metrics into host-based and network-based metrics. The
host-based metrics are classified into metrics ``without probability"" and ""with
probability"", while the network-based metrics are classified into ""path-based""
and ""non-path based"". Finally, we present and describe an approach to develop
composite security metrics and it's calculations using a Hierarchical Attack
Representation Model (HARM) via an example network. Our novel classification of
security metrics provides a new methodology to assess the security of a system.",http://arxiv.org/abs/2007.03486v2,arXiv
Information Systems,Information Security,Mixed Methods,Chaotic iterations versus Spread-spectrum: chaos and stego security,"A new framework for information hiding security, called chaos-security, has
been proposed in a previous study. It is based on the evaluation of
unpredictability of the scheme, whereas existing notions of security, as
stego-security, are more linked to information leaks. It has been proven that
spread-spectrum techniques, a well-known stego-secure scheme, are chaos-secure
too. In this paper, the links between the two notions of security is deepened
and the usability of chaos-security is clarified, by presenting a novel data
hiding scheme that is twice stego and chaos-secure. This last scheme has better
scores than spread-spectrum when evaluating qualitative and quantitative
chaos-security properties. Incidentally, this result shows that the new
framework for security tends to improve the ability to compare data hiding
scheme.",http://arxiv.org/abs/1005.0705v3,arXiv
Information Systems,Information Security,Mixed Methods,SMEs' Confidentiality Concerns for Security Information Sharing,"Small and medium-sized enterprises are considered an essential part of the EU
economy, however, highly vulnerable to cyberattacks. SMEs have specific
characteristics which separate them from large companies and influence their
adoption of good cybersecurity practices. To mitigate the SMEs' cybersecurity
adoption issues and raise their awareness of cyber threats, we have designed a
self-paced security assessment and capability improvement method, CYSEC. CYSEC
is a security awareness and training method that utilises self-reporting
questionnaires to collect companies' information about cybersecurity awareness,
practices, and vulnerabilities to generate automated recommendations for
counselling. However, confidentiality concerns about cybersecurity information
have an impact on companies' willingness to share their information. Security
information sharing decreases the risk of incidents and increases users'
self-efficacy in security awareness programs. This paper presents the results
of semi-structured interviews with seven chief information security officers of
SMEs to evaluate the impact of online consent communication on motivation for
information sharing. The results were analysed in respect of the Self
Determination Theory. The findings demonstrate that online consent with
multiple options for indicating a suitable level of agreement improved
motivation for information sharing. This allows many SMEs to participate in
security information sharing activities and supports security experts to have a
better overview of common vulnerabilities. The final publication is available
at Springer via https://doi.org/10.1007/978-3-030-57404-8_22",http://arxiv.org/abs/2007.06308v2,arXiv
Information Systems,Information Security,Mixed Methods,strideSEA: A STRIDE-centric Security Evaluation Approach,"Microsoft's STRIDE methodology is at the forefront of threat modeling,
supporting the increasingly critical quality attribute of security in
software-intensive systems. However, in a comprehensive security evaluation
process, the general consensus is that the STRIDE classification is only useful
for threat elicitation, isolating threat modeling from the other security
evaluation activities involved in a secure software development life cycle
(SDLC). We present strideSEA, a STRIDE-centric Security Evaluation Approach
that integrates STRIDE as the central classification scheme into the security
activities of threat modeling, attack scenario analysis, risk analysis, and
countermeasure recommendation that are conducted alongside software engineering
activities in secure SDLCs. The application of strideSEA is demonstrated in a
real-world online immunization system case study. Using STRIDE as a single
unifying thread, we bind existing security evaluation approaches in the four
security activities of strideSEA to analyze (1) threats using Microsoft threat
modeling tool, (2) attack scenarios using attack trees, (3) systemic risk using
NASA's defect detection and prevention (DDP) technique, and (4) recommend
countermeasures based on their effectiveness in reducing the most critical
risks using DDP. The results include a detailed quantitative assessment of the
security of the online immunization system with a clear definition of the role
and advantages of integrating STRIDE in the evaluation process. Overall, the
unified approach in strideSEA enables a more structured security evaluation
process, allowing easier identification and recommendation of countermeasures,
thus supporting the security requirements and eliciting design considerations,
informing the software development life cycle of future software-based
information systems.",http://arxiv.org/abs/2503.19030v1,arXiv
Information Systems,Information Security,Design and Development,"A Business Goal Driven Approach for Understanding and Specifying
  Information Security Requirements","In this paper we present an approach for specifying and prioritizing
information security requirements in organizations. It is important to
prioritize security requirements since hundred per cent security is not
achievable and the limited resources available should be directed to satisfy
the most important ones. We propose to link explicitly security requirements
with the organization's business vision, i.e. to provide business rationale for
security requirements. The rationale is then used as a basis for comparing the
importance of different security requirements. A conceptual framework is
presented, where the relationships between business vision, critical impact
factors and valuable assets (together with their security requirements) are
shown.",http://arxiv.org/abs/cs/0603129v1,arXiv
Information Systems,Information Security,Design and Development,Secure physical layer network coding versus secure network coding,"Secure network coding realizes the secrecy of the message when the message is
transmitted via noiseless network and a part of edges or a part of intermediate
nodes are eavesdropped. In this framework, if the channels of the network has
noise, we apply the error correction to noisy channel before applying the
secure network coding. In contrast, secure physical layer network coding is a
method to securely transmit a message by a combination of coding operation on
nodes when the network is given as a set of noisy channels. In this paper, we
give several examples of network, in which, secure physical layer network
coding has advantage over secure network coding.",http://arxiv.org/abs/1812.00117v1,arXiv
Information Systems,Information Security,Design and Development,A New Fuzzy MCDM Framework to Evaluate E-Government Security Strategy,"Ensuring security of e-government applications and infrastructures is crucial
to maintain trust among stakeholders to store, process and exchange information
over the e-government systems. Due to dynamic and continuous threats on
e-government information security, policy makers need to perform evaluation on
existing information security strategy as to deliver trusted e-government
services. This paper presents an information security evaluation framework
based on new fuzzy multi criteria decision making (MCDM) to help policy makers
conduct comprehensive assessment of e-government security strategy.",http://arxiv.org/abs/1011.3101v1,arXiv
Information Systems,Information Security,Design and Development,A Cross-Layer Security Analysis for Process-Aware Information Systems,"Information security in Process-aware Information System (PAIS) relies on
many factors, including security of business process and the underlying system
and technologies. Moreover, humans can be the weakest link that creates pathway
to vulnerabilities, or the worst enemy that compromises a well-defended system.
Since a system is as secure as its weakest link, information security can only
be achieved in PAIS if all factors are secure. In this paper, we address two
research questions: how to conduct a cross-layer security analysis that couple
security concerns at business process layer as well as at the technical layer;
and how to include human factor into the security analysis for the
identification of human-oriented vulnerabilities and threats. We propose a
methodology that supports the tracking of security interdependencies between
functional, technical, and human aspects which contribute to establish a
holistic approach to information security in PAIS. We demonstrate the
applicability with a scenario from the payment card industry.",http://arxiv.org/abs/1507.03415v1,arXiv
Information Systems,Information Security,Design and Development,Secure Web-Based Student Information Management System,"The reliability and success of any organization such as academic institution
rely on its ability to provide secure, accurate and timely data about its
operations. Erstwhile managing student information in academic institution was
done through paper-based information system, where academic records are
documented in several files that are kept in shelves. Several problems are
associated with paper-based information system. Managing information through
the manual approach require physical exertion to retrieve, alter, and re-file
the paper records. These are nonvalue added services results in data
inconsistency and redundancy, currently institutions have migrated to web-based
student information management system without considering the security
architecture of the web portal. This project seeks to ameliorates and secure
how information is being managed in Nigeria Police Academy through the
development of a secured web-based student information management system, which
has a friendly user interface that provides an easy and secure way to manage
academic information such as students information, staff information, course
registration, course materials and results. This project was developed using
Laravel 5.5 PHP Framework to provide a robust secure web-based student
information system that is not vulnerable to 2018 OWASP TOP 10 web
vulnerabilities.",http://arxiv.org/abs/2211.00072v1,arXiv
Information Systems,Information Security,Theoretical / Conceptual,"On the Equivalence of Two Security Notions for Hierarchical Key
  Assignment Schemes in the Unconditional Setting","The access control problem in a hierarchy can be solved by using a
hierarchical key assignment scheme, where each class is assigned an encryption
key and some private information. A formal security analysis for hierarchical
key assignment schemes has been traditionally considered in two different
settings, i.e., the unconditionally secure and the computationally secure
setting, and with respect to two different notions: security against key
recovery (KR-security) and security with respect to key indistinguishability
(KI-security), with the latter notion being cryptographically stronger.
Recently, Freire, Paterson and Poettering proposed strong key
indistinguishability (SKI-security) as a new security notion in the
computationally secure setting, arguing that SKI-security is strictly stronger
than KI-security in such a setting. In this paper we consider the
unconditionally secure setting for hierarchical key assignment schemes. In such
a setting the security of the schemes is not based on specific unproven
computational assumptions, i.e., it relies on the theoretical impossibility of
breaking them, despite the computational power of an adversary coalition. We
prove that, in this setting, SKI-security is not stronger than KI-security,
i.e., the two notions are fully equivalent from an information-theoretic point
of view.",http://arxiv.org/abs/1402.5371v2,arXiv
Information Systems,Information Security,Theoretical / Conceptual,"Guardians of the Web: The Evolution and Future of Website Information
  Security","Website information security has become a critical concern in the digital
age. This article explores the evolution of website information security,
examining its historical development, current practices, and future directions.
The early beginnings from the 1960s to the 1980s laid the groundwork for modern
cybersecurity, with the development of ARPANET, TCP/IP, public-key
cryptography, and the first antivirus programs. The 1990s marked a
transformative era, driven by the commercialization of the Internet and the
emergence of web-based services. As the Internet grew, so did the range and
sophistication of cyber threats, leading to advancements in security
technologies such as the Secure Sockets Layer (SSL) protocol, password
protection, and firewalls. Current practices in website information security
involve a multi-layered approach, including encryption, secure coding
practices, regular security audits, and user education. The future of website
information security is expected to be shaped by emerging technologies such as
artificial intelligence, blockchain, and quantum computing, as well as the
increasing importance of international cooperation and standardization efforts.
As cyber threats continue to evolve, ongoing research and innovation in website
information security will be essential to protect sensitive information and
maintain trust in the digital world.",http://arxiv.org/abs/2505.04308v1,arXiv
Information Systems,Information Security,Theoretical / Conceptual,"On the Impact of Perceived Vulnerability in the Adoption of Information
  Systems Security Innovations","A number of determinants predict the adoption of Information Systems (IS)
security innovations. Amongst, perceived vulnerability of IS security threats
has been examined in a number of past explorations. In this research, we
examined the processes pursued in analysing the relationship between perceived
vulnerability of IS security threats and the adoption of IS security
innovations. The study uses Systematic Literature Review (SLR) method to
evaluate the practice involved in examining perceived vulnerability on IS
security innovation adoption. The SLR findings revealed the appropriateness of
the existing empirical investigations of the relationship between perceived
vulnerability of IS security threats on IS security innovation adoption.
Furthermore, the SLR results confirmed that individuals who perceives
vulnerable to an IS security threat are more likely to engage in the adoption
an IS security innovation. In addition, the study validates the past studies on
the relationship between perceived vulnerability and IS security innovation
adoption.",http://arxiv.org/abs/1904.08229v1,arXiv
Information Systems,Information Security,Theoretical / Conceptual,"Blockchain-based Application Security Risks: A Systematic Literature
  Review","Although the blockchain-based applications are considered to be less
vulnerable due to the nature of the distributed ledger, they did not become the
silver bullet with respect to securing the information against different
security risks. In this paper, we present a literature review on the security
risks that can be mitigated by introducing the blockchain technology, and on
the security risks that are identified in the blockchain-based applications. In
addition, we highlight the application and technology domains where these
security risks are observed. The results of this study could be seen as a
preliminary checklist of security risks when implementing blockchain-based
applications.",http://arxiv.org/abs/1912.09556v1,arXiv
Information Systems,Information Security,Theoretical / Conceptual,"Factors of people-centric security climate: conceptual model and
  exploratory study in Vietnam","There is an increasing focus on the persuasive approach to develop a
people-centric security climate where employees are aware of the priority of
security and perform conscious security behaviour proactively. Employees can
evaluate the priority of security as they observe and interact with the
security features that constitute the security climate of the workplace. We
examined the fundamental challenge that not every employee could recognise
those features. In this multi-stage research, we adopted the theoretical lens
of symbolic interactionism to advance a conceptual model which explains the
relationship between organisation's social networks and the formation of
information security climate. A descriptive case study in Vietnam was then
conducted to refine the proposed model. The findings validated and extended the
dimensions of information security climate, as well as identified the relevant
organisation's social networks (i.e. information, affect, and power) that lead
to its formation.",http://arxiv.org/abs/1606.00884v1,arXiv
Information Systems,E-Government and E-Commerce,Quantitative,"The Effects of Website Quality on Adoption of E-Government Service:
  AnEmpirical Study Applying UTAUT Model Using SEM","In today global age, e-government services have become the main channel for
online communication between the government and its citizens. They aim to
provide citizens with more accessible, accurate, real-time and high quality
services. Therefore, the quality of government websites which provide
e-services is an essential factor in the successful adoption of e-government
services by the public. This paper discusses an investigation of the effect of
the Website Quality (WQ) factor on the acceptance of using e-government
services (G2C) in the Kingdom of Saudi Arabia (KSA) by adopting the Unified
Theory of Acceptance and Use of Technology (UTAUT) Model. Survey Data collected
from 400 respondents were examined using the structural equation modelling
(SEM) technique and utilising AMOS tools. This study found that the factors
that significantly influenced the Use Behaviour of e-government services in KSA
(USE) include Performance Expectancy (PE), Effort expectancy (EE), Facilitating
Conditions (FC) and Website Quality (WQ), while the construct known Social
Influence (SI) did not. Moreover, the results confirm the importance of quality
government websites and support systems as one of the main significant and
influential factors of e-government services adoption. The results of this
study can be helpful to Saudi governmental sectors to adjust their corporate
strategies and plans to advance successful adoption and diffusion of
e-government services (G2C) in KSA.",http://arxiv.org/abs/1211.2410v1,arXiv
Information Systems,E-Government and E-Commerce,Quantitative,"Analysis of Citizens Acceptance for E-government Services: Applying the
  UTAUT Model","E-government services aims to provide citizens with more accessible,
accurate, real-time and high quality services and information. Although the
public sectors in Kingdom of Saudi Arabia (KSA) have promoted their
e-Government services for many years, its uses and achievements are few.
Therefore, this paper explores the key factors of Saudi citizens acceptance
through a research survey and by gathering empirical evidence based on the
Unified Theory of Acceptance and the Use of Technology (UTAUT). Survey Data
collected from 400 respondents was examined using structural equation modelling
(SEM) technique and utilized AMOS tools. The study results explored the factors
that affect the acceptance of e-government services in KSA based on UTAUT
model. Moreover, as a result of this study an amended UTAUT model was proposed.
Such a model contributes to the discussion and development of adoption models
for technology.",http://arxiv.org/abs/1304.3157v1,arXiv
Information Systems,E-Government and E-Commerce,Quantitative,A Survey on Blockchain in E-Government Services: Status and Challenges,"Blockchain technology is referred to as a very secure decentralized,
distributed ledger that records the history of any digital asset. It is being
used in numerous governmental and private sector organizations across numerous
nations. Surveying the current state of blockchain applications and
difficulties in e-government services is the goal of this review. Held to the
account are use cases for current facilities that use blockchain. Finally, it
examines the research gap in blockchain deployment and makes suggestions for
future work for additional research.",http://arxiv.org/abs/2402.02483v1,arXiv
Information Systems,E-Government and E-Commerce,Quantitative,A Narrative Literature Review and E-Commerce Website Research,"In this study, a narrative literature review regarding culture and e-commerce
website design has been introduced. Cultural aspect and e-commerce website
design will play a significant role for successful global e-commerce sites in
the future. Future success of businesses will rely on e-commerce. To compete in
the global e-commerce marketplace, local businesses need to focus on designing
culturally friendly e-commerce websites. To the best of my knowledge, there has
been insignificant research conducted on correlations between culture and
e-commerce website design. The research shows that there are correlations
between e-commerce, culture, and website design. The result of the study
indicates that cultural aspects influence e-commerce website design. This study
aims to deliver a reference source for information systems and information
technology researchers interested in culture and e-commerce website design, and
will show lessfocused research areas in addition to future directions.",http://arxiv.org/abs/1806.07833v2,arXiv
Information Systems,E-Government and E-Commerce,Quantitative,Adoption of e-government in the Republic of Kazakhstan,"This paper identifies factors that influence Kazakhstan's e-Government portal
usage. It determines challenges encountered by citizens while using the portal.
Targeted respondents for the web-based questionnaire survey were the citizens
of Kazakhstan. The technology acceptance model was used as a methodology to
measure attitude towards portal usage. In addition, this paper discusses the
barriers which were experienced by the respondents and can prevent the
successful adoption of e-Government initiative. The results of the analysis
demonstrate that awareness among citizens is high, i.e. the majority of people
have visited it at least once and they perceive the portal to be useful, but
only a limited percentage of citizens' use it on the regular basis. Further,
paper can be used to help IT managers of the portal to improve management of
informational content and maintain a more effective adoption among citizens.",http://arxiv.org/abs/1802.06951v1,arXiv
Information Systems,E-Government and E-Commerce,Qualitative,"Factors Enhancing E-Government Service Gaps in a Developing Country
  Context","Globally, the discourse of e-government has gathered momentum in public
service delivery. No country has been left untouched in the implementation of
e-government. Several government departments and agencies are now using
information and communication technology (ICTs) to deliver government services
and information to citizens, other government departments, and businesses.
However, most of the government departments have not provided all of their
services electronically or at least the most important ones. Thus, this creates
a phenomenon of e-government service gaps. The objective of this study was to
investigate the contextual factors enhancing e-government service gaps in a
developing country. To achieve this aim, the TOE framework was employed
together with a qualitative case study to guide data collection and analysis.
The data was collected through semi-structured interviews from government
employees who are involved in the implementation of e-government services in
Zimbabwe as well as from citizens and businesses. Eleven (11) factors were
identified and grouped under the TOE framework. This research contributes
significantly to the implementation and utilisation of e-government services in
Zimbabwe. The study also contributes to providing a strong theoretical
understanding of the factors that enhance e-government service gaps explored in
the research model.",http://arxiv.org/abs/2108.09803v1,arXiv
Information Systems,E-Government and E-Commerce,Qualitative,"Improving municipal responsiveness through AI-powered image analysis in
  E-Government","Integration of Machine Learning (ML) techniques into public administration
marks a new and transformative era for e-government systems. While
traditionally e-government studies were focusing on text-based interactions,
this one explores the innovative application of ML for image analysis, an
approach that enables governments to address citizen petitions more
efficiently. By using image classification and object detection algorithms, the
model proposed in this article supports public institutions in identifying and
fast responding to evidence submitted by citizens in picture format, such as
infrastructure issues, environmental concerns or other urban issues that
citizens might face. The research also highlights the Jevons Paradox as a
critical factor, wherein increased efficiency from the citizen side (especially
using mobile platforms and apps) may generate higher demand which should lead
to scalable and robust solutions. Using as a case study a Romanian municipality
who provided datasets of citizen-submitted images, the author analysed and
proved that ML can improve accuracy and responsiveness of public institutions.
The findings suggest that adopting ML for e-petition systems can not only
enhance citizen participation but also speeding up administrative processes,
paving the way for more transparent and effective governance. This study
contributes to the discourse on e-government 3.0 by showing the potential of
Artificial Intelligence (AI) to transform public service delivery, ensuring
sustainable (and scalable) solutions for the growing demands of modern urban
governance.",http://arxiv.org/abs/2504.08972v1,arXiv
Information Systems,E-Government and E-Commerce,Qualitative,"How Retailers at different Stages of E-Commerce Maturity Evaluate Their
  Entry to E-Commerce Activities?","This paper investigates how retailers at different stages of e-commerce
maturity evaluate their entry to e-commerce activities. The study was conducted
using qualitative approach interviewing 16 retailers in Saudi Arabia. It comes
up with 22 factors that are believed the most influencing factors for retailers
in Saudi Arabia. Interestingly, there seem to be differences between retailers
in companies at different maturity stages in terms of having different
attitudes regarding the issues of using e-commerce. The businesses that have
reached a high stage of e-commerce maturity provide practical evidence of
positive and optimistic attitudes and practices regarding use of e-commerce,
whereas the businesses that have not reached higher levels of maturity provide
practical evidence of more negative and pessimistic attitudes and practices.
The study, therefore, should contribute to efforts leading to greater
e-commerce development in Saudi Arabia and other countries with similar
context.",http://arxiv.org/abs/1503.05172v1,arXiv
Information Systems,E-Government and E-Commerce,Qualitative,"Achieving employees agile response in e-governance: Exploring the
  synergy of technology and group collaboration","The transformation of technology and collaboration methods driven by the
e-government system forces government employees to reconsider their daily
workflow and collaboration with colleagues. Despite the extensive existing
knowledge of technology usage and collaboration, there are limitations in
explaining the synergy between technology usage and group collaboration in
achieving agile response from the perspective of government employees,
particularly in the e-government setting. To address these challenges, this
study provides a holistic understanding of the successful pathway to agile
response in e-governance from the perspective of government employees. This
study explores a dual path to achieve agile response in e-governance through
qualitative analysis, involving 34 in-depth semi-structured interviews with
government employees in several government sectors in China. By employing three
rounds of coding processes and adopting Interpretative Structural Modeling
(ISM), this study identifies the five-layer mechanisms leading to agile
response in e-governance, considering both government employee technology usage
and group collaboration perspectives. Findings of this study provides
suggestions and implications for achieving agile response in e-governance.",http://arxiv.org/abs/2411.15875v1,arXiv
Information Systems,E-Government and E-Commerce,Qualitative,"On the combination of static analysis for software security assessment
  -- a case study of an open-source e-government project","Static Application Security Testing (SAST) is a popular quality assurance
technique in software engineering. However, integrating SAST tools into
industry-level product development and security assessment poses various
technical and managerial challenges. In this work, we reported a longitudinal
case study of adopting SAST as a part of a human-driven security assessment for
an open-source e-government project. We described how SASTs are selected,
evaluated, and combined into a novel approach for software security assessment.
The approach was preliminarily evaluated using semi-structured interviews. Our
result shows that (1) while some SAST tools out-perform others, it is possible
to achieve better performance by combining more than one SAST tools and (2)
SAST tools should be used towards a practical performance and in the
combination with triangulated approaches for human-driven vulnerability
assessment in real-world projects.",http://arxiv.org/abs/2103.08010v2,arXiv
Information Systems,E-Government and E-Commerce,Mixed Methods,"Empirical Study of Sustaining the Actualized Value Propositions of
  Implemented E-Government Projects in Sub-Saharan Africa","Governments in sub-Saharan Africa have implemented e-Government projects.
Actualizing the value propositions and sustaining such values are becoming
problematic. Some scanty studies on the value propositions of implemented
e-Government projects did not consider actualization of the values. Besides,
such studies lack theoretical underpinnings, the identification, and measure of
what constitutes actualized values. Neither did they capture what mechanisms
could sustain the actualized values nor the contextual conditions enabling its
sustainability. Consequently, using a concept-centric systematic review, we
identified the value proposition of such implemented projects. By drawing from
theories of affordance actualization, realist evaluation (RE) theory,
self-determination theory, and sustainability framework for e-Government
success. We conducted a RE of the implemented e-Government projects in Rwanda
using RE as a methodology in three phases. In phase one, we developed the
initial program theory (IPT), in phase two, we used contingent valuation as a
quantitative approach and realist interview as qualitative method to validate
the IPT. Lastly, in the third phase, we synthesized the results of the two
investigative case studies to develop the actualized values sustainability
framework. Such framework encapsulates, the actualized value propositions,
mechanisms and enabling conditions in interactions to sustain the value
propositions discovered in the e-Government investigative contexts.",http://arxiv.org/abs/2108.09769v1,arXiv
Information Systems,E-Government and E-Commerce,Mixed Methods,"Barriers facing e-service adopting and implementation at local
  environment level in Nigeria","E-Government services offer a great deal of potential to improve government
activities and citizen support. However, there is a lack of research covering
E-Government services at the local government level, particularly in developing
countries. However, implementing successful E-Service technology in this part
of the world will not come without its barriers considering the unstable and
fragile economies in most developing countries. The research aim is to identify
the barriers facing E-Service adoption and implementation at a local
environment level, using Nigeria as a case example.
  This thesis adopts an interpretive paradigm and uses action research. It
consists of a large field study in Nigeria (interviews), an online survey of
government officials, online focus groups, and analyses government documents
and E-Service initiatives. A structured literature review method consisted of
sifting through 3,245 papers. The main theoretical tools used in this thesis
are the diffusion of innovation (DOI) theory and the theory of change.",http://arxiv.org/abs/2406.15375v1,arXiv
Information Systems,E-Government and E-Commerce,Mixed Methods,The Missing Link: Identifying Digital Intermediaries in E-Government,"The digitalization of public administration has advanced significantly on a
global scale. Many governments now view digital platforms as essential for
improving the delivery of public services and fostering direct communication
between citizens and public institutions. However, this view overlooks the role
played by digital intermediaries significantly shape the provision of
e-government services. Using Chile as a case study, we analyze these
intermediaries through a national survey on digitalization, we find five types
of intermediaries: family members, peers, political figures, bureaucrats, and
community leaders. The first two classes comprise close intermediaries, while
the latter three comprise hierarchical intermediaries. Our findings suggest
that all these intermediaries are a critical but underexplored element in the
digitalization of public administration.",http://arxiv.org/abs/2501.10846v1,arXiv
Information Systems,E-Government and E-Commerce,Mixed Methods,"Regional Pilot Study to Evaluate e-Readiness and Local e-Government
  Services","The concept of local e-Government has become a key factor for delivering
services in an efficient, cost effective, transparent and convenient way, in
circumstances where a) citizens do not have enough time available to
communicate with local authorities in order to perform their responsibilities
and needs, and b) information and communication technologies significantly
facilitate administrative procedures and citizens-government interaction. This
paper aims to identify e-services that local authorities provide, and to
investigate their readiness for delivering these services. A pilot research has
been conducted to identify the offer of e-services by local authorities, along
with e-readiness in municipalities of the Pelagonia region in the Republic of
Macedonia. The survey was carried out by means of structured interview
questions based on a modified model proposed by Partnership on Measuring ICT
for Development - web analysis of municipal websites in the region has been
conducted, as well. The study reveals uneven distribution according to the age
group of users, lack of reliability and confidence for processing the needs and
requests electronically by a large part of the population, and improperly
developed set of ICT tools by local governments for providing a variety of
services that can be fully processed electronically.",http://arxiv.org/abs/1406.7866v1,arXiv
Information Systems,E-Government and E-Commerce,Mixed Methods,Product Knowledge Graph Embedding for E-commerce,"In this paper, we propose a new product knowledge graph (PKG) embedding
approach for learning the intrinsic product relations as product knowledge for
e-commerce. We define the key entities and summarize the pivotal product
relations that are critical for general e-commerce applications including
marketing, advertisement, search ranking and recommendation. We first provide a
comprehensive comparison between PKG and ordinary knowledge graph (KG) and then
illustrate why KG embedding methods are not suitable for PKG learning. We
construct a self-attention-enhanced distributed representation learning model
for learning PKG embeddings from raw customer activity data in an end-to-end
fashion. We design an effective multi-task learning schema to fully leverage
the multi-modal e-commerce data. The Poincare embedding is also employed to
handle complex entity structures. We use a real-world dataset from
grocery.walmart.com to evaluate the performances on knowledge completion,
search ranking and recommendation. The proposed approach compares favourably to
baselines in knowledge completion and downstream tasks.",http://arxiv.org/abs/1911.12481v1,arXiv
Information Systems,E-Government and E-Commerce,Design and Development,A New Fuzzy MCDM Framework to Evaluate E-Government Security Strategy,"Ensuring security of e-government applications and infrastructures is crucial
to maintain trust among stakeholders to store, process and exchange information
over the e-government systems. Due to dynamic and continuous threats on
e-government information security, policy makers need to perform evaluation on
existing information security strategy as to deliver trusted e-government
services. This paper presents an information security evaluation framework
based on new fuzzy multi criteria decision making (MCDM) to help policy makers
conduct comprehensive assessment of e-government security strategy.",http://arxiv.org/abs/1011.3101v1,arXiv
Information Systems,E-Government and E-Commerce,Design and Development,"Semantic-Driven e-Government: Application of Uschold and King Ontology
  Building Methodology for Semantic Ontology Models Development","Electronic government (e-government) has been one of the most active areas of
ontology development during the past six years. In e-government, ontologies are
being used to describe and specify e-government services (e-services) because
they enable easy composition, matching, mapping and merging of various
e-government services. More importantly, they also facilitate the semantic
integration and interoperability of e-government services. However, it is still
unclear in the current literature how an existing ontology building methodology
can be applied to develop semantic ontology models in a government service
domain. In this paper the Uschold and King ontology building methodology is
applied to develop semantic ontology models in a government service domain.
Firstly, the Uschold and King methodology is presented, discussed and applied
to build a government domain ontology. Secondly, the domain ontology is
evaluated for semantic consistency using its semi-formal representation in
Description Logic. Thirdly, an alignment of the domain ontology with the
Descriptive Ontology for Linguistic and Cognitive Engineering (DOLCE) upper
level ontology is drawn to allow its wider visibility and facilitate its
integration with existing metadata standard. Finally, the domain ontology is
formally written in Web Ontology Language (OWL) to enable its automatic
processing by computers. The study aims to provide direction for the
application of existing ontology building methodologies in the Semantic Web
development processes of e-government domain specific ontology models; which
would enable their repeatability in other e-government projects and strengthen
the adoption of semantic technologies in e-government.",http://arxiv.org/abs/1111.1941v1,arXiv
Information Systems,E-Government and E-Commerce,Design and Development,An Innovative Approach for E-Government Transformation,"Despite the immeasurable investment in e-government initiatives throughout
the world, such initiatives have yet to succeed in fully meeting expectations
and desired outcomes. A key objective of this research article is to support
the government of the UAE in realizing its vision of e-government
transformation. It presents an innovative framework to support e-government
implementation, which was developed from a practitioner's perspective and based
on learnings from numerous e-government practices around the globe. The
framework presents an approach to guide governments worldwide, and UAE in
particular, to develop a top down strategy and leverage technology in order
realize its long term goal of e-government transformation. The study also
outlines the potential role of modern national identity schemes in enabling the
transformation of traditional identities into digital identities. The work
presented in this study is envisaged to help bridge the gap between policy
makers and implementers, by providing greater clarity and reducing misalignment
on key elements of e-government transformation. In the hands of leaders that
have a strong will to invest in e-government transformation, the work presented
in this study is envisaged to become a powerful tool to communicate and
coordinate initiatives, and provide a clear visualization of an integrated
approach to e-government transformation.",http://arxiv.org/abs/1105.6358v1,arXiv
Information Systems,E-Government and E-Commerce,Design and Development,"Electronic-government in Saudi Arabia: A positive revolution in the
  peninsula","The informatization practice of countries all over the world has shown that
the level of a government's informatization is one main factor that can affect
its international competitive power. At present, e-government construction is
regarded as one of the most important tasks for the national economy and
society upliftment and informatization in Saudi Arabia. Unlike the traditional
governments, an e-government takes on a new look with its framework and
operation mode more suitable for the contemporary era. In fact, it is a basic
national strategy to promote Saudi Arabia's informatization by means of
e-government construction. This talk firstly introduces the basic concepts and
relevant viewpoints of egovernment, then reviews the development process of
e-government in Saudi Arabia, and describes the current states, development
strategies of e-government in Saudi Arabia. And also review e-government
maturity models and synthesize them e-government maturity models are
investigated, in which the authors have proposed the Delloite's six-stage
model, Layne and Lee four-stage model and Accenture five-stage model. So, the
main e-government maturity stages are: online presence, interaction,
transaction, transformation and digital democracy. After that, according to
many references, the main technologies which are used in each stage are
summarized.",http://arxiv.org/abs/1205.3986v1,arXiv
Information Systems,E-Government and E-Commerce,Design and Development,"Belief-Rule-Based Expert Systems for Evaluation of E- Government: A Case
  Study","Little knowledge exists on the impact and results associated with
e-government projects in many specific use domains. Therefore it is necessary
to evaluate the efficiency and effectiveness of e-government systems. Since the
development of e-government is a continuous process of improvement, it requires
continuous evaluation of the overall e-government system as well as evaluation
of its various dimensions such as determinants, characteristics and results.
E-government development is often complex with multiple stakeholders, large
user bases and complex goals. Consequently, even experts have difficulties in
evaluating these systems, especially in an integrated and comprehensive way as
well as on an aggregate level. Expert systems are a candidate solution to
evaluate such complex e-government systems. However, it is difficult for expert
systems to cope with uncertain evaluation data that are vague, inconsistent,
highly subjective or in other ways challenging to formalize. This paper
presents an approach that can handle uncertainty in e-government evaluation:
The combination of Belief Rule Base (BRB) knowledge representation and
Evidential Reasoning (ES). This approach is illustrated with a concrete
prototype, known as Belief Rule Based Expert System (BRBES) and put to use in
the local e-government of Bangladesh. The results have been compared with a
recently developed method of evaluating e-Government, and it is shown that the
results of BRBES are more accurate and reliable. BRBES can be used to identify
the factors that need to be improved to achieve the overall aim of an
e-government project. In addition, various ""what if"" scenarios can be generated
and developers and managers can get a forecast of the outcomes. In this way,
the system can be used to facilitate decision making processes under
uncertainty.",http://arxiv.org/abs/1403.5618v2,arXiv
Information Systems,E-Government and E-Commerce,Theoretical / Conceptual,Towards an Understanding of Valence in E-Government Services,"The Australian government, to remind job seekers of appointments with
employment services providers in order to cut costs and free up human
resources, is using technologies such as Short Messaging Services (SMS).
However, the technologies in-use are but one side of this equation the
specifics of how these technologies are used is the other side, and these
specifics are highly under-theorized, particularly in regard to the views of
the people to which these technologies are directed. The purpose of this paper
is to provide a theoretical framing for this phenomenon as well as to introduce
an emerging methodological direction that may allow for a better understanding
of demographic-specific values and thereby better valence framing. The paper
also theorizes reactions to information that could be applicable elsewhere, not
just in e-government or with SMS, thereby contributing to discussions
surrounding the Big Data debate.",http://arxiv.org/abs/1606.01360v1,arXiv
Information Systems,E-Government and E-Commerce,Theoretical / Conceptual,"Towards an automated repository for indexing, analysis and
  characterization of municipal e-government websites in Mexico","This article addresses a problem in the electronic government discipline with
special interest in Mexico: the need for a concentrated and updated information
source about municipal e-government websites. One reason for this is the lack
of a complete and updated database containing the electronic addresses (web
domain names) of the municipal governments having a website. Due to diverse
causes, not all the Mexican municipalities have one, and a number of those
having it do not present information corresponding to the current governments
but, instead, to other previous ones. The scarce official lists of municipal
websites are not updated with the sufficient frequency, and manually
determining which municipalities have an operating and valid website in a given
moment is a time-consuming process. Besides, website contents do not always
comply with legal requirements and are considerably heterogeneous. In turn, the
evolution development level of municipal websites is valuable information that
can be harnessed for diverse theoretical and practical purposes in the public
administration field. Obtaining all these pieces of information requires
website content analysis. Therefore, this article investigates the need for and
the feasibility to automate implementation and updating of a digital repository
to perform diverse analyses of these websites. Its technological feasibility is
addressed by means of a literature review about web scraping and by proposing a
preliminary manual methodology. This takes into account known, proven,
techniques and software tools for web crawling and scraping. No new techniques
for crawling or scraping are proposed because the existing ones satisfy the
current needs. Finally, software requirements are specified in order to
automate the creation, updating, indexing, and analyses of the repository.",http://arxiv.org/abs/2006.14746v1,arXiv
Information Systems,E-Government and E-Commerce,Theoretical / Conceptual,"Intelligent Classification and Personalized Recommendation of E-commerce
  Products Based on Machine Learning","With the rapid evolution of the Internet and the exponential proliferation of
information, users encounter information overload and the conundrum of choice.
Personalized recommendation systems play a pivotal role in alleviating this
burden by aiding users in filtering and selecting information tailored to their
preferences and requirements. Such systems not only enhance user experience and
satisfaction but also furnish opportunities for businesses and platforms to
augment user engagement, sales, and advertising efficacy.This paper undertakes
a comparative analysis between the operational mechanisms of traditional
e-commerce commodity classification systems and personalized recommendation
systems. It delineates the significance and application of personalized
recommendation systems across e-commerce, content information, and media
domains. Furthermore, it delves into the challenges confronting personalized
recommendation systems in e-commerce, including data privacy, algorithmic bias,
scalability, and the cold start problem. Strategies to address these challenges
are elucidated.Subsequently, the paper outlines a personalized recommendation
system leveraging the BERT model and nearest neighbor algorithm, specifically
tailored to address the exigencies of the eBay e-commerce platform. The
efficacy of this recommendation system is substantiated through manual
evaluation, and a practical application operational guide and structured output
recommendation results are furnished to ensure the system's operability and
scalability.",http://arxiv.org/abs/2403.19345v1,arXiv
Information Systems,E-Government and E-Commerce,Theoretical / Conceptual,"Theoretical Understandings of Product Embedding for E-commerce Machine
  Learning","Product embeddings have been heavily investigated in the past few years,
serving as the cornerstone for a broad range of machine learning applications
in e-commerce. Despite the empirical success of product embeddings, little is
known on how and why they work from the theoretical standpoint. Analogous
results from the natural language processing (NLP) often rely on
domain-specific properties that are not transferable to the e-commerce setting,
and the downstream tasks often focus on different aspects of the embeddings. We
take an e-commerce-oriented view of the product embeddings and reveal a
complete theoretical view from both the representation learning and the
learning theory perspective. We prove that product embeddings trained by the
widely-adopted skip-gram negative sampling algorithm and its variants are
sufficient dimension reduction regarding a critical product relatedness
measure. The generalization performance in the downstream machine learning task
is controlled by the alignment between the embeddings and the product
relatedness measure. Following the theoretical discoveries, we conduct
exploratory experiments that supports our theoretical insights for the product
embeddings.",http://arxiv.org/abs/2102.12029v1,arXiv
Information Systems,E-Government and E-Commerce,Theoretical / Conceptual,So What's the Plan? Mining Strategic Planning Documents,"In this paper we present a corpus of Russian strategic planning documents,
RuREBus. This project is grounded both from language technology and
e-government perspectives. Not only new language sources and tools are being
developed, but also their applications to e-goverment research. We demonstrate
the pipeline for creating a text corpus from scratch. First, the annotation
schema is designed. Next texts are marked up using human-in-the-loop strategy,
so that preliminary annotations are derived from a machine learning model and
are manually corrected. The amount of annotated texts is large enough to
showcase what insights can be gained from RuREBus.",http://arxiv.org/abs/2007.00257v2,arXiv
Information Technology,Network and Infrastructure,Quantitative,"InfraRisk: An Open-Source Simulation Platform for Asset-Level Resilience
  Analysis in Interconnected Infrastructure Networks","Integrated simulation models are emerging as an alternative for analyzing
large-scale interdependent infrastructure networks due to their modeling
advantages over traditional interdependency models. This paper presents an
open-source integrated simulation package for the asset-level analysis of
interdependent infrastructure systems. The simulation platform, named
'InfraRisk' and developed in Python, can simulate disaster-induced
infrastructure failures and subsequent post-disaster restoration in
interconnected water-, power-, and road networks. InfraRisk consists of an
infrastructure module, a hazard module, a recovery module, a simulation module,
and a resilience quantification module. The infrastructure module integrates
existing infrastructure network packages (wntr for water networks, pandapower
for power systems, and a static traffic assignment model for transportation
networks) through an interface that facilitates the network-level simulation of
interdependent failures. The hazard module generates infrastructure component
failure sequences based on various disaster characteristics. The recovery
module determines repair sequences and assigns repair crews based on predefined
heuristics-based recovery strategies or model predictive control (MPC) based
optimization. Based on the schedule, the simulation module implements the
network-wide simulation of the consequences of the disaster impacts and the
recovery actions. The resilience quantification module offers system-level and
consumer-level metrics to quantify both the risks and resilience of the
integrated infrastructure networks against disaster events. InfraRisk provides
a virtual platform for decision-makers to experiment and develop
region-specific pre-disaster and post-disaster policies to enhance the overall
resilience of interdependent urban infrastructure networks.",http://arxiv.org/abs/2205.04717v1,arXiv
Information Technology,Network and Infrastructure,Quantitative,"Network Properties for Robust Multilayer Infrastructure Systems: A
  Percolation Theory Review","Infrastructure systems, such as power, transportation, telecommunication, and
water systems, are composed of multiple components which are interconnected and
interdependent to produce and distribute essential goods and services. So, the
robustness of infrastructure systems to resist disturbances is crucial for the
durable performance of modern societies. Multilayer networks have been used to
model the multiplicity and interrelation of infrastructure systems and
percolation theory is the most common approach to quantify the robustness of
such networks. This survey systematically reviews literature published between
2010 and 2021, on applying percolation theory to assess the robustness of
infrastructure systems modeled as multilayer networks. We discussed all network
properties applied to build infrastructure models. Among all properties,
interdependency strength and communities were the most common network property
whilst very few studies considered realistic attributes of infrastructure
systems such as directed links and feedback conditions. The review highlights
that the properties produced approximately similar model outcomes, in terms of
detecting improvement or deterioration in the robustness of multilayer
infrastructure networks, with few exceptions. Most of the studies focused on
highly simpliffied synthetic models rather than models built by real datasets.
Thus, this review suggests analyzing multiple properties in a single model to
assess whether they boost or weaken the impact of each other. In addition, the
effect size of different properties on the robustness of infrastructure systems
should be quantiffied. It can support the design and planning of robust
infrastructure systems by arranging and prioritizing the most effective
properties.",http://arxiv.org/abs/2105.12701v2,arXiv
Information Technology,Network and Infrastructure,Quantitative,Safety Requirement Specifications for Connected Vehicles,"In the coming years, transportation system will be revamped in a manner that
there will be more intelligent and autonomous vehicle phenomenon around us such
as smart cars, auto driving system, etc. Some of automotive industries are
already producing smart cars. However, the main concern of this paper is on the
infrastructure for connected vehicles, which can support such intelligent
transportation. Current transportation system lacks proper infrastructure to
support connected vehicles. Hence, in this article, we have surveyed and
analyzed the current transportation system in developed and developing
countries. In contrast, we are going to introduce secure intelligent
transportation (roadside) infrastructure that is user centric (Driver,
Autonomous driver etc.) for connected vehicles. In this paper we present the
basic requirements of safety engineering infrastructure of roadside
infrastructure in ITS for connected vehicles. Connected vehicles has network
infrastructure to communicate with vehicle-to-vehicle (V-to-V),
vehicle-to-infrastructure (V-to-I), lane correction system, and traffic
information system etc. The connected vehicle is a good model for learning
demands of infrastructure for ITS process because the system having a lot of
use-cases and we must understand relationship between public institutions,
people, companies in order to proceed ITS System.",http://arxiv.org/abs/1707.08715v2,arXiv
Information Technology,Network and Infrastructure,Quantitative,Topological Convergence of Urban Infrastructure Networks,"Urban infrastructure networks play a major role in providing reliable flows
of multitude critical services demanded by citizens in modern cities. We
analyzed here a database of 125 infrastructure networks, roads (RN); urban
drainage networks (UDN); water distribution networks (WDN), in 52 global
cities, serving populations ranging from 1,000 to 9,000,000. For all
infrastructure networks, the node-degree distributions, p(k), derived using
undirected, dual-mapped graphs, fit Pareto distributions. Variance around mean
gamma reduces substantially as network size increases. Convergence of
functional topology of these urban infrastructure networks suggests that their
co-evolution results from similar generative mechanisms. Analysis of growing
UDNs over non-concurrent 40 year periods in three cities suggests the likely
generative process to be partial preferential attachment under geospatial
constraints. This finding is supported by high-variance node-degree
distributions as compared to that expected for a Poisson random graph. Directed
cascading failures, from UDNs to RNs, are investigated. Correlation of
node-degrees between spatially co-located networks are shown to be a major
factor influencing network fragmentation by node removal. Our results hold
major implications for the network design and maintenance, and for resilience
of urban communities relying on multiplex infrastructure networks for mobility
within the city, water supply, and wastewater collection and treatment.",http://arxiv.org/abs/1902.01266v1,arXiv
Information Technology,Network and Infrastructure,Quantitative,"Integrating social capital with urban infrastructure networks for more
  resilient cities","More than half of the world's population now lives in urban environments,
which concentrate services and infrastructure to satisfy the material needs of
a growing number of inhabitants. The interdependencies between physical
infrastructure systems are required for cities to function efficiently, but
simultaneously expose cities to new hazards. Failures that emerge from one
infrastructure system and cascade through these interdependencies are becoming
larger and more frequent due to climate change and growing urban environments.
Because of the uneven distribution of resources and basic services, cascade
failures often exacerbate pre-existing socioeconomic inequalities. Human
communities rely on both social capital and infrastructure services to prepare
for, manage, and recover from these challenging scenarios, but the overlap
between social and physical infrastructure creates unpredictable feedback
dynamics. While prior research has focused on either social capital or physical
infrastructure in urban disaster management, an integrative view of these two
perspectives is seldom explored. In this paper, the feedback mechanisms between
the physical and social layers of different urban designs are identified and
analyzed to optimize relief response. Methodologically, we identify cities with
high accessibility that have undergone disasters. From these cities, we measure
their physical and social resilience indicators before and after disaster as a
means to evaluate the impact of accessibility on disaster relief and
preparedness. We will supplement this empirical analysis with a simulation that
captures a cascade failure/disaster through a multilayer infrastructure and
social network model.",http://arxiv.org/abs/2502.06328v1,arXiv
Information Technology,Network and Infrastructure,Qualitative,"Modelling interdependencies between the electricity and information
  infrastructures","The aim of this paper is to provide qualitative models characterizing
interdependencies related failures of two critical infrastructures: the
electricity infrastructure and the associated information infrastructure. The
interdependencies of these two infrastructures are increasing due to a growing
connection of the power grid networks to the global information infrastructure,
as a consequence of market deregulation and opening. These interdependencies
increase the risk of failures. We focus on cascading, escalating and
common-cause failures, which correspond to the main causes of failures due to
interdependencies. We address failures in the electricity infrastructure, in
combination with accidental failures in the information infrastructure, then we
show briefly how malicious attacks in the information infrastructure can be
addressed.",http://arxiv.org/abs/0809.4107v1,arXiv
Information Technology,Network and Infrastructure,Qualitative,"A Dynamic Game Analysis and Design of Infrastructure Network Protection
  and Recovery","Infrastructure networks are vulnerable to both cyber and physical attacks.
Building a secure and resilient networked system is essential for providing
reliable and dependable services. To this end, we establish a two-player
three-stage game framework to capture the dynamics in the infrastructure
protection and recovery phases. Specifically, the goal of the infrastructure
network designer is to keep the network connected before and after the attack,
while the adversary aims to disconnect the network by compromising a set of
links. With costs for creating and removing links, the two players aim to
maximize their utilities while minimizing the costs. In this paper, we use the
concept of subgame perfect equilibrium (SPE) to characterize the optimal
strategies of the network defender and attacker. We derive the SPE explicitly
in terms of system parameters. Finally, we use a case study of UAV-enabled
communication networks for disaster recovery to corroborate the obtained
analytical results.",http://arxiv.org/abs/1707.07054v1,arXiv
Information Technology,Network and Infrastructure,Qualitative,On the resilience of cellular networks: how can national roaming help?,"Cellular networks have become one of the critical infrastructures, as many
services depend increasingly on wireless connectivity. Therefore, it is
important to quantify the resilience of existing cellular network
infrastructures against potential risks, ranging from natural disasters to
security attacks, that might occur with a low probability but can lead to
severe disruption of the services. In this paper, we combine models with public
data from national bodies on mobile network operator (MNO) infrastructures,
population distribution, and urbanity level to assess the coverage and capacity
of a cellular network at a country scale. Our analysis offers insights on the
potential weak points that need improvement to ensure a low fraction of
disconnected population (FDP) and high fraction of satisfied population (FSP).
As a resilience improvement approach, we investigate in which regions and to
what extent each MNO can benefit from infrastructure sharing or national
roaming, i.e., all MNOs act as a single national operator. As our case study,
we focus on Dutch cellular infrastructure and model risks as random failures,
correlated failures in a geographic region, and abrupt increase in the number
of users. Our analysis shows that there is a wide performance difference across
MNOs and geographic regions in terms of FDP and FSP. However, national roaming
consistently offers significant benefits, e.g., up to 13% improvement in FDP
and up to 55% in FSP when the networks function without any failures.",http://arxiv.org/abs/2301.03250v3,arXiv
Information Technology,Network and Infrastructure,Qualitative,A Road Segment Prioritization Approach for Cycling Infrastructure,"Understanding the motivators and deterrents to cycling is essential for
creating infrastructure that gets more people to adopt cycling as a mode of
transport. This paper demonstrates a new approach to support the prioritization
of cycling infrastructure and cycling network design, accounting for cyclist
preferences and the growing emphasis on 'filtered permeability' and 'Low
Traffic Neighborhood' interventions internationally. The approach combines
distance decay, route calculation, and network analysis methods to examine
where future cycling demand is most likely to arise, how such demand could be
accommodated within existing street networks, and how to ensure a fair
distribution of investment. Although each of these methods has been applied to
cycling infrastructure prioritization in previous research, this is the first
time that they have been combined, creating an integrated road segment
prioritization approach. The approach, which can be applied to other cities, as
shown in the Appendix, is demonstrated in a case study of Manchester, resulting
in cycling networks that balance directness against the need for safe and
stress-free routes under different investment scenarios. A key benefit of the
approach from a policy perspective is its ability to support egalitarian and
cost-effective strategic cycle network planning.",http://arxiv.org/abs/2105.03712v1,arXiv
Information Technology,Network and Infrastructure,Qualitative,OnionBots: Subverting Privacy Infrastructure for Cyber Attacks,"Over the last decade botnets survived by adopting a sequence of increasingly
sophisticated strategies to evade detection and take overs, and to monetize
their infrastructure. At the same time, the success of privacy infrastructures
such as Tor opened the door to illegal activities, including botnets,
ransomware, and a marketplace for drugs and contraband. We contend that the
next waves of botnets will extensively subvert privacy infrastructure and
cryptographic mechanisms. In this work we propose to preemptively investigate
the design and mitigation of such botnets. We first, introduce OnionBots, what
we believe will be the next generation of resilient, stealthy botnets.
OnionBots use privacy infrastructures for cyber attacks by completely
decoupling their operation from the infected host IP address and by carrying
traffic that does not leak information about its source, destination, and
nature. Such bots live symbiotically within the privacy infrastructures to
evade detection, measurement, scale estimation, observation, and in general all
IP-based current mitigation techniques. Furthermore, we show that with an
adequate self-healing network maintenance scheme, that is simple to implement,
OnionBots achieve a low diameter and a low degree and are robust to
partitioning under node deletions. We developed a mitigation technique, called
SOAP, that neutralizes the nodes of the basic OnionBots. We also outline and
discuss a set of techniques that can enable subsequent waves of Super
OnionBots. In light of the potential of such botnets, we believe that the
research community should proactively develop detection and mitigation methods
to thwart OnionBots, potentially making adjustments to privacy infrastructure.",http://arxiv.org/abs/1501.03378v1,arXiv
Information Technology,Network and Infrastructure,Mixed Methods,"A Bayesian Approach to Reconstructing Interdependent Infrastructure
  Networks from Cascading Failures","Analyzing the behavior of complex interdependent networks requires complete
information about the network topology and the interdependent links across
networks. For many applications such as critical infrastructure systems,
understanding network interdependencies is crucial to anticipate cascading
failures and plan for disruptions. However, data on the topology of individual
networks are often publicly unavailable due to privacy and security concerns.
Additionally, interdependent links are often only revealed in the aftermath of
a disruption as a result of cascading failures. We propose a scalable
nonparametric Bayesian approach to reconstruct the topology of interdependent
infrastructure networks from observations of cascading failures.
Metropolis-Hastings algorithm coupled with the infrastructure-dependent
proposal are employed to increase the efficiency of sampling possible graphs.
Results of reconstructing a synthetic system of interdependent infrastructure
networks demonstrate that the proposed approach outperforms existing methods in
both accuracy and computational time. We further apply this approach to
reconstruct the topology of one synthetic and two real-world systems of
interdependent infrastructure networks, including gas-power-water networks in
Shelby County, TN, USA, and an interdependent system of power-water networks in
Italy, to demonstrate the general applicability of the approach.",http://arxiv.org/abs/2211.15590v1,arXiv
Information Technology,Network and Infrastructure,Mixed Methods,Internet-human infrastructures: Lessons from Havana's StreetNet,"We propose a mixed-methods approach to understanding the human infrastructure
underlying StreetNet (SNET), a distributed, community-run intranet that serves
as the primary 'Internet' in Havana, Cuba. We bridge ethnographic studies and
the study of social networks and organizations to understand the way that power
is embedded in the structure of Havana's SNET. By quantitatively and
qualitatively unpacking the human infrastructure of SNET, this work reveals how
distributed infrastructure necessarily embeds the structural aspects of
inequality distributed within that infrastructure. While traditional technical
measurements of networks reflect the social, organizational, spatial, and
technical constraints that shape the resulting network, ethnographies can help
uncover the texture and role of these hidden supporting relationships. By
merging these perspectives, this work contributes to our understanding of
network roles in growing and maintaining distributed infrastructures, revealing
new approaches to understanding larger, more complex Internet-human
infrastructures---including the Internet and the WWW.",http://arxiv.org/abs/2004.12207v1,arXiv
Information Technology,Network and Infrastructure,Mixed Methods,Planning minimum regret $CO_2$ pipeline networks,"The transition to a low-carbon economy necessitates effective carbon capture
and storage (CCS) solutions, particularly for hard-to-abate sectors. Herein,
pipeline networks are indispensable for cost-efficient $CO_2$ transportation
over long distances. However, there is deep uncertainty regarding which
industrial sectors will participate in such systems. This poses a significant
challenge due to substantial investments as well as the lengthy planning and
development timelines required for $CO_2$ pipeline projects, which are further
constrained by limited upgrade options for already built infrastructure. The
economies of scale inherent in pipeline construction exacerbate these
challenges, leading to potential regret over earlier decisions. While numerous
models were developed to optimize the initial layout of pipeline infrastructure
based on known demand, a gap exists in addressing the incremental development
of infrastructure in conjunction with deep uncertainty. Hence, this paper
introduces a novel optimization model for $CO_2$ pipeline infrastructure
development, minimizing regret as its objective function and incorporating
various upgrade options, such as looping and pressure increases. The model's
effectiveness is also demonstrated by presenting a comprehensive case study of
Germany's cement and lime industries. The developed approach quantitatively
illustrates the trade-off between different options, which can help in deriving
effective strategies for $CO_2$ infrastructure development.",http://arxiv.org/abs/2502.12035v1,arXiv
Information Technology,Network and Infrastructure,Mixed Methods,"Modeling Infrastructure Sharing in mmWave Networks with Shared Spectrum
  Licenses","Competing cellular operators aggressively share infrastructure in many major
US markets. If operators also were to share spectrum in next-generation
millimeter-wave (mmWave) networks, intra-cellular interference will become
correlated with inter-cellular interference. We propose a mathematical
framework to model a multi-operator mmWave cellular network with co-located
base-stations (BSs). We then characterize the signal-to-interference-plus-noise
ratio (SINR) distribution for an arbitrary network and derive its coverage
probability. To understand how varying the spatial correlation between
different networks affects coverage probability, we derive special results for
the two-operator scenario, where we construct the operators' individual
networks from a single network via probabilistic coupling. For external
validation, we devise a method to quantify and estimate spatial correlation
from actual base-station deployments. We compare our two-operator model against
an actual macro-cell-dominated network and an actual network primarily
comprising distributed-antenna-system (DAS) nodes. Using the actual deployment
data to set the parameters of our model, we observe that coverage probabilities
for the model and actual deployments not only compare very well to each other,
but also match nearly perfectly for the case of the DAS-node-dominated
deployment. Another interesting observation is that a network that shares
spectrum and infrastructure has a lower rate coverage probability than a
network of the same number of BSs that shares neither spectrum nor
infrastructure, suggesting that the latter is more suitable for low-rate
applications.",http://arxiv.org/abs/1709.05741v2,arXiv
Information Technology,Network and Infrastructure,Mixed Methods,"The Sensitivity of Electric Power Infrastructure Resilience to the
  Spatial Distribution of Disaster Impacts","Credibly assessing the resilience of energy infrastructure in the face of
natural disasters is a salient concern facing researchers, government
officials, and community members. Here, we explore the influence of the spatial
distribution of disruptions due to hurricanes and other natural hazards on the
resilience of power distribution systems. We find that incorporating
information about the spatial distribution of disaster impacts has significant
implications for estimating infrastructure resilience. Specifically, the
uncertainty associated with estimated infrastructure resilience metrics to
spatially distributed disaster-induced disruptions is much higher than
determined by previous methods. We present a case study of an electric power
distribution grid impacted by a major landfalling hurricane. We show that
improved characterizations of disaster disruption drastically change the way in
which the grid recovers, including changes in emergent system properties such
as antifragility. Our work demonstrates that previous methods for estimating
critical infrastructure resilience may be overstating the confidence associated
with estimated network recoveries due to the lack of consideration of the
spatial structure of disruptions.",http://arxiv.org/abs/1902.02879v1,arXiv
Information Technology,Network and Infrastructure,Design and Development,Cybersecurity Policies for Critical Infrastructure: Legal Mandates and Network Protection Requirements,"<jats:p>Securing crucial infrastructure, which incorporates ranges like vitality, transportation, healthcare, and finance, is exceptionally imperative since it plays an enormous portion in keeping the nation secure and secure. This paper study about how cybersecurity approaches and law necessities work together to ensure safeguard the critical resources from modern cyber threats. It gives an intensive approach toward the rules and directions that control hacking in critical areas, centring on the ones that must be taken after and universal guidelines of cybersecurity policies. The paper also discuss about the need for network security and centres on ways to form systems more versatile, such as through hazard evaluations, risk data, and occurrence reaction methods. Modern innovations like AI and blockchain are looked at to see how they could be able to form critical frameworks more secure. At the same time, the issues of lawful compliance, national issues, and information protection are carefully considered. This study considers supportive since it combines legitimate necessities and specialized security measures. It appears how to form solid cybersecurity plans that take after national and universal rules. These plans will ensure basic framework and keep operations running easily whereas moreover being legitimately mindful.</jats:p>",https://doi.org/10.70985/ns.v2024i8.51,CrossRef
Information Technology,Network and Infrastructure,Design and Development,Scheduling Algorithm in Infrastructure Less Network for SJF,"<jats:p>Some of recent researchers interested in the field of sensor network based wireless field (WSN) because of very difficult to communication via air medium due to lack of problems in using same carrier channel in all communication and easy to attack for security issues like malicious attacks or misbehaviour attacks, grey hole attacks, block hole attacks, etc. In truest based path not easy to fine this drawback overcome we proposed and implement one type of scheduling algorithm called shortest first job with help of infrastructure less network. This leading algorithm is overcome above issue of security issue in the wireless sensor networks problems, because of finding best cost and distance unbreakable path between sender to receiver simulated using NS2.</jats:p>",https://doi.org/10.48001/jocnv.2023.118-10,CrossRef
Information Technology,Network and Infrastructure,Design and Development,Generating network representations of small‐scale infrastructure using generally available data,"<jats:title>Abstract</jats:title><jats:p>Risk analysis (including resilience analysis) of infrastructure requires models that describe the connection of components and subsequent flow dynamics. However, the detailed information needed to define these models may not be available, especially for small‐scale infrastructure that connect to every building. In this paper, we generate location‐specific small‐scale networks using detailed data that should always be available. We propose a general framework where we generate the network topology, we estimate the resource demand at each building, and we design the network components to meet the demands. This general framework is applicable to all types of infrastructure, but many procedures are specific to the type of network being generated. This paper develops the necessary procedures to generate sewer networks and illustrates the usage for an example network in a small study area in Seaside, Oregon. The proposed sewer network generator produces realistic sewer networks as compared to the real network of Seaside.</jats:p>",https://doi.org/10.1111/mice.13137,CrossRef
Information Technology,Network and Infrastructure,Design and Development,Building Seamless Network Infrastructure for Scalable Service Integration in Smart Enterprises,"<jats:p>Building seamless network infrastructure with resilient QoS provisioning capabilities is important for integrating multiple virtualization technologies and scalable multiple service level agreement (SLA)-based services in an enterprise supporting large organizations and crowds. In this paper, a cloud network infrastructure for SLA-based networking as a service (NaaS) is introduced, which can evolve with SDN/NFV-based cloud networking innovations . Moreover, an intelligent agent-based reliable service provisioning framework is proposed that helps networking and cloud service integrating in a seamless manner in enterprises by investigating the cloud networking technologies and their interoperability challenges. Smart Enterprises are being formed and expanded by numerous organizations near and far owing to the fast advancement of information and communication technologies (ICTs). Their business process entails integration with numerous services of diverse service providers. Cloud service provisioning needs faster on-demand integration of possible service providers and their services in an enterprise. In such an enterprise with thousands of service providers, scalability, performance reliability, security, customer driven SLA assurance innovations and QoS monitoring are mainly important for meeting future service provisioning needs . Resilient QoS provisioning capabilities are critically important for large NeTs supporting enterprise-wide and city-wise resilient communications. Key innovations include new architectures for Wide Area, Access and Metro coalescence, SDN/NFV-based cloud networking innovations, cloud resource-rich network infrastructures and new QoS provisioning algorithms. Huge numbers of large Carrier NeTs are being massively deployed with numerous access technologies in many places, which is critical for the sustainable expansion and early realization of Smart Cities. Wi-Fi hotspots, Mobile Cellular communications and Optical Fiber access are a few add-on examples.</jats:p>",https://doi.org/10.53555/jrtdd.v6i10s(2).3609,CrossRef
Information Technology,Network and Infrastructure,Design and Development,Infrastructure as Code for Security Automation and Network Infrastructure Monitoring,"<jats:p>The Corona Virus (COVID-19) pandemic that has spread throughout the world has created a new work culture, namely working remotely by utilizing existing technology. This has the effect of increasing crime and cyber attacks as more and more devices are connected to the internet for work. Therefore, the priority on security and monitoring of network infrastructure should be increased. The security and monitoring of this infrastructure requires an administrator in its management and configuration. One administrator can manage multiple infrastructures, making the task more difficult and time-consuming. This research implements infrastructure as code for security automation and network infrastructure monitoring including IDS, honeypot, and SIEM. Automation is done using ansible tools to create virtual machines to security configuration and monitoring of network infrastructure automatically. The results obtained are automation processes and blackbox testing is carried out and validation is carried out using a User Acceptance Test to the computer apparatus of the IT Poltek SSN Unit to prove the ease of the automation carried out. Based on the results of the UAT, a score of 154 was obtained in the Agree area with an acceptance rate of 81.05% for the implementation of infrastructure as code for the automation carried out</jats:p>",https://doi.org/10.30812/matrik.v22i1.2471,CrossRef
Information Technology,Network and Infrastructure,Theoretical / Conceptual,Reliability of Critical Infrastructure Networks: Challenges,"Critical infrastructures form a technological skeleton of our world by
providing us with water, food, electricity, gas, transportation, communication,
banking, and finance. Moreover, as urban population increases, the role of
infrastructures become more vital. In this paper, we adopt a network
perspective and discuss the ever growing need for fundamental interdisciplinary
study of critical infrastructure networks, efficient methods for estimating
their reliability, and cost-effective strategies for enhancing their
resiliency. We also highlight some of the main challenges arising on this way,
including cascading failures, feedback loops, and cross-sector
interdependencies.",http://arxiv.org/abs/1701.00594v1,arXiv
Information Technology,Network and Infrastructure,Theoretical / Conceptual,Uncertainty-Aware Resource Provisioning for Network Slicing,"Network slicing allows Mobile Network Operators to split the physical
infrastructure into isolated virtual networks (slices), managed by Service
Providers to accommodate customized services. The Service Function Chains
(SFCs) belonging to a slice are usually deployed on a best-effort premise:
nothing guarantees that network infrastructure resources will be sufficient to
support a varying number of users, each with uncertain requirements. Taking the
perspective of a network Infrastructure Provider (InP), this paper proposes a
resource provisioning approach for slices, robust to a partly unknown number of
users with random usage of the slice resources. The provisioning scheme aims to
maximize the total earnings of the InP, while providing a probabilistic
guarantee that the amount of provisioned network resources will meet the slice
requirements. Moreover, the proposed provisioning approach is performed so as
to limit its impact on low-priority background services, which may co-exist
with slices in the infrastructure network. A Mixed Integer Linear Programming
formulation of the slice resource provisioning problem is proposed. Optimal
joint and suboptimal sequential solutions are proposed. These solutions are
compared to a provisioning scheme that does not account for best-effort
services sharing the common infrastructure network.",http://arxiv.org/abs/2006.01104v1,arXiv
Information Technology,Network and Infrastructure,Theoretical / Conceptual,"ex uno pluria: The Service-Infrastructure Cycle, Ossification, and the
  Fragmentation of the Internet","In this article I will first argue that a Service-Infrastructure Cycle is
fundamental to networking evolution. Networks are built to accommodate certain
services at an expected scale. New applications and/or a significant increase
in scale require a rethinking of network mechanisms which results in new
deployments. Four decades-worth of iterations of this process have yielded the
Internet as we know it today, a common and shared global networking
infrastructure that delivers almost all services. I will further argue, using
brief historical case studies, that success of network mechanism deployments
often hinges on whether or not mechanism evolution follows the iterations of
this Cycle. Many have observed that this network, the Internet, has become
ossified and unable to change in response to new demands. In other words, after
decades of operation, the Service-Infrastructure Cycle has become stuck.
However, novel service requirements and scale increases continue to exert
significant pressure on this ossified infrastructure. The result, I will
conjecture, will be a fragmentation, the beginnings of which are evident today,
that will ultimately fundamentally change the character of the network
infrastructure. By ushering in a ManyNets world, this fragmentation will
lubricate the Service-Infrastructure Cycle so that it can continue to govern
the evolution of networking. I conclude this article with a brief discussion of
the possible implications of this emerging ManyNets world on networking
research.",http://arxiv.org/abs/1712.04379v1,arXiv
Information Technology,Network and Infrastructure,Theoretical / Conceptual,"A Taxonomy for Blockchain-based Decentralized Physical Infrastructure
  Networks (DePIN)","As digitalization and technological advancements continue to shape the
infrastructure landscape, the emergence of blockchain-based decentralized
physical infrastructure networks (DePINs) has gained prominence. However, a
systematic categorization of DePIN components and their interrelationships is
still missing. To address this gap, we conduct a literature review and analysis
of existing frameworks and derived a taxonomy of DePIN systems from a
conceptual architecture. Our taxonomy encompasses three key dimensions:
distributed ledger technology, cryptoeconomic design and physicial
infrastructure network. Within each dimension, we identify and define relevant
components and attributes, establishing a clear hierarchical structure.
Moreover, we illustrate the relationships and dependencies among the identified
components, highlighting the interplay between governance models, hardware
architectures, networking protocols, token mechanisms, and distributed ledger
technologies. This taxonomy provides a foundation for understanding and
classifying diverse DePIN networks, serving as a basis for future research and
facilitating knowledge exchange, fostering collaboration and standardization
within the emerging field of decentralized physical infrastructure networks.",http://arxiv.org/abs/2309.16707v2,arXiv
Information Technology,Network and Infrastructure,Theoretical / Conceptual,"Admission Control and Resource Reservation for Prioritized Slice
  Requests with Guaranteed SLA under Uncertainties","Network slicing has emerged as a key concept in 5G systems, allowing Mobile
Network Operators (MNOs) to build isolated logical networks (slices) on top of
shared infrastructure networks managed by Infrastructure Providers (InP).
Network slicing requires the assignment of infrastructure network resources to
virtual network components at slice activation time and the adjustment of
resources for slices under operation. Performing these operations just-in-time,
on a best-effort basis, comes with no guarantee on the availability of enough
infrastructure resources to meet slice requirements.
  This paper proposes a prioritized admission control mechanism for concurrent
slices based on an infrastructure resource reservation approach. The
reservation accounts for the dynamic nature of slice requests while being
robust to uncertainties in slice resource demands. Adopting the perspective of
an InP, reservation schemes are proposed that maximize the number of slices for
which infrastructure resources can be granted while minimizing the costs
charged to the MNOs. This requires the solution of a max-min optimization
problem with a non-linear cost function and non-linear constraints induced by
the robustness to uncertainties of demands and the limitation of the impact of
reservation on background services. The cost and the constraints are linearized
and several reduced-complexity strategies are proposed to solve the slice
admission control and resource reservation problem. Simulations show that the
proportion of admitted slices of different priority levels can be adjusted by a
differentiated selection of the delay between the reception and the processing
instants of a slice resource request.",http://arxiv.org/abs/2203.09367v1,arXiv
Information Technology,Cybersecurity,Quantitative,OVERVIEW OF CYBERSECURITY METHODS AND STRATEGIES USING ARTIFICIAL INTELLIGENCE,"<jats:p>In today’s world, information technology is rapidly evolving, leading to an increase in both the number and complexity of cyber threats, including phishing, malware, and social engineering attacks. The growth in the quantity and sophistication of cyber threats creates an urgent need to improve methods for protecting information systems. Artificial Intelligence (AI), particularly machine learning and deep learning technologies, shows significant potential in enhancing cybersecurity. This article is dedicated to reviewing contemporary AI-based cybersecurity methods and strategies, as well as evaluating their effectiveness in detecting and countering cyber threats. The paper analyzes recent research by both domestic and international scientists, emphasizing AI’s ability to analyze large volumes of data, uncover hidden patterns, predict potential threats, and automate incident response processes. It highlights key research directions, including anomaly detection, threat modeling, incident response automation, and ensuring the interpretability of decisions made by AI systems. Special attention is given to the integration of AI into existing cybersecurity systems and its capacity to adapt to new threats. The article also discusses the main challenges and prospects of applying AI in cybersecurity, including ethical and legal aspects such as privacy issues, decision transparency, and accountability for actions taken based on AI system decisions. Recent statistical data indicate a rapid growth in the market for AI-based cybersecurity tools, underscoring the importance and relevance of this topic in contemporary conditions. The analysis results confirm that using AI allows for automating monitoring, threat detection, and response processes, reducing incident response time and enhancing the overall protection level of information systems. At the same time, implementing AI in cybersecurity faces several challenges, such as ensuring the transparency of AI decisions and protecting against potential threats created using the same technologies. Research in this field promotes strategic development and innovation in cybersecurity, providing researchers and professionals with new tools and methods for ensuring information system security. Thus, given the rapid growth and evolution of cyber threats, studying the role of AI in cybersecurity is extremely relevant and important. It not only enhances protection efficiency but also fosters the development of new strategies and technologies to counter threats in the digital age.</jats:p>",https://doi.org/10.28925/2663-4023.2024.25.379389,CrossRef
Information Technology,Cybersecurity,Quantitative,The THE GENDER EQUALITY IN CYBERSECURITY,"<jats:p>The article examines gender aspects in the domestic cybersecurity sector. The insufficient level of representation of women in this field is caused by a small number of girls and women who chose this field of activity for training and professional realization due to insufficient awareness of it, various stereotypes and prejudices. However, the ongoing full-scale war in Ukraine, its hybrid nature, the spread of gender misinformation, the demographic crisis, and the shortage of labor in the labor market that are taking place in the country require the expansion of women's participation in combating cybercrime and strengthening the cyber resilience of the state. The study conducted by the authors is based on an analysis of current international and national legislation on gender equality in the country and, in particular, in the defense and security sector, as well as modern publications by foreign and Ukrainian scientists and researchers. Quantitative analysis of gender aspects in the field of cybersecurity, in particular, in matters of employment and education, was carried out based on data from the State Statistics Service of Ukraine.

The results of the study showed that it was the martial law in the country and the mobilization of men that forced women to replace them in a number of professions. The representation of women in the security and defense sector has increased significantly, which has a positive effect on the overall level of security in society. The number of women choosing professions in the field of cybersecurity is gradually increasing, which gives hope for their further employment there. In order to promote gender equality in cybersecurity, expand the professional realization of women in this field, the authors recommend that employers use the existing high educational potential of women and girls more diversely and inclusively, their ability to quickly learn and adapt to changes, the availability of digital skills and the level of access to the Internet. To overcome obstacles in future professional realization, form more skills that are practical in female students, hone the ability to think outside the box, and teach them to work with people.</jats:p>",https://doi.org/10.28925/2663-4023.2025.27.742,CrossRef
Information Technology,Cybersecurity,Quantitative,Valet attack on privacy: a cybersecurity threat in automotive Bluetooth infotainment systems,"<jats:title>Abstract</jats:title><jats:p>Modern automobiles are equipped with connectivity features to enhance the user’s comfort. Bluetooth is one such communication technology that is used to pair a personal device with an automotive infotainment unit. Upon pairing, the user could access the personal information on the phone through the automotive head unit with minimum distraction while driving. However, such connectivity introduces a possibility for privacy attacks. Hence, performing an in-depth analysis of the system with privacy constraints is extremely important to prevent unauthorized access to personal information. In this work, we perform a systematic analysis of the Bluetooth network of an automotive infotainment unit to exploit security and privacy-related vulnerabilities. We model the identified threat with respect to privacy constraints of the system, emphasize the severity of attacks through a standardized rating metric and then provide potential countermeasures to prevent the attack. We perform System Theoretic Process Analysis for Privacy as a part of the systematic analysis and use the Common Vulnerability Scoring System to derive attack severity. The identified vulnerabilities are due to design flaws and assumptions on Bluetooth protocol implementation on automotive infotainment systems. We then elicit the vulnerability by performing a privacy attack on the Automotive system in an actual vehicle. We use Android Open-Source Project to report our findings and propose defense strategies.</jats:p>",https://doi.org/10.1186/s42400-022-00132-x,CrossRef
Information Technology,Cybersecurity,Quantitative,Cybersecurity Defence Mechanism Against DDoS Attack with Explainability,"<jats:p>Application-layer attacks (Layer 7 attacks), a form of distributed denial-of-service (DDoS) aimed at web servers, have become a significant concern in cybersecurity because of their ability to disrupt services by overwhelming server resources. This study focuses on addressing the challenges of detecting and mitigating the impact of such attacks, which are difficult to counter due to their sophisticated nature. The primary objective of this study is to develop an effective monitoring and defence model to detect, defend, and respond to these attacks efficiently. To achieve this, SHapley Additive exPlanations (SHAP) technology was used to understand the behaviour of the model and to increase the efficiency of the detection classifiers. The defence model is designed with three states: normal, observing, and suspicious. The observing mode, which represents the detection part, is triggered when the server load exceeds a predefined threshold. The detection system incorporates five machine learning (ML) algorithms: decision trees (DTs), support vector machines (SVMs), logistic regression (LR), naive Bayes (NB), and K-nearest neighbours (KNNs). A stacked classifier (SC) was then employed to combine these models to achieve optimal performance. The algorithms were evaluated in terms of accuracy (ACC), precision (PRC), recall (REC), F1 score (F1), and time (T). The SC demonstrates superior accuracy in distinguishing between legitimate traffic and malicious traffic. If the server continues to suffer from overload, the suspicious part of the defence model will be activated, and the mitigation algorithm will be called, which, in turn, bans users responsible for the attack and prevents illegitimate users from connecting to the server. The effects of the mitigation algorithm were noticeable in the server traffic rate, transmission rate, memory utilization, and CPU utilization, confirming its ability to defend against application-layer attacks.
 </jats:p>",https://doi.org/10.58496/mjcs/2024/027,CrossRef
Information Technology,Cybersecurity,Quantitative,Enhanced detection of obfuscated malware in memory dumps: a machine learning approach for advanced cybersecurity,"<jats:title>Abstract</jats:title><jats:p>In the realm of cybersecurity, the detection and analysis of obfuscated malware remain a critical challenge, especially in the context of memory dumps. This research paper presents a novel machine learning-based framework designed to enhance the detection and analytical capabilities against such elusive threats for binary and multi type’s malware. Our approach leverages a comprehensive dataset comprising benign and malicious memory dumps, encompassing a wide array of obfuscated malware types including Spyware, Ransomware, and Trojan Horses with their sub-categories. We begin by employing rigorous data preprocessing methods, including the normalization of memory dumps and encoding of categorical data. To tackle the issue of class imbalance, a Synthetic Minority Over-sampling Technique is utilized, ensuring a balanced representation of various malware types. Feature selection is meticulously conducted through Chi-Square tests, mutual information, and correlation analyses, refining the model’s focus on the most indicative attributes of obfuscated malware. The heart of our framework lies in the deployment of an Ensemble-based Classifier, chosen for its robustness and effectiveness in handling complex data structures. The model’s performance is rigorously evaluated using a suite of metrics, including accuracy, precision, recall, F1-score, and the area under the ROC curve (AUC) with other evaluation metrics to assess the model’s efficiency. The proposed model demonstrates a detection accuracy exceeding 99% across all cases, surpassing the performance of all existing models in the realm of malware detection.</jats:p>",https://doi.org/10.1186/s42400-024-00205-z,CrossRef
Information Technology,Cybersecurity,Qualitative,Exploiting Human Trust in Cybersecurity: Which Trust Development Process Is Predominant in Phishing Attacks?,"<jats:p>Humans live in an interconnected world that is increasingly featured with virtual interactions in cyberspace. That world has raised cybersecurity concerns, particularly regarding the exploitation of human trust through various means, such as phishing. Phishing remains one of the most prevalent forms of cybercrime. It exploits human trust to manipulate individuals into divulging sensitive information. This study investigates the trust development mechanisms most exploited by cybercriminals in phishing attacks. It focuses on two primary trust development processes: relationship history and future expectations. The study uses qualitative content analysis of 42 phishing messages collected from diverse secondary sources. The findings reveal that future expectations—such as promises of rewards, urgent requests, or threats of penalties—dominate phishing tactics. In contrast, relationship history mechanisms exploit existing or fabricated relationships to evoke trust and compliance. These findings provide critical insights into the psychological manipulations leveraged in phishing schemes and highlight the need to integrate behavioural and cognitive principles into cybersecurity education. Practical implications include tailored training programs for distinct user groups, such as seniors, employees, and students. The training should emphasise recognising urgency cues, emotional manipulation, and verification strategies.</jats:p>",https://doi.org/10.60097/acig/199452,CrossRef
Information Technology,Cybersecurity,Qualitative,Criminological Features of the Cybersecurity Threats,"<jats:p>[Purpose] Currently, novel tools have converted many traditional phenomena into cyber ones. The absence of a standardized terminology and classification of cybersecurity threats has raised significant concerns among researchers and lawmakers. Ignoring the emerging risks that necessitate appropriate responses is impracticable. Prior to devising countermeasures to combat cybercrime, it is imperative to accurately define the concept of cybersecurity threat and differentiate it from other related notions such as information security, computer security, cyberattack, cyberspace attack, cyber incident, cybersecurity incident, cyber threat, and cybersecurity event, whose definitions may be ascertained from the glossaries of various standardization institutes.
[Methodology/Approach/Design] This study presents a descriptive investigation of cybersecurity threats and their causes, utilizing genetic, systematic-functional, and systematization methods. Cyberattacks are identified as the primary threat, and data is represented through qualitative research and summarized in tables. The study also considers the historical background of concepts and cyber-criminality.
[Findings] The present study delves into a comprehensive analysis of distinct categories of cybersecurity threats, the trajectory of cybercrime, and the factors that underpin the emergence of new cybersecurity threats. The research scrutinizes both the general causes for cyber-criminality and the specific determinants for criminal activities that target the energy sector, a critical component of a state's infrastructure. The study reveals that the major sources of threats comprise terrorists, insiders (i.e., disgruntled employees), commercial spies, and black hackers or crackers, whose malicious acts are themselves considered threats to cybersecurity.</jats:p>",https://doi.org/10.26512/lstr.v15i2.45997,CrossRef
Information Technology,Cybersecurity,Qualitative,An Analysis of Cybersecurity Policy Compliance in Organisations,"<jats:p>In the contemporary digital landscape, cyber-attacks and incidents have placed cyber-security at the forefront of priorities in organisations. As organisations face cyber risks, it becomes imperative to implement and comply with various cyber-security policies. However, due to factors such as policy complexity and resistance from employees, compliance can be a challenging task. The study, which took a comprehensive approach, investigated the variables that affect an organisation's adherence to cyber-security policies. The findings of this study provide insights into the challenges and factors influencing compliance with cyber-security policies in organisations. A case study design was chosen as part of a qualitative approach to answer the research question. For data gathering, semi-structured interviews were performed, and existing documents were also considered when available to supplement interviews. The gathered data was meticulously organised, coded, and analysed using the Actor-Network Theory perspective, with a focus on its four moments of translation: problematisation, interessement, enrolment, and mobilisation. The analysis revealed that insider threats and phishing attempts are the two cyber threats that affect organisations; behavioural challenges and enforcement limitations are factors that influence and contribute to the non-compliance of cyber-security policy; phishing exercises and policy development processes are used to enforce cyber-security policies.</jats:p>",https://doi.org/10.60097/acig/191942,CrossRef
Information Technology,Cybersecurity,Qualitative,Digital Divide in Cybersecurity,"<jats:p>The digital gap actively determines how people obtain varying degrees of access to technological resources, educational opportunities, and job opportunities, specifically within cybersecurity. Research explores digital divide patterns in cybersecurity populations by analyzing unequal opportunities related to cybersecurity education, job opportunities, and security protection levels. This study combines demographic analytics and interview data to reveal how socioeconomic standing, racial background, gender identities, and regional locations simultaneously shape cybersecurity accessibility and workforce take-up. The data reveals systemic obstacles between low-income citizens and minority groups while closing access to awareness regarding entry into cybersecurity roles. The study ends by suggesting policy and educational outreach reforms that might eliminate digital inequalities.</jats:p>",https://doi.org/10.53889/citj.v3i1.622,CrossRef
Information Technology,Cybersecurity,Qualitative,Cybersecurity service level agreements: understanding government data confidentiality requirements,"<jats:title>Abstract</jats:title><jats:p>Cybersecurity requirements, such as data security, are often used as evidence for the Government's relationship with external service providers to process, store and transmit sensitive government data. However, cybersecurity researchers have not profoundly studied the practical application of government data security requirements (e.g. data confidentiality) in service level agreements (SLAs) in the context of an outsourced scenario. The relationships with external service providers are usually established through SLAs as trust-enhancing instruments. However, there is a concern that existing SLAs mainly focus on the system availability and performance aspects but overlook cybersecurity requirements (e.g. data security) in SLAs. Such an understanding is essential to develop government SLA data confidentiality requirements into the formulation of security-related SLAs. We seek to provide insights by developing and conducting a grounded adaptive Delphi method (GADM) with 35 government participants through group discussions and individual sessions. The work on the Indonesian Government's data confidentiality requirements was used as a case study. This paper provides insights into three understandings of the increasing considerations of the Government's data confidentiality requirements in SLA definitions. The three perceptions of security-related SLAs are the target of protection, the data confidentiality risks and the government SLA data confidentiality requirements. Our findings play important implications for a better understanding of how to incorporate data confidentiality requirements according to perceived threats for government data classification in security-SLAs. Based on these findings, we recommend that the Government and service providers improve existing security-related SLAs and future research lines.</jats:p>",https://doi.org/10.1093/cybsec/tyac004,CrossRef
Information Technology,Cybersecurity,Mixed Methods,"Strengthening Cybersecurity Resilience in Agriculture Through
  Educational Interventions: A Case Study of the Ponca Tribe of Nebraska","The increasing digitization of agricultural operations has introduced new
cybersecurity challenges for the farming community. This paper introduces an
educational intervention called Cybersecurity Improvement Initiative for
Agriculture (CIIA), which aims to strengthen cybersecurity awareness and
resilience among farmers and food producers. Using a case study that focuses on
farmers from the Ponca Tribe of Nebraska, the research evaluates pre- and post-
intervention survey data to assess participants' cybersecurity knowledge and
awareness before and after exposure to the CIIA. The findings reveal a
substantial baseline deficiency in cybersecurity education among participants,
however, post-intervention assessments demonstrate improvements in the
comprehension of cybersecurity concepts, such as password hygiene, multi-factor
authentication, and the necessity of routine data backups. These initial
findings highlight the need for a continued and sustained, community-specific
cybersecurity education effort to help mitigate emerging cyber threats in the
agricultural sector.",http://arxiv.org/abs/2505.23800v1,arXiv
Information Technology,Cybersecurity,Mixed Methods,Cybersecurity Entity Alignment via Masked Graph Attention Networks,"Cybersecurity vulnerability information is often recorded by multiple
channels, including government vulnerability repositories,
individual-maintained vulnerability-gathering platforms, or
vulnerability-disclosure email lists and forums. Integrating vulnerability
information from different channels enables comprehensive threat assessment and
quick deployment to various security mechanisms. Efforts to automatically
gather such information, however, are impeded by the limitations of today's
entity alignment techniques. In our study, we annotate the first
cybersecurity-domain entity alignment dataset and reveal the unique
characteristics of security entities. Based on these observations, we propose
the first cybersecurity entity alignment model, CEAM, which equips GNN-based
entity alignment with two mechanisms: asymmetric masked aggregation and
partitioned attention. Experimental results on cybersecurity-domain entity
alignment datasets demonstrate that CEAM significantly outperforms
state-of-the-art entity alignment methods.",http://arxiv.org/abs/2207.01434v1,arXiv
Information Technology,Cybersecurity,Mixed Methods,"Creating a Cybersecurity Concept Inventory: A Status Report on the CATS
  Project","We report on the status of our Cybersecurity Assessment Tools (CATS) project
that is creating and validating a concept inventory for cybersecurity, which
assesses the quality of instruction of any first course in cybersecurity. In
fall 2014, we carried out a Delphi process that identified core concepts of
cybersecurity. In spring 2016, we interviewed twenty-six students to uncover
their understandings and misconceptions about these concepts. In fall 2016, we
generated our first assessment tool--a draft Cybersecurity Concept Inventory
(CCI), comprising approximately thirty multiple-choice questions. Each question
targets a concept; incorrect answers are based on observed misconceptions from
the interviews. This year we are validating the draft CCI using cognitive
interviews, expert reviews, and psychometric testing. In this paper, we
highlight our progress to date in developing the CCI.
  The CATS project provides infrastructure for a rigorous evidence-based
improvement of cybersecurity education. The CCI permits comparisons of
different instructional methods by assessing how well students learned the core
concepts of the field (especially adversarial thinking), where instructional
methods refer to how material is taught (e.g., lab-based, case-studies,
collaborative, competitions, gaming). Specifically, the CCI is a tool that will
enable researchers to scientifically quantify and measure the effect of their
approaches to, and interventions in, cybersecurity education.",http://arxiv.org/abs/1706.05092v1,arXiv
Information Technology,Cybersecurity,Mixed Methods,"Development of a threat modelling framework and a web-based threat
  modelling tool for micro businesses","While there is a plethora of cybersecurity and risk management frameworks for
different target audiences and use cases, micro-businesses (MBs) are often
overlooked. As the smallest business entities, MBs represent a special case
with regard to cybersecurity for two reasons: (1) Having fewer than 10
employees, they tend to lack cybersecurity expertise. (2) Because of their low
turnover, they usually have a limited budget for cybersecurity. As a result,
MBs are often the victims of security breaches and cyber-attacks every year, as
demonstrated by various studies. This calls for a non-technical, simple
solution tailored specifically for MBs. To address this pressing need, the
SEANCE Cybersecurity Framework was developed through a 7-step methodology: (1)
A literature review was conducted to explore the current state of research and
available frameworks and methodologies, (2) followed by a qualitative survey to
identify the cybersecurity challenges faced by MBs. (3) After analyzing the
results of the literature review and the survey, (4) the relevant aspects of
existing frameworks and tools for MBs were identified and (5) a non-technical
framework was developed. (6) A web-based tool was developed to facilitate the
implementation of the framework and (7) another qualitative survey was
conducted to gather feedback. The SEANCE Framework suggests considering
possible vulnerabilities and cyber threats in six hierarchical layers: (1)
Self, (2) Employees, (3) Assets, (4) Network, (5) Customers and (6)
Environment, with the underlying idea of a vulnerability in an inner layer
propagates to the outer layers and therefore needs to be prioritized.",http://arxiv.org/abs/2411.14450v1,arXiv
Information Technology,Cybersecurity,Mixed Methods,Frontier AI's Impact on the Cybersecurity Landscape,"As frontier AI advances rapidly, understanding its impact on cybersecurity
and inherent risks is essential to ensuring safe AI evolution (e.g., guiding
risk mitigation and informing policymakers). While some studies review AI
applications in cybersecurity, none of them comprehensively discuss AI's future
impacts or provide concrete recommendations for navigating its safe and secure
usage. This paper presents an in-depth analysis of frontier AI's impact on
cybersecurity and establishes a systematic framework for risk assessment and
mitigation. To this end, we first define and categorize the marginal risks of
frontier AI in cybersecurity and then systemically analyze the current and
future impacts of frontier AI in cybersecurity, qualitatively and
quantitatively. We also discuss why frontier AI likely benefits attackers more
than defenders in the short term from equivalence classes, asymmetry, and
economic impact. Next, we explore frontier AI's impact on future software
system development, including enabling complex hybrid systems while introducing
new risks. Based on our findings, we provide security recommendations,
including constructing fine-grained benchmarks for risk assessment, designing
AI agents for defenses, building security mechanisms and provable defenses for
hybrid systems, enhancing pre-deployment security testing and transparency, and
strengthening defenses for users. Finally, we present long-term research
questions essential for understanding AI's future impacts and unleashing its
defensive capabilities.",http://arxiv.org/abs/2504.05408v2,arXiv
Information Technology,Cybersecurity,Design and Development,OSINT TECHNOLOGIES AS A THREAT TO STATE CYBERSECURITY,"<jats:p>The article examines OSINT technologies as one of the key challenges for the national security of Ukraine. With the development of the digital space, methods of collecting, analyzing and using information from open sources have become widespread, in particular in the field of cybersecurity. The authors emphasize that, despite the legitimacy and general availability of OSINT technologies, they can be used by attackers to collect personal data, identify vulnerabilities in critical infrastructure and plan cyberattacks. The study is based on the analysis of open state registers, social networks, satellite images, mapping services and other sources containing potentially sensitive information. The main threats associated with OSINT are identified, including: theft of personal data, phishing, analysis of corporate information, unauthorized interference in state information systems. Special attention is paid to the analysis of real incidents of information leakage through OSINT technologies, including examples from international and Ukrainian practice. The authors have examined the legal and ethical aspects of the use of OSINT tools, in particular the conflict between the need for openness of state data and cybersecurity threats. The article provides an analysis of the legislative norms of Ukraine that regulate access to information, and also considers international experience in the field of countering OSINT threats. A separate section of the article is devoted to methods of countering threats that arise through the use of OSINT tools. Counter-OSINT methods are also studied, which involve minimizing the digital trace, disinformation, data anonymization, control over information leaks, and the use of means of countering OSINT analysis of images and videos. The role of state structures in the development and implementation of regulatory legal acts aimed at strengthening the protection of information in open sources is separately considered.</jats:p>",https://doi.org/10.28925/2663-4023.2025.27.749,CrossRef
Information Technology,Cybersecurity,Design and Development,CYBERSECURITY OF MQTT CONNECTIONS IN AN AUTOMATED GATE CONTROL SYSTEM,"<jats:p>With the development of the Internet of Things (IoT), the issue of data protection and the secure operation of IoT systems has become increasingly important. One of the major threats is unprotected MQTT connections, which are vulnerable to traffic interception (MitM attacks), command spoofing, unauthorized access, and DDoS attacks.This paper explores MQTT security methods using the example of an automated gate control system. It presents an analysis of recent research in IoT cybersecurity, identifies the main vulnerabilities of MQTT brokers and clients, and proposes measures to secure IoT infrastructure.Special attention is given to TLS/SSL encryption for traffic protection, MQTT client authentication, access restrictions using ACL (Access Control List), and the isolation of IoT devices in separate networks (VPN/VLAN).The research findings confirm that a comprehensive implementation of security measures significantly reduces attack risks and ensures the reliable and secure operation of IoT projects.</jats:p>",https://doi.org/10.28925/2663-4023.2025.27.727,CrossRef
Information Technology,Cybersecurity,Design and Development,METHOD OF COMPREHENSIVE CYBERSECURITY RISKS ASSESSMENT IN DISTRIBUTED INFORMATION SYSTEMS,"<jats:p>Cybersecurity risk assessment and analysis is an important element for building an effective information security management system.  The high complexity and scalability of the architecture of modern distributed systems, the heterogeneity of equipment and infrastructure, as well as constant changes in the configuration and scaling of the environment give rise to a number of problems related to the collection and analysis of information for risk assessment, the need for operational processing of large arrays of complex in structure and heterogeneous in nature data coming from differentiated security and monitoring systems, event logs, audit reports and other sources, as well as the lack of a single format for their presentation. The limitations of existing standards and methodologies in the dynamic conditions of modern DIS, their conceptual nature and the complexity of practical implementation and application require the development of flexible methodological and technological solutions for cyber risk analysis that would integrate the advantages of existing approaches, provide automation of calculations and take into account the dynamic aspects of distributed environment. The article presents a comprehensive adaptive method for quantitative assessment of cybersecurity risks in distributed information systems, which is relevant in dynamic conditions of complex multi-component and scalable DIS. The proposed method, integrating a metric-oriented approach based on the results of a complex of neural network models for assessing DIS infrastructure security indicators and compliance metrics for regulatory frameworks and leading standards, provides an opportunity to create a scalable and dynamic cyber risk management system that effectively responds to modern threats in DIS and open opportunities for the comprehensive implementation of intelligent information security management systems in risk management processes.</jats:p>",https://doi.org/10.28925/2663-4023.2024.26.731,CrossRef
Information Technology,Cybersecurity,Design and Development,IPatch: a remote adversarial patch,"<jats:title>Abstract</jats:title><jats:p>Applications such as autonomous vehicles and medical screening use deep learning models to localize and identify hundreds of objects in a single frame. In the past, it has been shown how an attacker can fool these models by placing an adversarial patch within a scene. However, these patches must be placed in the target location and do not explicitly alter the semantics elsewhere in the image. In this paper, we introduce a new type of adversarial patch which alters a model’s perception of an image’s semantics. These patches can be placed anywhere within an image to change the classification or semantics of locations far from the patch. We call this new class of adversarial examples ‘remote adversarial patches’ (RAP). We implement our own RAP called IPatch and perform an in-depth analysis on without pixel clipping on image segmentation RAP attacks using five state-of-the-art architectures with eight different encoders on the CamVid street view dataset. Moreover, we demonstrate that the attack can be extended to object recognition models with preliminary results on the popular YOLOv3 model. We found that the patch can change the classification of a remote target region with a success rate of up to 93% on average.</jats:p>",https://doi.org/10.1186/s42400-023-00145-0,CrossRef
Information Technology,Cybersecurity,Design and Development,GRAPH-BASED ANALYSIS OF INFORMATION FLOWS IN TELEGRAM FOR CYBERSECURITY THREAT DETECTION,"<jats:p>This paper explores modern methods for analyzing information flows in messengers, emphasizing their role in cybersecurity. The study compares different approaches, including API-based data collection, the use of graph and relational databases, and the automation of open data gathering. Special attention is given to the theoretical foundations of information flow analysis, focusing on the social graph concept and its application in modeling the dissemination of information across networks. The advantages of graph databases for detecting, visualizing, and analyzing networks of information distribution are examined, highlighting their effectiveness in uncovering hidden connections between channels. A prototype system for automating open data collection has been developed, integrating methods for extracting, processing, and structuring information from messenger platforms. The proposed system employs a combination of graph-based and relational techniques to enhance the accuracy and efficiency of detecting interconnections between communication channels. A series of computational experiments has been conducted to validate the effectiveness of the developed algorithms and software prototypes. The results confirm that combining these methods significantly improves the ability to identify information threats, including disinformation campaigns, automated bot activity, and coordinated attacks within messenger ecosystems. Actionable recommendations for the practical implementation of these approaches in cybersecurity tasks are provided. Specifically, they outline strategies for improving the monitoring and detection of malicious information activities, optimizing data collection and analysis pipelines, and leveraging graph-based insights to enhance situational awareness in digital communication environments. These findings contribute to the ongoing development of advanced cybersecurity solutions aimed at mitigating risks associated with modern information warfare.</jats:p>",https://doi.org/10.28925/2663-4023.2025.27.746,CrossRef
Information Technology,Cybersecurity,Theoretical / Conceptual,CYBERSECURITY RISK ASSESSMENT FOR SELECTING A CLOUD SERVICE PROVIDER,"<jats:p>This paper presents the development of a cybersecurity risk assessment module for selecting a cloud service provider, enabling organizations to make informed decisions based on all aspects of security. The module is designed as part of an integrated decision support system (DSS) and utilizes a detailed taxonomy of cloud services, covering various models and deployment options (IaaS, PaaS, SaaS, public, private, and hybrid clouds). The system performs security assessments based on collected vulnerability data, including information from the National Vulnerability Database (NVD) and other sources.
One of the key stages of the assessment is determining the risks associated with each service, which allows for the accurate identification of potential threats and the selection of a provider with the best security performance. The module evaluates various factors, including the frequency and severity of vulnerabilities, the likelihood of exploitation by attackers, and the speed of vulnerability remediation. The collected data is used to form a weighted risk assessment matrix that aids decision-making based on specific criteria.
The results of the study show that the developed module can significantly improve the cloud service provider selection process, particularly for large organizations with high data security requirements. Future research will focus on integrating this module into automated decision support systems, which will allow the selection process to be adapted to the rapidly changing conditions of cloud technologies and emerging threats.</jats:p>",https://doi.org/10.28925/2663-4023.2025.27.773,CrossRef
Information Technology,Cybersecurity,Theoretical / Conceptual,"The (Il)legitimacy of
Cybersecurity. An Application
of Just Securitization Theory
to Cybersecurity based on
the Principle of Subsidiarity","<jats:p>This contribution applies Floyd’s just securitization theory (JST) to cybersecurity and develops an own version of JST focused on subsidiarity from the critical discussion of this application. Floyd’s JST pursues a subsidiary approach. It emphasizes that securitization is only legitimate if it has “a reasonable chance of success” to avert threats to the satisfaction of “basic human needs”. From this perspective, cyber-securitization would be only legitimate if it protects critical infrastructure, which is a too narrow scope. Whilst Floyd’s JST focuses exclusively on permissibility and needs instead of rights, I argue that there are cases in which states’ compliance with Human Rights requires the guarantee of cybersecurity, most importantly regarding the human right to privacy. Furthermore, my version of JST strengthens the principle of subsidiarity, in the sense that stakeholders directly affected by a threat should participate in securitization. In order to strengthen a kind of subsidiarity focused on the private sector, I argue for the legitimacy of private active self-defence in cyberspace and emphasize the importance of a ‘whole-of-society approach’ involving digital literacy and ‘everyday security practices’. In contrast to that, I argue that far-reaching securitization on the nation-state level can be connected to hyper-securitization. I argue that the securitization of the digital public sphere following unclear notions such as ‘digital sovereignty’ creates a ‘cybersecurity dilemma’ and a ‘societal security dilemma’. Furthermore, I argue that this kind of counterproductive hyper-securitization involves an ‘invisible handshake’ between Big Tech and governments that should be counteracted by structural desecuritization focused on subsidiarity.</jats:p>",https://doi.org/10.5604/01.3001.0016.1093,CrossRef
Information Technology,Cybersecurity,Theoretical / Conceptual,FUNCTIONS OF THE INFORMATION SECURITY AND CYBERSECURITY SYSTEM OF CRITICAL INFORMATION INFRASTRUCTURE,"<jats:p>The subject of research in the scientific article is the system of Information Protection and cybersecurity of critical information infrastructure objects. An information security and cybersecurity system is a complex set of software, cryptographic, organizational, and other tools, methods, and measures designed to protect information and cybersecurity. Since the system of Information Protection and cybersecurity of critical information infrastructure facilities is relatively new, there is no single view on what functions this system should perform. As a result, the process of its formation and formation as a system continues. There was a need to define functions for further evaluation of the effectiveness of its functioning as a system. Evaluation is supposed to be carried out both in the process of creation, acceptance, and daily operation. Partial performance indicators are required to implement the procedure for evaluating the effectiveness of the information security system and cybersecurity of critical information infrastructure facilities. Using these indicators, it is possible to characterize the degree of achievement of the system's tasks assigned to it. The following performance indicators are proposed according to the functions: ID identification of cybersecurity risks; PR Cyber Defense; DE detection of cyber incidents; RS response to cyber incidents; RC restoration of the state of cybersecurity. The scientific novelty of the obtained result lies in the fact that Universal functions are proposed that the information security and cybersecurity system should implement at critical information infrastructure facilities. The presented study does not exhaust all aspects of this problem. The theoretical results obtained in the course of scientific research form the basis for further justification of indicators and criteria for evaluating the effectiveness of the information security and cybersecurity system.</jats:p>",https://doi.org/10.28925/2663-4023.2022.15.1241341,CrossRef
Information Technology,Cybersecurity,Theoretical / Conceptual,BLOCKCHAIN AS A COMPONENT OF INFORMATION SECURITY,"<jats:p>The article describes the use of information and telecommunication systems in public and private institutions and disadvantages for the construction of information and telecommunication systems for decentralization. The analysis of recent researches and publications on the subject of the block is conducted. The paper describes the principle of the technology, the block and the ways in which a block protects itself from attempting to make unauthorized changes or deletion of data. The expediency and perspectives of using information security technologies from the point of view of the triad of information security services as confidentiality, integrity and accessibility are considered. The rapid development of information technology is expected to rapidly increase and increase, and also threatens the information and telecommunication systems that have most of these systems. A promising direction for the construction of information and telecommunication systems is the use of decentralization. Therefore, it is important to analyze the use of Blockchain technology for the construction of decentralized information and telecommunication systems in terms of information security.</jats:p>",https://doi.org/10.28925/2663-4023.2019.4.8589,CrossRef
Information Technology,Cybersecurity,Theoretical / Conceptual,"Cybersecurity for Sustainable Smart Healthcare: State of the Art, Taxonomy, Mechanisms, and Essential Roles","<jats:p>Cutting-edge technologies have been widely employed in healthcare delivery, resulting in transformative advances and promising enhanced patient care, operational efficiency, and resource usage. However, the proliferation of networked devices and data-driven systems has created new cybersecurity threats that jeopardize the integrity, confidentiality, and availability of critical healthcare data. This review paper offers a comprehensive evaluation of the current state of cybersecurity in the context of smart healthcare, presenting a structured taxonomy of its existing cyber threats, mechanisms and essential roles. This study explored cybersecurity and smart healthcare systems (SHSs). It identified and discussed the most pressing cyber threats and attacks that SHSs face, including fake base stations, medjacking, and Sybil attacks. This study examined the security measures deployed to combat cyber threats and attacks in SHSs. These measures include cryptographic-based techniques, digital watermarking, digital steganography, and many others. Patient data protection, the prevention of data breaches, and the maintenance of SHS integrity and availability are some of the roles of cybersecurity in ensuring sustainable smart healthcare. The long-term viability of smart healthcare depends on the constant assessment of cyber risks that harm healthcare providers, patients, and professionals. This review aims to inform policymakers, healthcare practitioners, and technology stakeholders about the critical imperatives and best practices for fostering a secure and resilient smart healthcare ecosystem by synthesizing insights from multidisciplinary perspectives, such as cybersecurity, healthcare management, and sustainability research. Understanding the most recent cybersecurity measures is critical for controlling escalating cyber threats and attacks on SHSs and networks and encouraging intelligent healthcare delivery.</jats:p>",https://doi.org/10.58496/mjcs/2024/006,CrossRef
Information Technology,IT Service Management,Quantitative,"Resource Management and Quality of Service Provisioning in 5G Cellular
  Networks","With the commercial launch of 5G technologies and fast pace of expansion of
cellular network infrastructure, it is expected that cellular and mobile
networks traffic will exponentially increase. In addition, new services are
expected to spread widely, such as the Internet of Things connected to mobile
networks. This will add additional burden in terms of traffic load. As a
result, some studies suggest that mobile traffic may increase more than 1000
times compared to the amount of traffic that is generated nowadays. This means
that network resources for mobile services must be managed and controlled in a
smart way, because resources are always limited, but the demand for services
and the need for keeping user equipment always connected to mobile networks can
be considered unlimited, leaving gap between huge service demands and available
resources. In order to narrow this gap, major consideration should be given to
the management of network resources to avoid network congestion and performance
degradation during peak hour/s and traffic spikes, and allow access to network
services to more customers when demand is high. On the other hand, guaranteeing
quality of service requirements for the wide range of new services is another
challenge that must be met in 5G networks. In this paper we will review 5G
networks characteristics and specifications, then carry out a survey on
resource management and QoS provisioning to improve and manage resource
utilization in 5G networks.",http://arxiv.org/abs/2008.09601v1,arXiv
Information Technology,IT Service Management,Quantitative,"Mining Target-Oriented Fuzzy Correlation Rules to Optimize Telecom
  Service Management","To optimize telecom service management, it is necessary that information
about telecom services is highly related to the most popular telecom service.
To this end, we propose an algorithm for mining target-oriented fuzzy
correlation rules. In this paper, we show that by using the fuzzy statistics
analysis and the data mining technology, the target-oriented fuzzy correlation
rules can be obtained from a given database. We conduct an experiment by using
a sample database from a telecom service provider in Taiwan. Our work can be
used to assist the telecom service provider in providing the appropriate
services to the customers for better customer relationship management.",http://arxiv.org/abs/1103.0083v1,arXiv
Information Technology,IT Service Management,Quantitative,Change Impact Analysis Based Regression Testing of Web Services,"Reducing the effort required to make changes in web services is one of the
primary goals in web service projects maintenance and evolution. Normally,
functional and non-functional testing of a web service is performed by testing
the operations specified in its WSDL. The regression testing is performed by
identifying the changes made thereafter to the web service code and the WSDL.
In this thesis, we present a tool-supported approach to perform efficient
regression testing of web services. By representing a web service as a directed
graph of WSDL elements, we identify and gathers the changed portions of the
graph and use this information to reduce regression testing efforts.
Specifically, we identify, categorize, and capture the web service testing
needs in two different ways, namely, Operationalized Regression Testing of Web
Service (ORTWS) and Parameterized Regression Testing of Web Service (PRTWS).
Both of the approach can be combined to reduce the regression testing efforts
in the web service project. The proposed approach is prototyped as a tool,
named as Automatic Web Service Change Management (AWSCM), which helps in
selecting the relevant test cases to construct reduced test suite from the old
test suite. We present few case studies on different web service projects to
demonstrate the applicability of the proposed tool. The reduction in the effort
for regression testing of web service is also estimated.",http://arxiv.org/abs/1408.1600v1,arXiv
Information Technology,IT Service Management,Quantitative,Efficient Resource Management in Cloud Environment,"In cloud computing resource management plays a significant role in data
centres and it is directly dependent on the application workload. Various
services such as Infrastructure as a Service (IaaS), Platform as a Service
(PaaS), and Software as a Service (SaaS) are offered by cloud computing to
provide compute, network, and storage capabilities to the cloud users utilizing
the pay-per-usage approach. Resource allocation is a prior solution to address
various demanding situations like the under/overload handling, resource
wastage, load balancing, Quality-of-Services (QoS) violations, VM migration and
many more. The primary aim of Virtual Machine Placement (VMP) is mapping of
Virtual Machines (VMs) to physical machines (PMs), such that the PMs may be
utilized to their maximum efficiency, where the already active VMs are not to
be interrupted. It provides a list of live VM migrations that must be
accomplished to get the optimum solution and reduces energy consumption to a
larger extent. The inefficient VMP leads to wastage of resources, excessive
energy consumption and also increase overall operational cost of the data
center. On this context, this article provides an extensive survey of resource
management schemes in cloud environment. A conceptual scheme for resource
management, grouping of current machine learning based resource allocation
strategies, and fundamental problems of ineffective distribution of physical
resources are analyzed. Thereafter, a complete survey of existing techniques in
machine learning based mechanisms in the field of cloud resource management are
explained. Ultimately, the paper explores and concludes distinct approaching
challenges and future research guidelines associated to resource management in
cloud environment.",http://arxiv.org/abs/2207.12085v1,arXiv
Information Technology,IT Service Management,Quantitative,Managing Word-of-Mouth through Capacity Allocation and Advertisement,"Advancements in service sector and growing online platforms are intensifying
the information exchange between customers through (electronic) word-of-mouth
(WoM). The information obtained by WoM has shown to be a dominant factor in
customers' purchase decisions creating an endogenous demand structure. Service
providers can monitor how their service is perceived by consumers through
different methods, for example surveys. Although this requires an additional
effort, the understanding and integration of endogenous demand into operational
decisions offer great benefits. In this paper, we study a service system where
customers are sensitive to the on-demand access to the service. Customers form
a perception based on the information obtained by WoM communication and the
advertisement. Depending on the type of the service environment, service
capacity can be flexible or constant. We consider two types of service
providers: aware firm that has complete information on endogenous demand
structure, and na\""ive firm that has partial information. Our focus is to
understand the optimal advertisement and capacity decisions, and the value of
information on the underlying demand. For firms that have flexible service
capacity, we show that it is optimal to employ aggressive advertisement
strategies in the early stages. Myopic naive firms often misinterpret the
market conditions and cease operating, where they could in fact realize profit.
For the cases where the capacity is not flexible, it may not be possible to
avoid negative WoM. Therefore, the service provider is forced to keep the level
of advertisements more compatible with the actual quality of the service. This
prevents the firm from overcrowding the system through advertisement without
considering the service quality.",http://arxiv.org/abs/2201.04779v1,arXiv
Information Technology,IT Service Management,Qualitative,"Assessing the maturity of software testing services using CMMI-SVC: An
  industrial case study","Context: While many companies conduct their software testing activities
in-house, many other companies outsource their software testing needs to other
firms who act as software testing service providers. As a result, Testing as a
Service (TaaS) has emerged as a strong service industry in the last several
decades. In the context of software testing services, there could be various
challenges (e.g., during the planning and service delivery phases) and, as a
result, the quality of testing services is not always as expected. Objective:
It is important, for both providers and also customers of testing services, to
assess the quality and maturity of test services and subsequently improve them.
Method: Motivated by a real industrial need in the context of several testing
service providers, to assess the maturity of their software testing services,
we chose the existing CMMI for Services maturity model (CMMI-SVC), and
conducted a case study using it in the context of two Turkish testing service
providers. Results: The case-study results show that maturity appraisal of
testing services using CMMI-SVC was helpful for both companies and their test
management teams by enabling them objectively assess the maturity of their
testing services and also by pinpointing potential improvement areas.
Conclusion: We empirically observed that, after some minor customization,
CMMI-SVC is indeed a suitable model for maturity appraisal of testing services.",http://arxiv.org/abs/2005.12570v2,arXiv
Information Technology,IT Service Management,Qualitative,Slicing-Based AI Service Provisioning on Network Edge,"Edge intelligence leverages computing resources on network edge to provide
artificial intelligence (AI) services close to network users. As it enables
fast inference and distributed learning, edge intelligence is envisioned to be
an important component of 6G networks. In this article, we investigate AI
service provisioning for supporting edge intelligence. First, we present the
features and requirements of AI services. Then, we introduce AI service data
management, and customize network slicing for AI services. Specifically, we
propose a novel resource pooling method to jointly manage service data and
network resources for AI services. A trace-driven case study demonstrates the
effectiveness of the proposed resource pooling method. Through this study, we
illustrate the necessity, challenge, and potential of AI service provisioning
on network edge.",http://arxiv.org/abs/2105.07052v1,arXiv
Information Technology,IT Service Management,Qualitative,"An Approach for Selecting Cloud Service Adequate to Big Data Case Study:
  E-health Context","The expanding Cloud computing's services offers great opportunities for
consumers to find the best service and best cost. It offers a computing power
and a storage space adapted especially for Big Data processing. However, it
raises new challenges on how to select the best service out of the huge pool.
It is time-consuming for consumers to collect the necessary information and
analyze all service providers to make the right decision. Moreover, it'is a
highly demanding task from a computational perspective, because the same
computations may be conducted repeatedly by multiple consumers who have similar
requirements. Therefore, in this paper, we propose an approach based on
Analytic Hierarchy Process (AHP) method, which manages the selection of the
Cloud Service adequate to Big Data based on its parameters and criteria. We
applied this approach on a case study in order to validate its efficity. The
studied case is about the selection of the adequate Cloud Service for Big Data
in the context of National Health Service (NHS) of United Kingdom (UK).",http://arxiv.org/abs/2004.01640v1,arXiv
Information Technology,IT Service Management,Qualitative,"Semantic web based Sensor Planning Services (SPS) for Sensor Web
  Enablement (SWE)","The Sensor Planning Service (SPS) is service model to define the web service
interface for requesting user driven acquisitions and observation. It's defined
by the Open Geospatial Consortium (OGC) Sensor Web Enablement (SWE) group to
provide standardized interface for tasking sensors to allow to defining,
checking, modifying and cancelling tasks of sensor and sensor data. The goal of
Sensor Planning Service (SPS) of OGC - SWE is standardize the interoperability
between a client and a server collection management environment. The Sensor
Planning Service (SPS) is need to automate complex data flow in a large
enterprises that are depend on live & stored data from sensors and multimedia
equipment. The obstacle are faced in Sensor Planning Service (SPS) are (I)
Observation from sensor at the right time and right place will be problem, (II)
acquisition information(data) that are collected at a specific time and
specific place will be problem. The above two obstacle are accomplished and
obtained by the web based semantic technology in order to provide & apply the
ontology based semantic rule to user driven a acquisitions and observation of
Sensor Planning Service (SPS). The novelty of our approach is by adding the
semantic rule to Sensor Planning Service model in SWE and we implemented Sensor
Planning Service (SPS) with semantic knowledge based to achieve high
standardized service model for Sensor Planning Service (SPS) of OGC - SWE.",http://arxiv.org/abs/1207.5310v1,arXiv
Information Technology,IT Service Management,Qualitative,"Defining the optimal level of business benefits within IS/IT projects:
  Insights from benefit identification practices adopted in an IT Service
  Management (ITSM) project","The popularity of benefit realization management (BRM) in today's IT-enabled
world is fast gaining traction within IT organisations around the world.
However, there appears to be limited attention paid to the intra-organisational
practice by which benefits are identified. The purpose of this paper is
twofold: firstly, to describe and define a practice approach to the
identification of benefits that exploits a number of benefit identification
methods in an effort to more comprehensively identify IS/IT related benefits
that reflect the ongoing organisational investment. Secondly, to underline the
importance of this benefit identification process in the context of IT service
management (ITSM). This is achieved through a case study of an information
technology infrastructure library (ITIL) implementation in a multi-national
organization. The case study exposes a pragmatic practice approach of
customising such implementations in an effort to achieve a cost effective
implementation of IT services that reflects the context and requirements of
that specific organisation.",http://arxiv.org/abs/1606.03537v1,arXiv
Information Technology,IT Service Management,Mixed Methods,PECULIARITIES OF THE UNIVERSITIES' MANAGEMENT LOCATED ON FRONTLINE TERRITORIES,"<jats:p>The article analyzes the peculiarities of the management of universities (including displaced ones) located on the frontline territories, in the conditions of the war between Russia and Ukraine. It has been found that universities in these regions face unique challenges that require adaptation and the development of new management strategies, such as ensuring physical security, psychological support, infrastructure restoration, providing access to communications, quality educational services, hybrid learning models, scientific activities, financing and adaptation to new conditions. It is justified that the research is determined by the need for prompt response to security threats, ensuring the continuity of the educational process, infrastructure restoration, personnel support, international cooperation, development of innovative solutions, and socio-economic stability of the regions. The analysis and systematization of specific aspects of the management of higher education institutions operating in the frontline territories aimed at identifying key challenges and developing effective strategies for overcoming them was carried out. It has been proven that university management includes normative-procedural, structural-functional, and strategic-organizational processes. Peculiarities of the processes of implementation of innovative educational activities and conducting scientific research in universities located in the frontline territories were studied. Synergistic, systemic, statistical, comparative, and functional methods were used for the research. It is found that the synergistic method integrates various management components such as organizational, financial, personnel, and infrastructure management to create a holistic picture of wartime management. The system method allows you to consider the university as a whole system interacting with the environment, using interviews, discussions, surveys, and statistical analysis to collect data. The statistical method evaluates the impact of various factors on university management through quantitative data using means, medians, variances, regression models, and statistical tests. The comparative method analyzes and compares management practices, identifying the best of them to improve efficiency. The functional method assesses the effectiveness of key management functions such as planning, organization, resource allocation, communication, and control. It was found that the methodology for researching the peculiarities of the management of universities in the frontline territories involves a comprehensive approach, which includes the collection of primary and secondary data, qualitative and quantitative analysis, the development of recommendations and their validation, which provides a deep understanding of the problem and the development of effective strategies for the support and development of universities in war conditions.</jats:p>",https://doi.org/10.31891/dsim-2024-6(20),CrossRef
Information Technology,IT Service Management,Mixed Methods,Service Quality Management: A Process‐control Approach,"<jats:p>The world is moving into a services industry environment which is
evidencing many of the same productivity and quality issues associated
with manufacturing during the past two decades. Quick fixes using
qualitative approaches have not been universally successful and some
have advocated a more quantitative direction. Proposes a middle ground
incorporating both ideas. Reports the development of a process model
based on current service sector paradigms and more traditional
statistical quality‐control techniques from manufacturing management.
Details a test of the proposed model in the travel agency industry with
results generally confirming the potential for transporting
quality‐control concepts from manufacturing into services on a selected
basis. While the heterogeneity of services may constrain and even
preclude direct application of the process‐control approach in all
cases, the five‐step plan proposed may prove to be a useful tool for
service management across diverse businesses.</jats:p>",https://doi.org/10.1108/eum0000000002808,CrossRef
Information Technology,IT Service Management,Mixed Methods,Knowledge management in public service provision: the Child Support Agency,"<jats:p>The paper addresses the issue of knowledge management in public service organisations where the concept of provider competitiveness is of limited significance but other priorities prevail. The broad aim is to understand how the concept of “competitive advantage through knowledge management”, as practised in the broader business community, might translate to the modern Civil Service? This issue is explored through the medium of a study within the UK's Child Support Agency (CSA) based on the results of interviews with, and questionnaire responses from, the senior management group. The central question thereby investigated was: “To what extent can the conditions required for successful knowledge management be observed and evaluated?” A “conditions framework” and associated analysis is then used to assess broader implications and the possibility of wider application within other such public service organisations.</jats:p>",https://doi.org/10.1108/09564230310478828,CrossRef
Information Technology,IT Service Management,Mixed Methods,Exploring the alignment between service strategy and service innovation,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>Literature is relatively sparse on describing how companies should align their determinants for service innovations with their different types of service strategies. This study seeks to explore the alignment between three types of service strategies and determinants for service innovations.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>A qualitative, multi‐case research design on 12 Western European capital goods manufacturers including 24 service innovation projects was employed. The study is based on multiple sources of evidence: internal documentation of service innovation and development projects and, most importantly, interview data and participation in internal innovation workshops. Traditional inductive research methods were used to analyze the case studies.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>These indicate that aligning service strategies with determinants for service innovations is very complex. The configurations of the determinants are associated with the innovation success. Alternative configurations of determinants can create counterproductive effects and can limit the success of service innovation projects as well as implementation of service strategies.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Research limitations/implications</jats:title><jats:p>The study is based on interviews and case studies, but the external validity (generalizability) of the alignments could not be assessed accurately. Future research would benefit from insights obtained from quantitative data. The findings supplement existing research on success factors for the service business in manufacturing companies.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p>The findings imply that managers contemplating a specific service strategy have to consider the service innovation and reframe the determinant for service innovations accordingly. Companies trying to implement an after‐sales service strategy should focus on a narrow range of determinants for service innovations. The resulting configurations guide managers to set up an efficient and effective service innovation management that helps them to implement their service strategy through successful service innovation project.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>This empirical study shows that the configuration of determinants for service innovation differs for each service strategy. Whereas, the few similarities in determinants on service innovation are mainly other applications of existing theories on service innovation, the differences modify the existing theories.</jats:p></jats:sec>",https://doi.org/10.1108/09564231111175004,CrossRef
Information Technology,IT Service Management,Mixed Methods,Linking task and goal interdependence to quality service,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>With a twofold aim, the purpose of this paper is to focus on the service climate, including its antecedents, consequences, and a moderator. First, it examines whether task‐ and goal‐interdependent configuration facilitates the level of service climate; second, it tests the strength of the moderating role of service climate between service climate levels and service behavior.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>Among 54 nursing units at six hospitals, the data were collected using multiple methods (surveys, observations, administrative data).</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>Mixed‐linear model analyses indicated that the joint effects of task and goal interdependence related significantly to service climate level. Service climate strength moderated the relationship of service climate level to quality service behavior.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Research limitations/implications</jats:title><jats:p>The research approach may diminish the generalizability of the research results. Further work should test the propositions in other research contexts.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p>Quality service behaviors and the service climate could be promoted through well‐designed task‐ and goal‐interdependence structures within units. Assimilating a service climate in units is not enough. To promote high quality service behaviors, managers must direct their efforts toward finding agreement among team members with regard with the importance of service in their unit.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>The paper's findings offer empirical support to the persistent social interaction explanation of climate formation and point to the important role of interdependence for creating and maintaining service climate levels and promoting service behaviors in units.</jats:p></jats:sec>",https://doi.org/10.1108/09564231311323944,CrossRef
Information Technology,IT Service Management,Design and Development,"SONATA: Service Programming and Orchestration for Virtualized Software
  Networks","In conventional large-scale networks, creation and management of network
services are costly and complex tasks that often consume a lot of resources,
including time and manpower. Network softwarization and network function
virtualization have been introduced to tackle these problems. They replace the
hardware-based network service components and network control mechanisms with
software components running on general-purpose hardware, aiming at decreasing
costs and complexity of implementing new services, maintaining the implemented
services, and managing available resources in service provisioning platforms
and underlying infrastructures. To experience the full potential of these
approaches, innovative development support tools and service provisioning
environments are needed. To answer these needs, we introduce the SONATA
architecture, a service programming, orchestration, and management framework.
We present a development toolchain for virtualized network services, fully
integrated with a service platform and orchestration system. We motivate the
modular and flexible architecture of our system and discuss its main components
and features, such as function- and service-specific managers that allow fine-
grained service management, slicing support to facilitate multi-tenancy,
recursiveness for improved scalability, and full-featured DevOps support.",http://arxiv.org/abs/1605.05850v1,arXiv
Information Technology,IT Service Management,Design and Development,"Inter-organizational fault management: Functional and organizational
  core aspects of management architectures","Outsourcing -- successful, and sometimes painful -- has become one of the
hottest topics in IT service management discussions over the past decade. IT
services are outsourced to external service provider in order to reduce the
effort required for and overhead of delivering these services within the own
organization. More recently also IT services providers themselves started to
either outsource service parts or to deliver those services in a
non-hierarchical cooperation with other providers. Splitting a service into
several service parts is a non-trivial task as they have to be implemented,
operated, and maintained by different providers. One key aspect of such
inter-organizational cooperation is fault management, because it is crucial to
locate and solve problems, which reduce the quality of service, quickly and
reliably. In this article we present the results of a thorough use case based
requirements analysis for an architecture for inter-organizational fault
management (ioFMA). Furthermore, a concept of the organizational respective
functional model of the ioFMA is given.",http://arxiv.org/abs/1101.3891v1,arXiv
Information Technology,IT Service Management,Design and Development,Managing and Querying Web Services Communities: A Survey,"With the advance of Web Services technologies and the emergence of Web
Services into the information space, tremendous opportunities for empowering
users and organizations appear in various application domains including
electronic commerce, travel, intelligence information gathering and analysis,
health care, digital government, etc. However, the technology to organize,
search, integrate these Web Services has not kept pace with the rapid growth of
the available information space. The number of Web Services to be integrated
may be large and continuously changing. To ease and improve the process of Web
services discovery in an open environment like the Internet, it is suggested to
gather similar Web services into groups known as communities. Although Web
services are intensively investigated, the community management issues have not
been addressed yet In this paper we draw an overview of several Web services
Communities' management approaches based on some currently existing communities
platforms and frameworks. We also discuss different approaches for querying and
selecting Web services under the umbrella of Web services communities'. We
compare the current approaches among each others with respect to some key
requirements.",http://arxiv.org/abs/1103.0921v1,arXiv
Information Technology,IT Service Management,Design and Development,"Towards a decentralized data privacy protocol for self-sovereignty in
  the digital world","A typical user interacts with many digital services nowadays, providing these
services with their data. As of now, the management of privacy preferences is
service-centric: Users must manage their privacy preferences according to the
rules of each service provider, meaning that every provider offers its unique
mechanisms for users to control their privacy settings. However, managing
privacy preferences holistically (i.e., across multiple digital services) is
just impractical. In this vision paper, we propose a paradigm shift towards an
enriched user-centric approach for cross-service privacy preferences
management: the realization of a decentralized data privacy protocol.",http://arxiv.org/abs/2404.12837v1,arXiv
Information Technology,IT Service Management,Design and Development,CapExec: Towards Transparently-Sandboxed Services (Extended Version),"Network services are among the riskiest programs executed by production
systems. Such services execute large quantities of complex code and process
data from arbitrary and untrusted network sources, often with high levels of
system privilege. It is desirable to confine system services to a
least-privileged environment so that the potential damage from a malicious
attacker can be limited, but existing mechanisms for sandboxing services
require invasive and system-specific code changes and are insufficient to
confine broad classes of network services.
  Rather than sandboxing one service at a time, we propose that the best place
to add sandboxing to network services is in the service manager that starts
those services. As a first step towards this vision, we propose CapExec, a
process supervisor that can execute a single service within a sandbox based on
a service declaration file in which, required resources whose limited access to
are supported by Caper services, are specified. Using the Capsicum
compartmentalization framework and its Casper service framework, CapExec
provides robust application sandboxing without requiring any modifications to
the application itself. We believe that this is a first step towards ubiquitous
sandboxing of network services without the costs of virtualization.
  Keywords: application security, sandboxing, service manager, Capsicum,
compartmentalization",http://arxiv.org/abs/1909.12282v1,arXiv
Information Technology,IT Service Management,Theoretical / Conceptual,Blockchain in Service Management and Service Research - Developing a Research Agenda and Managerial Implications,"<p>As blockchain technology is maturing to be confidently used in practice, its applications are becoming evident and, correspondingly, more blockchain research is being published, also extending to more domains than before. To date, scientific research in the field has predominantly focused on subject areas such as finance, computer science, and engineering, while the area of service management has largely neglected this topic. Therefore, we invited a group of renowned scholars from different academic fields to share their views on emerging topics regarding blockchain in service management and service research. Their individual commentaries and conceptual contributions refer to different theoretical and domain perspectives, including managerial implications for service companies as well as forward-looking suggestions for further research.</p>",https://doi.org/10.15358/2511-8676-2021-2-71,CrossRef
Information Technology,IT Service Management,Theoretical / Conceptual,Service Management: An Evaluation and the Future,"<jats:p>Deals with the future of service management in specific service
management terms but also in broader, societal terms, from both a
practitioner and a scholarly perspective. Claims that service
management concerns not only what is traditionally known as service
organizations, but also constitutes a future paradigm for organizations
in general. The goods‐services division in its traditional sense is
outdated; it represents a myopic production view, while the service
economy is an expression for customer‐oriented and citizen‐oriented,
value‐enhancing offering. Although service management has taken a giant
step since the late 1970s, we are just beginning to see a new era of
management that will fight the battle for economic survival in the
future service society.</jats:p>",https://doi.org/10.1108/09564239410051920,CrossRef
Information Technology,IT Service Management,Theoretical / Conceptual,QUIS 9 symposium – service excellence in management,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>To introduce the special issue focusing on the QUIS 9 symposium.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>A brief perspective of the best papers presented at the Quality in Services (QUIS9) symposium held at Karlstad university, Sweden in June 2004.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>Outlines some of the highlights surrounding the conference.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>Provides a brief report of the context of the conference.</jats:p></jats:sec>",https://doi.org/10.1108/09564230510592261,CrossRef
Information Technology,IT Service Management,Theoretical / Conceptual,Viable service systems and decision making in service management,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>The purpose of this paper is to highlight how systems thinking contributes to decision making in uncertain contexts that are characteristic of service systems. Based on the assumption that service systems face complex conditions, the paper posits that systems thinking may support the understanding of key issues in service management.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>This paper proposes an interpretation of complexity in the context of service systems, which highlights the perspective change that occurs when a systems approach is adopted. The offered conceptual perspective is then brought to an operational level, in spite of the complexity of the decisions driving a viable system, by modelling a service system as a network of agents, resources, processes and decisions through the use of fuzzy logic. The paper reviews service management research streams, and takes a deeper look at the concepts of service systems and complex service systems. The paper then proceeds to discuss how systems thinking contributes to service management by proposing a systems interpretation of complexity.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>Service management theories and models may be enhanced by integrating prevailing approaches, based on a quantitative and mechanistic view of service systems dynamics, with systems thinking‐based meta‐models that can be used in better understanding service exchanges. The findings of the paper also show how the integration of an engineering approach can be insightful to the understanding of service systems; adopting a Viable Systems Approach (VSA) as a meta‐model can be useful in fully comprehending market behaviour in uncertain conditions.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>The originality of this paper lies in exploring the contribution of systems thinking, in particular of the Viable Systems Approach (VSA), to service management and decision making.</jats:p></jats:sec>",https://doi.org/10.1108/09564231211260396,CrossRef
Information Technology,IT Service Management,Theoretical / Conceptual,A capacity management model in service industries,"<jats:p>The problem of capacity management is one of the most difficult to tackle in business management; a situation which is aggravated in the majority of services, due to uncertain demand and personalized requirements, which make it difficult to plan and assign productive capacity. While overstaffing implies extra costs, insufficient capacity implies a lower level of attention to customer needs and therefore a lack of perceived quality. The present article tackles this problem, presenting a model that enables minimum staffing to be easily determined. By taking into account historical staffing data associated with quality data, minimum recommended workload is calculated as a function of the theoretical staff needed according to standard time. The model has been applied in two real cases.</jats:p>",https://doi.org/10.1108/09564230210431983,CrossRef
Information Technology,Data Management,Quantitative,Data Wrangling and Management in R,"<jats:p>
            This tutorial explores how scholars can organize 'tidy' data, understand R packages to manipulate data, and conduct basic data analysis.
          </jats:p>",https://doi.org/10.46430/phen0063,CrossRef
Information Technology,Data Management,Quantitative,Data Governance Evaluation of the Data Management Office at King Saud University based on the National Data Management Office standards,"<jats:p>This study aimed to investigate the implementation of data governance at the Data Management Office of King Saud University by examining the adoption of the strategic plan, implementation mechanisms, and compliance with data governance standards and controls established by the National Data Management Office (NDMO), the national regulatory and reference authority for data management and governance.Using a descriptive-analytical approach, a questionnaire was designed based on three dimensions: the strategic dimension, the executive dimension, and the challenges dimension. The study sample included all 15 employees of the Data Management Office at King Saud University. The data were analyzed using IBM SPSS Modeler statistical software.The results of the study at the level of the main dimensions showed that the strategic dimension achieved the highest arithmetic mean (4.51) with a very high degree of compliance, followed by the executive dimension (3.69) with a high degree of compliance, and finally, the challenges dimension obtained a medium degree with an average of (3.05).The study revealed that the lack of data unification and its inconsistency across university departments, as well as the scarcity of financial resources that limit the improvement of the technological infrastructure at the Data Management Office, represent the most important challenges facing the implementation mechanisms of data governance at the Data Management Office at King Saud University.The researcher recommends the necessity of creating a clear framework for cooperation to define roles, responsibilities, and joint operations between the university colleges and the Data Management Office regarding data circulation, and strengthening cooperation with external parties with expertise in the field of data governance to improve implementation processes.</jats:p>",https://doi.org/10.52783/jisem.v10i10s.1530,CrossRef
Information Technology,Data Management,Quantitative,Construction of financial data management and analysis system based on big data,"<jats:p>To solve financial data management, the author proposes research on the framework of big data analysis system. In the activities of various enterprises, massive high-frequency data will be generated, and these data are often characterized by infinity, suddenness, disorder, and volatility. Due to the above characteristics of the financial data related to the production activities of the enterprise, the financial data of the enterprise is often different from the actual situation, and the access is not standardized. To solve the above problems, it is necessary to improve the efficiency of enterprise financial management. With the application of big data in the enterprise industry as the background, the author builds a big data-based enterprise financial data analysis system, the construction of the financial data analysis system of electric enterprises is described from three aspects: big data storage, big data preprocessing and financial sharing center construction, to help enterprises improve the accuracy of financial data and the efficiency of financial management.</jats:p>",https://doi.org/10.54691/bcpbm.v37i.3617,CrossRef
Information Technology,Data Management,Quantitative,Data literacy and management of research data – a prerequisite for the sharing of research data,"<jats:sec><jats:title content-type=""abstract-subheading"">Purpose</jats:title><jats:p>The purpose of this paper is to investigate the knowledge and attitude about research data management, the use of data management methods and the perceived need for support, in relation to participants’ field of research.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title><jats:p>This is a quantitative study. Data were collected by an email survey and sent to 792 academic researchers and doctoral students. Total response rate was 18% (<jats:italic>N</jats:italic> = 139). The measurement instrument consisted of six sets of questions: about data management plans, the assignment of additional information to research data, about metadata, standard file naming systems, training at data management methods and the storing of research data.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Findings</jats:title><jats:p>The main finding is that knowledge about the procedures of data management is limited, and data management is not a normal practice in the researcher's work. They were, however, in general, of the opinion that the university should take the lead by recommending and offering access to the necessary tools of data management. Taken together, the results indicate that there is an urgent need to increase the researcher's understanding of the importance of data management that is based on professional knowledge and to provide them with resources and training that enables them to make effective and productive use of data management methods.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Research limitations/implications</jats:title><jats:p>The survey was sent to all members of the population but not a sample of it. Because of the response rate, the results cannot be generalized to all researchers at the university. Nevertheless, the findings may provide an important understanding about their research data procedures, in particular what characterizes their knowledge about data management and attitude towards it.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Practical implications</jats:title><jats:p>Awareness of these issues is essential for information specialists at academic libraries, together with other units within the universities, to be able to design infrastructures and develop services that suit the needs of the research community. The findings can be used, to develop data policies and services, based on professional knowledge of best practices and recognized standards that assist the research community at data management.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Originality/value</jats:title><jats:p>The study contributes to the existing literature about research data management by examining the results by participants’ field of research. Recognition of the issues is critical in order for information specialists in collaboration with universities to design relevant infrastructures and services for academics and doctoral students that can promote their research data management.</jats:p></jats:sec>",https://doi.org/10.1108/ajim-04-2020-0110,CrossRef
Information Technology,Data Management,Quantitative,POURQUOI MANAGEMENT &amp; DATA SCIENCE ABANDONNE L’ÉVALUATION EN DOUBLE AVEUGLE ?,"<jats:p>La globalisation du secteur académique, conjuguée à une accélération sans précédent de la transformation digitale, implique de repenser en profondeur le rôle des revues scientifiques (Mamavi et Zerbib, 2020). C’est en tout cas la conviction de Management &amp; Data Science, une plateforme ouverte de création et de diffusion de la connaissance sur le thème de la transformation digitale. Management &amp; Data Science offre une alternative concrète à la logique du “publish or perish” (Mamavi et Zerbib, 2019) en permettant une reconnaissance de nos auteurs via les altmetrics. La ligne éditoriale de notre revue défend un positionnement fondé sur la rigueur, la pertinence, la vitesse et l'impact alternatif. Management &amp; Data Science estime qu'une étude peut être qualifiée de scientifique dès lors qu'elle répond à au moins deux critères fondamentaux : la transparence et la reproductibilité. Or, le double aveugle ne nous semble pas ici constituer un moyen incontournable pour évaluer ces deux critères.</jats:p>",https://doi.org/10.36863/mds.a.16285,CrossRef
Information Technology,Data Management,Qualitative,Fitting random cash management models to data,"Organizations use cash management models to control balances to both avoid
overdrafts and obtain a profit from short-term investments. Most management
models are based on control bounds which are derived from the assumption of a
particular cash flow probability distribution. In this paper, we relax this
strong assumption to fit cash management models to data by means of stochastic
and linear programming. We also introduce ensembles of random cash management
models which are built by randomly selecting a subsequence of the original cash
flow data set. We illustrate our approach by means of a real case study showing
that a small random sample of data is enough to fit sufficiently good
bound-based models.",http://arxiv.org/abs/2401.08548v1,arXiv
Information Technology,Data Management,Qualitative,UDBMS: Road to Unification for Multi-model Data Management,"A traditional database systems is organized around a single data model that
determines how data can be organized, stored and manipulated. But the vision of
this paper is to develop new principles and techniques to manage multiple data
models against a single, integrated backend. For example, semi-structured,
graph and relational models are examples of data models that may be supported
by a new system. Having a single data platform for managing both
well-structured data and NoSQL data is beneficial to users; this approach
significantly reduces integration, migration, development, maintenance and
operational issues. The problem is challenging: the existing database
principles mainly work for a single model and the research on multi-model data
management is still at an early stage. In this paper, we envision a UDBMS
(Unified Database Management System) for multi-model data management in one
platform. UDBMS will provide several new features such as unified data model
and flexible schema, unified query processing, unified index structure and
cross-model transaction guarantees. We discuss our vision as well as present
multiple research challenges that we need to address.",http://arxiv.org/abs/1612.08050v1,arXiv
Information Technology,Data Management,Qualitative,"Legitimization of Data Quality Practices in Health Management
  Information Systems Using DHIS2. Case of Malawi","Medical doctors consider data quality management a secondary priority when
delivering health care. Medical practitioners find data quality management
practices intrusive to their operations. Using Health Management Information
System (HMIS) that uses DHIS2 platform, our qualitative case study establishes
that isomorphism leads to legitimization of data quality management practices
among health practitioners and subsequently data quality. This case study
employed the methods of observation, semi structured interviews and review of
artefacts to explore how through isomorphic processes data quality management
practices are legitimized among the stakeholders. Data was collected from
Ministry of Health's (Malawi) HMIS Technical Working Group members in Lilongwe
and from medical practitioners and data clerks in Thyolo district. From the
findings we noted that mimetic isomorphism led to moral and pragmatic
legitimacy while and normative isomorphism led to cognitive legitimacy within
the HMIS structure and helped to attain correctness and timeliness of the data
and reports respectively. Through this understanding we firstly contribute to
literature on organizational issues in IS research. Secondly, we contribute to
practice as we motivate health service managers to capitalize on isomorphic
forces to help legitimization of data quality management practices among health
practitioners.",http://arxiv.org/abs/2108.09942v1,arXiv
Information Technology,Data Management,Qualitative,"Data Management in Integrated Research Institutes: Undertaking a Review
  of Research Data Management at the Rosalind Franklin Institute","Managing Research Data, and making it available for use/reuse by others in
line with the UKRI Concordat on Open Research Data and FAIR principles, is a
major issue for research-intensive organisations. In this case study we outline
an institute-wide review of data management in practice, carried out at the
Rosalind Franklin Institute (The Franklin) in partnership with external
consultants, Curlew Research, in March 2022. We aim to describe the processes
involved in undertaking a review of the services already in place that support
good practice in managing research data, and their uptake, with an emphasis on
the methodology used. We conducted interviews with scientific Theme Leads which
set the scope for the Data Management Workshops subsequently held with
Researchers. Workshops were valuable in both providing actionable insights for
data management and in priming the audience for future discussions. The final
report produced for The Franklin, summarising results of the analysis, provides
a snapshot of current practice for the Institute, highlighting what is working
well and where improvements might be made, and provides a benchmark against
which development can be measured in the coming years. The Review will continue
to be conducted on an annual basis, reflecting changes in a fast-moving area
and enabling an agile approach to research data management.",http://arxiv.org/abs/2211.07284v2,arXiv
Information Technology,Data Management,Qualitative,"Computer Application Research based on Chinese Human Resources and
  Network Information Security Technology Management and Analysis In Chinese
  Universities","This study investigates the current state of computer network security and
human resource management within Chinese universities, emphasizing the growing
importance of safeguarding digital infrastructures. To support the analysis,
interviews were conducted with managers from two leading Chinese cybersecurity
firms and the qualitative data obtained was carefully analyzed to extract key
insights and conclusions.",http://arxiv.org/abs/2411.00474v1,arXiv
Information Technology,Data Management,Mixed Methods,Integrated data management: New perspectives for management control,"<jats:p>The purpose of this special issue is to contribute to the international debate on novel approaches to corporate data management for the improvement of man-agement control systems. The theme embraces the interdisciplinary relationships that management controls and the accounting function that it, increasingly, should have with other disciplines. These relationships are permeated by both quantitative and qualitative approaches. Quantitative models include those created through data science and those that refer to mathematics, statistics, and information tech-nology. Moreover, corporate data management models combining qualitative and quantitative approaches are explored and discussed within disciplinary areas that, due to their consolidated history, are close to management control: e.g., legal, soci-ological, historical and other humanities areas. This theme embraces not only business data management but also knowledge management; it is not only about internal (accounting and non-accounting) data, but also about external data of a statistical, economic, and social nature, which are of interest from different disci-plinary perspectives concerning the integrated management of accounting data and Big Data. With the development of new technologies, such as the ‘Internet of Things', and the increasingly extensive applications of blockchain, social net-works, and mobile devices, organizations are generating huge volumes of data in different formats much faster than in the past. In this sense, big data analytics techniques present opportunities to improve decision-making processes of both a strategic and an operational nature, due to their ability to extract knowledge from data, to facilitate problem solving, and to favor predictive and prescriptive ap-proaches to business phenomena. From an organizational point of view, it is im-portant to analyze the impact of Big Data on the professional profiles of actors typically involved in accounting and management control processes.</jats:p>",https://doi.org/10.3280/maco2022-002-s1001,CrossRef
Information Technology,Data Management,Mixed Methods,DATA ANALYSIS OF METROLOGICAL DATA,"<jats:p>This project report is set to give an interactive visualization and analytical presentation for Meteorological records in Finland. These Meteorological records Data of Finland is recorded by way of integrating the three current infrastructures for numerical weather prediction, observational information and satellite tv for pc image processing and this is recorded. The Meteorological data used in the study consists of near- floor atmospheric elements including wind direction, apparent temperature, cloud layer(s), ceiling peak, visibility, current weather, wind velocity, cloud cowl and precipitation amount and so on. The data consists of hourly recorded data for the past ten years of Finland from 2006-04- 01 at time 00:00:00.000 to 2016-09-09 at time 23:00:00.000. The analysis had been performed out for 2-m floor temperature. Through this analysis, all the valuable insights into the changing weather patterns and environmental conditions in Finland are analyzed and presented in an interactive visualization, providing a solid foundation for understanding and addressing the impacts of Global Warming in the region. This project main objective is to show the complete analysis of the influences of the Global Warming on the Apparent temperature &amp; humidity in Finland over the course of 10 years from 2006 to 2016.     Keywords — Data Analysis, Data Visualization, Meteorological Data, Climate Change, Global Warning, Apparent Temperature, Humidity</jats:p>",https://doi.org/10.55041/ijsrem34399,CrossRef
Information Technology,Data Management,Mixed Methods,Data Shuffling—A New Masking Approach for Numerical Data,"<jats:p> This study discusses a new procedure for masking confidential numerical data—a procedure called data shuffling—in which the values of the confidential variables are “shuffled” among observations. The shuffled data provides a high level of data utility and minimizes the risk of disclosure. From a practical perspective, data shuffling overcomes reservations about using perturbed or modified confidential data because it retains all the desirable properties of perturbation methods and performs better than other masking techniques in both data utility and disclosure risk. In addition, data shuffling can be implemented using only rank-order data, and thus provides a nonparametric method for masking. We illustrate the applicability of data shuffling for small and large data sets. </jats:p>",https://doi.org/10.1287/mnsc.1050.0503,CrossRef
Information Technology,Data Management,Mixed Methods,Management trends and implementation of AI in university management,"<jats:p>The objective of this article was to explore managerial trends and the implementation of artificial intelligence in university management, with a particular focus on the Latin American context. To this end, a mixed study was designed, operationalized through a documentary review with bibliometric procedures, a qualitative thematic analysis, a triangulation system, and an integration of data supported by external sources. The results were organized into five management strategies, three emerging trends, five recommendations for managers, and five main themes. These trends reflect significant progress, but also pose challenges, especially in regions with structural inequalities and resource constraints. The data analyzed indicate the need for a balanced approach that combines technological innovation with ethical and social considerations. Furthermore, the findings emphasize the importance of international collaboration and local capacity building to ensure equitable and sustainable implementation of AI. It is concluded that it is cardinal to underline the potential of AI to transform higher education, provided that technical, ethical, and social challenges are addressed comprehensively.</jats:p>",https://doi.org/10.56294/dm2025866,CrossRef
Information Technology,Data Management,Mixed Methods,Assessing Knowledge Management Readiness to Improve Data Quality by Prevent Incorrect Data Input on ERP System in Component Rebuild Center PT Kalcoal,"<jats:p>To fulfil the demand of availability component for supporting the mining operation at PT Kalcoal. Recondition component daily activity of recondition is entanglement from one process to another until the end of process. For transform digitalization, recondition component activity will using Enterprise Resource Planning (ERP) in every process.  The purpose of this study is to understand the level of knowledge management readiness in the component rebuild center in the Implementation of the ERP work system, to understand the strengths and areas for improvement for Knowledge Management, and to find tools that can be used to implement Knowledge Management in the component rebuild center.  This research using quantitative method, collected through a questionnaire. Furthermore, qualitative data was used to enrich the primary data collected through focus group discussions. By using APO as tools of knowledge management readiness, the results showed that the component rebuild center is still in the expansion phase, where knowledge management efforts are present in the core activity process and the company sees the benefits of knowledge sharing. The component rebuild center can further develop itself by using people as its accelerator. Having committed change groups and leaders will create a culture of knowledge sharing that can be followed by all lines of business in the company. In order for employees to have a single source of knowledge, it is important to create a knowledge management tool that can be done by building a new portal that can be accessed by all employee users. For the source of knowledge to always align the needs and developments of the company, it is necessary to have the ability to grow and store information safely and freely for employees so that information is always updated and well-maintained.</jats:p>",https://doi.org/10.47191/ijcsrr/v7-i5-100,CrossRef
Information Technology,Data Management,Design and Development,Understanding management data systems for enterprise performance management,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>Managing enterprise performance is an important, yet a difficult process due to its complexity. The process involves monitoring the strategic focus of an enterprise, whose performance is measured from the analysis of data generated from a wide range of interrelated business activities performed at different levels within the enterprise. This study aims to investigate management data systems technologies in terms of how they are used and the issues that are related to their effective management within the broader context of enterprise performance management (EPM).</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>A range of recently published research literature on data warehousing, online analytic processing and EPM is reviewed to explore their current state, issues and challenges learned from their practice.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>The findings of the study are reported in two parts. The first part discusses the current business practices of these technologies, and the second part identifies and discusses the issues and challenges the business managers dealing with these technologies face for gaining competitive advantage for their businesses.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>The study findings are intended to assist the business managers to effectively understand the issues and technologies behind EPM implementation.</jats:p></jats:sec>",https://doi.org/10.1108/02635570610640988,CrossRef
Information Technology,Data Management,Design and Development,Need for Data Standardization and Infrastructure of Research Data Management to Promote Using Real-world Data,"<jats:p>Real-world data (RWD) have been increasingly used for regulatory decision-making and as a control group for new drug approval applications. RWD is also helpful in understanding information such as risk factors (e.g., pre-existing medical conditions, personal protective equipment, travel, contacts, smoking, and exposure to animals) and vaccination status for the coronavirus disease 2019 (COVID-19). The methodology of utilizing RWD is inconsistent across healthcare institutions. However, there are possible solutions to standardize RWD for clinical data use, which include the use of Clinical Data Interchange Standards Consortium (CDISC) standards, tools, and concepts. This study examines the availability of CDISC and other international standards for the utilization of RWD with concrete examples and presents the potential platform for implementation.
We consider the solution currently available to temporarily convert clinical data-warehouse (DWH) data into the Fast Healthcare Interoperability Resources (FHIR) format to comply with the CDISC standard. This approach would allow for converting institution-level standards to national standards as an interim solution until FHIR is supported, mapping national standards to international standards. We believe that the ideal research environment is a data platform that complies with national and international regulations related to RWD applications. Within such a platform, users can share data freely, rather than rely on a specific facility or vendor. Data platform developments are progressing in Japan and globally. In Japan, initiatives to use research data on research data platforms are being conducted. We are experimenting with implementing tools and knowledge shared by CDISC.</jats:p>",https://doi.org/10.47912/jscdm.210,CrossRef
Information Technology,Data Management,Design and Development,Data Modeling and Management in the Big Data Era,"<jats:p>CFX Inc, an e-commerce start-up based out of India, has built a large e-marketplace that allows sellers and buyers to transact online. The firm currently has 30,000 sellers and aims to have around 50,000 sellers by FY 2015–16. In order to provide best shopping experience to their growing customer base, the firm needs to collect, store and analyze different kinds of data and improve their customer shopping experience. It is in the process of identifying and designing suitable data management systems to sustain and manage their business growth. The management needs a concrete set of recommendations in terms of the nature of solution, choice of the database, a data model that suits CFX's requirements, cost-benefit trade-offs involved and implementation considerations.</jats:p>",https://doi.org/10.1108/case.iima.2020.000057,CrossRef
Information Technology,Data Management,Design and Development,Towards an Enhanced Data- and Knowledge Management Capability: A Data Life Cycle Model Proposition for Integrated Vehicle Health Management,"<jats:p>The creation, capturing, using and sharing of knowledge is based on data. The rate of data creation, collection, and elicitation through wide range experiments, simulations and measurements is rapidly increasing within Integrated Vehicle Health Management (IVHM). In addition, Knowledge Management (KM), data abstraction, analyses, storage and accessibility challenges persist, resulting in loss of knowledge and increased costs. This growth in the creation of research data, algorithms, technical papers, reports and logs, requires both a strategy and tool to address these challenges. A Data Life Cycle Model (DLCM) ensures the efficient and effective abstraction and management of both data and knowledge outputs. IVHM which depend heavily on high-quality data to perform data-driven, model-based and hybrid computational analysis of asset health. IVHM Centre does not yet have a systematic and coherent approach to its data management. The absence of a DLCM means that valuable knowledge might be lost or is difficult to find. Data visualization is fragmented and done on a project by project basis leading to increased costs. There is insufficient algorithm documentation and communication for easy transition between subsequent researchers and personnel. A systematic review of DLCMs, frameworks, standards and process models pertaining to data- and KM in the context of IVHM, found that there is no DLCM that is consistent with IVHM data and knowledge management requirements. Specifically, there is a need to develop a DLCM based on Open System Architecture for Condition-Based Maintenance framework.</jats:p>",https://doi.org/10.36001/phmconf.2019.v11i1.842,CrossRef
Information Technology,Data Management,Design and Development,Contextual Data-Integrated Newsvendor Solution with Operational Data Analytics (ODA),"<jats:p> We study the data-integrated newsvendor problem in which the random demand depends on a set of covariates. Observing from the solutions analyzed in the existing studies, we identify the equivariant class of operational statistics (i.e., the mapping from the demand and covariate data to the inventory decision) to develop the operational data analytics (ODA) framework for the contextual newsvendor problem. The equivariant property is intuitively appealing, and it is justified by the fact that, regardless of the sample size, no other decision rule can uniformly dominate the optimal operational statistic within the equivariant class. We also demonstrate that nonequivariant solutions can produce unstable empirical performance with limited samples, whereas equivariant solutions exhibit robustness. When the distribution family of the demand is known but the coefficients of the demand function are unknown, we can directly validate the decision performance of operational statistics within the equivariant class and derive the uniformly optimal solution. When the distribution family of the demand is unknown, we formulate the data integration model as a subclass of equivariant operational statistics, obtained through adaptively boosting some candidate solution. For decision validation, we project the validation data to the demand for the covariates of interests, and the projection is constructed by utilizing the structure of the candidate solution. We demonstrate the superior small-sample performance of adaptive boosting and establish the consistency of the boosted operational statistics. Our ODA formulation, building on the inherent characteristics of the contextual newsvendor problem, highlights the importance of understanding structural properties in data-integrated decision making. </jats:p><jats:p> This paper was accepted by David Simchi-Levi, operations management. </jats:p><jats:p> Supplemental Material: The online appendices are available at https://doi.org/10.1287/mnsc.2023.04164 . </jats:p>",https://doi.org/10.1287/mnsc.2023.04164,CrossRef
Information Technology,Data Management,Theoretical / Conceptual,"A Systematic Literature Review on Task Allocation and Performance
  Management Techniques in Cloud Data Center","As cloud computing usage grows, cloud data centers play an increasingly
important role. To maximize resource utilization, ensure service quality, and
enhance system performance, it is crucial to allocate tasks and manage
performance effectively. The purpose of this study is to provide an extensive
analysis of task allocation and performance management techniques employed in
cloud data centers. The aim is to systematically categorize and organize
previous research by identifying the cloud computing methodologies, categories,
and gaps. A literature review was conducted, which included the analysis of 463
task allocations and 480 performance management papers. The review revealed
three task allocation research topics and seven performance management methods.
Task allocation research areas are resource allocation, load-Balancing, and
scheduling. Performance management includes monitoring and control, power and
energy management, resource utilization optimization, quality of service
management, fault management, virtual machine management, and network
management. The study proposes new techniques to enhance cloud computing work
allocation and performance management. Short-comings in each approach can guide
future research. The research's findings on cloud data center task allocation
and performance management can assist academics, practitioners, and cloud
service providers in optimizing their systems for dependability,
cost-effectiveness, and scalability. Innovative methodologies can steer future
research to fill gaps in the literature.",http://arxiv.org/abs/2402.13135v1,arXiv
Information Technology,Data Management,Theoretical / Conceptual,"The Changing Locus of Health Data Production and Use: Patient-Generated
  Health Data, Observations of Daily Living, and Personal Health Information
  Management","Despite the growing attention of researcher, healthcare managers and policy
makers, data gathering and information management practices are largely
untheorized areas. In this work are presented and discussed some early-stage
conceptualizations: Patient-Generated Health Data (PGHD), Observations of Daily
Living (ODLs) and Personal Health Information Management (PHIM). As I shall try
to demonstrate, these labels are not neutral rather they underpin quite
different perspectives with respect to health, patient-doctor relationship, and
the status of data.",http://arxiv.org/abs/1606.09589v3,arXiv
Information Technology,Data Management,Theoretical / Conceptual,"Exploring Data Management Challenges and Solutions in Agile Software
  Development: A Literature Review and Practitioner Survey","Context: Managing data related to a software product and its development
poses significant challenges for software projects and agile development teams.
These include integrating data from diverse sources and ensuring data quality
amidst continuous change and adaptation. Objective: The paper systematically
explores data management challenges and potential solutions in agile projects,
aiming to provide insights into data management challenges and solutions for
both researchers and practitioners. Method: We employed a mixed-methods
approach, including a systematic literature review (SLR) to understand the
state-of-research followed by a survey with practitioners to reflect on the
state-of-practice. The SLR reviewed 45 studies, identifying and categorizing
data management aspects along with their associated challenges and solutions.
The practitioner survey captured practical experiences and solutions from 32
industry practitioners who were significantly involved in data management to
complement the findings from the SLR. Results: Our findings identified major
data management challenges in practice, such as managing data integration
processes, capturing diverse data, automating data collection, and meeting
real-time analysis requirements. To address the challenges, solutions such as
automation tools, decentralized data management practices, and ontology-based
approaches have been identified. The solutions enhance data integration,
improve data quality, and enable real-time decision-making by providing
flexible frameworks tailored to agile project needs. Conclusion: The study
pinpointed significant challenges and actionable solutions in data management
for agile software development. Our findings provide practical implications for
practitioners and researchers, emphasizing the development of effective data
management practices and tools to address those challenges and improve project
success.",http://arxiv.org/abs/2402.00462v4,arXiv
Information Technology,Data Management,Theoretical / Conceptual,"Designing for Recommending Intermediate States in A Scientific Workflow
  Management System","To process a large amount of data sequentially and systematically, proper
management of workflow components (i.e., modules, data, configurations,
associations among ports and links) in a Scientific Workflow Management System
(SWfMS) is inevitable. Managing data with provenance in a SWfMS to support
reusability of workflows, modules, and data is not a simple task. Handling such
components is even more burdensome for frequently assembled and executed
complex workflows for investigating large datasets with different technologies
(i.e., various learning algorithms or models). However, a great many studies
propose various techniques and technologies for managing and recommending
services in a SWfMS, but only a very few studies consider the management of
data in a SWfMS for efficient storing and facilitating workflow executions.
Furthermore, there is no study to inquire about the effectiveness and
efficiency of such data management in a SWfMS from a user perspective. In this
paper, we present and evaluate a GUI version of such a novel approach of
intermediate data management with two use cases (Plant Phenotyping and
Bioinformatics). The technique we call GUI-RISPTS (Recommending Intermediate
States from Pipelines Considering Tool-States) can facilitate executions of
workflows with processed data (i.e., intermediate outcomes of modules in a
workflow) and can thus reduce the computational time of some modules in a
SWfMS. We integrated GUI-RISPTS with an existing workflow management system
called SciWorCS. In SciWorCS, we present an interface that users use for
selecting the recommendation of intermediate states (i.e., modules' outcomes).
We investigated GUI-RISP's effectiveness from users' perspectives along with
measuring its overhead in terms of storage and efficiency in workflow
execution.",http://arxiv.org/abs/2010.04880v1,arXiv
Information Technology,Data Management,Theoretical / Conceptual,"DroneXNFT: An NFT-Driven Framework for Secure Autonomous UAV Operations
  and Flight Data Management","Non-Fungible Tokens (NFTs) have emerged as a revolutionary method for
managing digital assets, providing transparency and secure ownership records on
a blockchain. In this paper, we present a theoretical framework for leveraging
NFTs to manage UAV (Unmanned Aerial Vehicle) flight data. Our approach focuses
on ensuring data integrity, ownership transfer, and secure data sharing among
stakeholders. This framework utilizes cryptographic methods, smart contracts,
and access control mechanisms to enable a tamper-proof and privacy-preserving
management system for UAV flight data.",http://arxiv.org/abs/2409.06507v1,arXiv
Information Technology,User Support,Quantitative,Implementasi Algoritma Support Vector Machine dan Chi Square untuk Analisis Sentimen User Feedback Aplikasi,"<jats:p>In order to adapt with evolving requirements and perform continuous software maintenance, it is essential for the software developers to understand the content of user feedback. User feedback such as bug report could provide so much information regarding the product from user's point of view, especially parts that need improvements. However, it is often difficult to read all the feedback for products with enormous number of users as manually reading and analyzing each feedback could take too much time and effort. This research aims to develop a model for automatic feedback classification by implementing Support Vector Machine for the classifier's algorithm and Chi-square method for feature selection. The model is developed using Python programming language and is then evaluated under different scenarios in order to measure its performance. Using a ratio of training and testing set of 80:20, our model achieved 77% accuracy, 50% precision, 55% recall, and 73% F1-score with 6.63 critical value and C=100 and gamma 0.001 as the SVM hyperparameters.</jats:p>",https://doi.org/10.31937/ti.v12i2.1828,CrossRef
Information Technology,User Support,Quantitative,Expert Consensus Survey on Digital Health Tools for Patients With Serious Mental Illness: Optimizing for User Characteristics and User Support,"<jats:sec>
            <jats:title>Background</jats:title>
            <jats:p>Digital technology is increasingly being used to enhance health care in various areas of medicine. In the area of serious mental illness, it is important to understand the special characteristics of target users that may influence motivation and competence to use digital health tools, as well as the resources and training necessary for these patients to facilitate the use of this technology.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Objective</jats:title>
            <jats:p>The aim of this study was to conduct a quantitative expert consensus survey to identify key characteristics of target users (patients and health care professionals), barriers and facilitators for appropriate use, and resources needed to optimize the use of digital health tools in patients with serious mental illness.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Methods</jats:title>
            <jats:p>A panel of 40 experts in digital behavioral health who met the participation criteria completed a 19-question survey, rating predefined responses on a 9-point Likert scale. Consensus was determined using a chi-square test of score distributions across three ranges (1-3, 4-6, 7-9). Categorical ratings of first, second, or third line were designated based on the lowest category into which the CI of the mean ratings fell, with a boundary &gt;6.5 for first line. Here, we report experts’ responses to nine questions (265 options) that focused on (1) user characteristics that would promote or hinder the use of digital health tools, (2) potential benefits or motivators and barriers or unintended consequences of digital health tool use, and (3) support and training for patients and health care professionals.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Results</jats:title>
            <jats:p>Among patient characteristics most likely to promote use of digital health tools, experts endorsed interest in using state-of-the-art technology, availability of necessary resources, good occupational functioning, and perception of the tool as beneficial. Certain disease-associated signs and symptoms (eg, more severe symptoms, substance abuse problems, and a chaotic living situation) were considered likely to make it difficult for patients to use digital health tools. Enthusiasm among health care professionals for digital health tools and availability of staff and equipment to support their use were identified as variables to enable health care professionals to successfully incorporate digital health tools into their practices. The experts identified a number of potential benefits of and barriers to use of digital health tools by patients and health care professionals. Experts agreed that both health care professionals and patients would need to be trained in the use of these new technologies.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Conclusions</jats:title>
            <jats:p>These results provide guidance to the mental health field on how to optimize the development and deployment of digital health tools for patients with serious mental illness.</jats:p>
          </jats:sec>",https://doi.org/10.2196/mental.9777,CrossRef
Information Technology,User Support,Quantitative,"The Effect of Enterprise Resource Planning (ERP) System Implementation, User Training, and Management Support on User Satisfaction in Manufacturing Companies","<jats:p>This study examines the impact of Enterprise Resource Planning (ERP) system implementation, User Training, and Management Support on User Satisfaction within manufacturing companies. Using a quantitative approach, data were collected from 160 respondents and analyzed using Structural Equation Modeling-Partial Least Squares (SEM-PLS). The results indicate that all three factors—ERP system implementation, User Training, and Management Support—have a positive and significant effect on User Satisfaction. Among these, User Training emerged as the most influential factor, followed by Management Support and ERP system implementation. These findings underscore the critical role of human-centric factors in ensuring successful ERP adoption and highlight the importance of comprehensive training and strong management involvement. The study offers practical insights for organizations aiming to enhance user satisfaction with ERP systems, emphasizing the need for a holistic approach that integrates technical and human factors.</jats:p>",https://doi.org/10.58812/wsist.v2i02.1209,CrossRef
Information Technology,User Support,Quantitative,User Engagement Within an Online Peer Support Community (Depression Connect) and Recovery-Related Changes in Empowerment: Longitudinal User Survey,"<jats:sec>
            <jats:title>Background</jats:title>
            <jats:p>The chronic nature of depression and limited availability of evidence-based treatments emphasize the need for complementary recovery-oriented services, such as peer support interventions (PSIs). Peer support is associated with positive effects on clinical and personal recovery from mental illness, but little is known about the processes of engagement that foster change, and studies targeting individuals with depression specifically are limited.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Objective</jats:title>
            <jats:p>This study aimed to evaluate whether the level of user engagement, assessed on several dimensions, in an online peer support community for individuals with depression promotes empowerment and the use of self-management strategies and reduces symptom severity and disability.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Methods</jats:title>
            <jats:p>In a longitudinal survey conducted from June 2019 to September 2020, we analyzed the data of the users of Depression Connect (DC), an online peer support community hosted by the Dutch Patient Association for Depression and the Pro Persona Mental Health Care institute, on measures of empowerment, self-management, depression, and disability. Of the 301 respondents, 49 (16.3%) respondents completed the survey again after 3 months and 74 (24.6%) respondents, after 6 months. Analysis of 3 parameters (ie, total time spent on the platform, number of page views, and number of posts) derived from their data logs yielded 4 engagement profiles. Linear mixed models were fitted to determine whether the outcomes had significantly changed over time and differed for the various profiles.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Results</jats:title>
            <jats:p>Baseline engagement with the online peer support community was “very low” (177/301, 58.8%) or “low” (87/301, 28.9%) for most of the participants, with few showing “medium” (30/301, 9.9%) or “high” engagement patterns (7/301, 2.3%), while user profiles did not differ in demographic and clinical characteristics. Empowerment, self-management, depressive symptoms, and disability improved over time, but none were associated with the intensity or nature of user engagement.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Conclusions</jats:title>
            <jats:p>With most DC members showing very low to low engagement and only a few being identified as high-engaged users, it is likely that this flexibility in use frequency is what provides value to online PSI users. In other more formal supportive environments for depression, a certain level of engagement is predetermined either by their organizational or by their societal context; at DC, users can adapt the intensity and nature of their engagement to their current needs on their personal road to recovery. This study added to the current knowledge base on user engagement for PSIs because previous studies targeting depression with an online format focused on active users, precluding passive and flexible engagement. Future studies should explore the content and quality of the interactions in online PSIs to identify optimal user engagement as a function of current, self-reported clinical parameters and reasons to engage in the PSI.</jats:p>
          </jats:sec>",https://doi.org/10.2196/39912,CrossRef
Information Technology,User Support,Quantitative,Anwenderunterstützung bei der digitalisierten Produktionsplanung/User support in digital production planning,"<p>In der Produktionsplanung kommt vermehrt Simulationssoftware zum Einsatz, um Zeit und Kosten zu reduzieren. Viele Anwender benutzen die Programme allerdings nur gelegentlich. Aufgrund des relativ geringen Übungsgrades besteht ein hoher Bedarf an Unterstützung. In marktgängigen Softwarewerkzeugen sind jedoch bislang kaum wirksame Unterstützungsfunktionen implementiert. Basierend auf Anwenderbefragungen in Unternehmen innerhalb des europäischen Forschungsprojektes DREAM (simulation based application Decision support in Real-time for Efficient Agile Manufacturing) werden drei Konzepte für die Benutzerunterstützung vorgeschlagen.</p>
          <p>&amp;nbsp;</p>
          <p>Production planning is increasingly performed by the application of simulation software in order to reduce costs and time. But many users use this kind of software just occasionally. Due to this low level of usage, there is a great demand in support. For those software packages, useful and efficient support tools were not implemented until now. Based on surveys realized in some companies in the European project DREAM three different concepts of user support are suggested.</p>",https://doi.org/10.37544/1436-4980-2016-04-59,CrossRef
Information Technology,User Support,Qualitative,Public user innovation: exploring the support mechanisms and user roles in a public organisation,"<jats:sec><jats:title content-type=""abstract-subheading"">Purpose</jats:title><jats:p>This article expands literature on user innovation by exploring the mechanisms that support user innovations in the context of a public organisation. Research has hitherto documented support mechanisms for user innovation in producer companies, where users contribute in early or temporary innovation phases as external non-employees or lead-users engaged by the producer. Complementarily, this paper explores a lesser known area of support mechanisms, those that support internal user innovations in a public sector setting.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title><jats:p>Employing a qualitative study of a Norwegian public hospital at the interface between users (personnel and patients) and organisational support (facilitators who orchestrate user innovations), this article analyses in-house user innovation based on observations, text documentation and interviews over a four-year period.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Findings</jats:title><jats:p>In this public hospital, holistic organisational facilitation of “public user innovators” formed the key support mechanism built on “people” (facilitating co-creation), “process” (facilitating ideas, project realisation and implementation) and “coordination” (facilitating systems and communication). The findings show that public and producer organisational mechanisms both resemble and differ in many respects, as illustrated by the framework developed to describe these characteristics, such as that producers insource users, while the public organisation outsources production.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Originality/value</jats:title><jats:p>The originality of the article lies in the identification and description of “public user innovation”, a new term developed from this study of a public organisation in contrast to the dominant literature on producer companies. This article contributes new insights by differentiating the roles of user innovators and the mechanisms that support such innovations. New implications are drawn from the public side of organisational support in user innovation research.</jats:p></jats:sec>",https://doi.org/10.1108/ejim-04-2022-0217,CrossRef
Information Technology,User Support,Qualitative,Integrating Key User Characteristics in User-Centered Design of Digital Support Systems for Seniors’ Physical Activity Interventions to Prevent Falls: Protocol for a Usability Study,"<jats:sec>
            <jats:title>Background</jats:title>
            <jats:p>The goal of user-centered design (UCD) is to understand the users’ perspective and to use that knowledge to shape more effective solutions. The UCD approach provides insight into users’ needs and requirements and thereby improves the design of the developed services. However, involving users in the development process does not guarantee that feedback from different subgroups of users will shape the development in ways that will make the solutions more useful for the entire target user population.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Objective</jats:title>
            <jats:p>The aim of this study was to describe a protocol for systematic analysis and prioritization of feedback from user subgroups in the usability testing of a digital motivation support for fall-preventive physical activity (PA) interventions in seniors (aged 65 years and older). This protocol can help researchers and developers to systematically exploit feedback from relevant user subgroups in UCD.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Methods</jats:title>
            <jats:p>Gender, PA level, and level of technology experience have been identified in the literature to influence users’ experience and use of digital support systems for fall-preventive PA interventions in seniors. These 3 key user characteristics were dichotomized and used to define 8 (ie, 23) possible user subgroups. The presented method enables systematic tracking of the user subgroups’ contributions in iterative development. The method comprises (1) compilation of difficulties and deficiencies in the digital applications identified in usability testing, (2) clustering of the identified difficulties and deficiencies, and (3) prioritization of deficiencies to be rectified. Tracking user subgroup representation in the user feedback ensures that the development process is prioritized according to the needs of different subgroups. Mainly qualitative data collection methods are used.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Results</jats:title>
            <jats:p>A protocol was developed to ensure that feedback from users representing all possible variants of 3 selected key user characteristics (gender, PA level, and level of technology experience) is considered in the iterative usability testing of a digital support for seniors’ PA. The method was applied in iterative usability testing of two digital applications during spring/summer 2018. Results from the study on the users’ experiences and the iterative modification of the digital applications are expected to be published during 2021.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>Conclusions</jats:title>
            <jats:p>Methods for systematic collection, analysis, and prioritization of feedback from user subgroups might be particularly important in heterogenous user groups (eg, seniors). This study can contribute to identifying and improving the understanding of potential differences between user subgroups of seniors in their use and experiences of digital support for fall-preventive PA interventions. This knowledge may be relevant for developing digital support systems that are appropriate, useful, and attractive to users and for enabling the design of digital support systems that target specific user subgroups (ie, tailoring of the support). The protocol needs to be further used and investigated in order to validate its potential value.</jats:p>
          </jats:sec>
          <jats:sec>
            <jats:title>International Registered Report Identifier (IRRID)</jats:title>
            <jats:p>RR1-10.2196/20061</jats:p>
          </jats:sec>",https://doi.org/10.2196/20061,CrossRef
Information Technology,User Support,Qualitative,Beyond formalized plans: User involvement in support in daily living – users’ and support workers’ experiences,"<jats:sec><jats:title>Background:</jats:title><jats:p> User involvement, based on respect and carried out through dialogue, has been shown to lead to increased self-respect, self-confidence and positive identity. In Sweden, the Social Service Act requires that interventions be designed and implemented together with the individual concerned. The basic criterion for social support is prolonged severe mental illness (usually at least 6 months), with no criteria for specific diagnosis or institutional history. The most common form of social support is ‘support in daily living’, a community care intervention for people aged 18 years or older who have their own homes and living arrangements. </jats:p></jats:sec><jats:sec><jats:title>Aim:</jats:title><jats:p> This article aims to deepen our understanding of user involvement at the individual level in the provision of an ongoing social work intervention. What elements of user involvement can be found in users’ and support workers’ descriptions of helpful support in daily living? </jats:p></jats:sec><jats:sec><jats:title>Method:</jats:title><jats:p> Qualitative interviews were conducted with 18 users, who had experienced support in daily living as helpful, and 16 interviews with the users’ support workers. </jats:p></jats:sec><jats:sec><jats:title>Results:</jats:title><jats:p> Three major, interconnected themes emerged: Constant dialogue; Framing the flexibility, in relation to formalized intervention plans and regulations; The importance of ‘small things’, decisions concerning daily life. </jats:p></jats:sec><jats:sec><jats:title>Conclusion:</jats:title><jats:p> Both users and support workers described user involvement at the individual, micro-level to be an integral part of helpful support in daily living. It was possible to create a space for dialogue and co-creation in which users were involved in formulating and deciding the contents of their support at an informal level, to influence their own everyday lives. While a formal framework of rules, restrictions and plans surrounds meetings between users and professionals, a facilitating factor may be the absence of too detailed plans and regulations, leaving trust to users and professionals and their capacity to manage most of the choices they have to make. </jats:p></jats:sec>",https://doi.org/10.1177/0020764019894603,CrossRef
Information Technology,User Support,Qualitative,WEARABLES IN LONG-TERM DEMENTIA RESEARCH: A MIXED METHOD STUDY OF USER EXPERIENCES AND SUPPORT NEEDS,"<jats:title>Abstract</jats:title>
               <jats:p>Passive wearables data collection may be specifically beneficial to aging research featuring dementia populations, who have caregiving and cognitive burdens that can make study participation and reliable data collection more difficult, especially as dementia progresses. This three-phase project aims to inform best practice recommendations to enhance recruitment and adherence in long-term wearables research featuring this population. Based on our systematic review and preliminary in-house data testing, we selected three wearables offering different capabilities and form (from Garmin, Pulse HR, and AngelSense) to test real world usability, data quality, and support needs. This is the first study to recruit persons living with dementia and their caregivers to evaluate multiple devices outside of a laboratory or focus group setting (N=12 dyads). The person living with dementia assented to wearing each wearable for two weeks. Their caregiver rated many facets of each device following its use with the Quebec User Evaluation of Satisfaction with Assistive Technology measure. Open-ended questions and a cumulative semi-structured interview provided context and in-depth comparative perspectives of their experiences in the study. Wearable durability, simplicity, and data availability/monitoring capacity were important to participant favorability and adherence. Technical help and check-ins were also deemed highly valuable. Data indicate how the devices suited the dyads’ needs or caused issues, as well as how study staff could better support ongoing use. Collectively, our findings suggest ideal criteria to guide wearable selection and key protocol factors that can enhance participant recruitment and adherence in long-term dementia research.</jats:p>",https://doi.org/10.1093/geroni/igae098.3187,CrossRef
Information Technology,User Support,Qualitative,Analyzing User Costs In a Hospital: Methodological Implication of Space Syntax to Support Whole-life Target Value Design,"<jats:p>Research Hypothesis:  H1: Space Syntax analysis can be used to simulate a user’s experience and movement for investigating design alternatives in the design of healthcare facilities.  H2: Space Syntax can efficiently be used to support Whole-life Target Value Design (TVD).  Purpose: This paper investigates a methodological implication of Space Syntax to Whole- life TVD in the design of healthcare facilities.  Research Design/Method: Three hypothetical hospital ward design alternatives are selected – shallow-plan, deep-plan, and courtyard-plan type – to analyze user costs in hospital design to determine which alternative is the most cost-efficient. These three hypothetical design alternatives are evaluated using a Space Syntax program, and then the findings are interpreted to determine user costs.  Findings: The study finds that the deep-plan type has four “low” scores, the shallow-plan type has three “high” and one “medium” score, and the courtyard type has two “high” scores and two “medium” scores. Thus, the deep-plan type is determined to be the lowest user cost type, and the shallow-plan type is expected to have the highest user costs.  Limitations: User costs are discussed in qualitative basis such as high, medium, or low with proportion to the simulation due to the lack of empirical evidence in financial value.  Implications: Space Syntax assures valid results of spatial analysis in relation to users’ movement within the built environment.  Value for practitioners: Space Syntax allows designers to visually compare design alternatives relating to space planning during set-based design using spatial analysis applications.</jats:p>",https://doi.org/10.60164/a7g7i1d4c,CrossRef
Information Technology,User Support,Mixed Methods,"Reaction or Speculation: Building Computational Support for Users in
  Catching-Up Series Based on an Emerging Media Consumption Phenomenon","A growing number of people are using catch-up TV services rather than
watching simultaneously with other audience members at the time of broadcast.
However, computational support for such catching-up users has not been well
explored. In particular, we are observing an emerging phenomenon in online
media consumption experiences in which speculation plays a vital role. As the
phenomenon of speculation implicitly assumes simultaneity in media consumption,
there is a gap for catching-up users, who cannot directly appreciate the
consumption experiences. This conversely suggests that there is potential for
computational support to enhance the consumption experiences of catching-up
users. Accordingly, we conducted a series of studies to pave the way for
developing computational support for catching-up users. First, we conducted
semi-structured interviews to understand how people are engaging with
speculation during media consumption. As a result, we discovered the
distinctive aspects of speculation-based consumption experiences in contrast to
social viewing experiences sharing immediate reactions that have been discussed
in previous studies. We then designed two prototypes for supporting catching-up
users based on our quantitative analysis of Twitter data in regard to reaction-
and speculation-based media consumption. Lastly, we evaluated the prototypes in
a user experiment and, based on its results, discussed ways to empower
catching-up users with computational supports in response to recent
transformations in media consumption.",http://arxiv.org/abs/2102.06422v1,arXiv
Information Technology,User Support,Mixed Methods,"Questionnaires and Qualitative Feedback Methods to Measure User
  Experience in Mixed Reality","Evaluating the user experience of a software system is an essential final
step of every research. Several concepts such as flow, affective state,
presences, or immersion exist to measure user experience. Typical measurement
techniques analyze physiological data, gameplay data, and questionnaires.
Qualitative feedback methods are another approach to collect detailed user
insights. In this position paper, we will discuss how we used questionnaires
and qualitative feedback methods in previous mixed reality work to measure user
experience. We will present several measurement examples, discuss their current
limitations, and provide guideline propositions to support comparable mixed
reality user experience research in the future.",http://arxiv.org/abs/2104.06221v1,arXiv
Information Technology,User Support,Mixed Methods,A Framework To Improve User Story Sets Through Collaboration,"Agile methodologies have become increasingly popular in recent years. Due to
its inherent nature, agile methodologies involve stakeholders with a wide range
of expertise and require interaction between them, relying on collaboration and
customer involvement. Hence, agile methodologies encourage collaboration
between all team members so that more efficient and effective processes are
maintained. Generating requirements can be challenging, as it requires the
participation of multiple stakeholders who describe various aspects of the
project and possess a shared understanding of essential concepts. One simple
method for capturing requirements using natural language is through user
stories, which document the agreed-upon properties of a project. Stakeholders
try to strive for completeness while generating user stories, but the final
user story set may still be flawed. To address this issue, we propose SCOUT:
Supporting Completeness of User Story Sets, which employs a natural language
processing pipeline to extract key concepts from user stories and construct a
knowledge graph by connecting related terms. The knowledge graph and different
heuristics are then utilized to enhance the quality and completeness of the
user story sets by generating suggestions for the stakeholders. We perform a
user study to evaluate SCOUT and demonstrate its performance in constructing
user stories. The quantitative and qualitative results indicate that SCOUT
significantly enhance the quality and completeness of the user story sets. Our
contribution is threefold. First, we develop heuristics to suggest new concepts
to include in user stories by considering both the individuals' and other team
members' contributions. Second, we implement an open-source collaborative tool
to support writing user stories and ensuring their quality. Third, we share the
experimental setup and materials.",http://arxiv.org/abs/2301.10070v1,arXiv
Information Technology,User Support,Mixed Methods,"Exploring User Acceptance Of Portable Intelligent Personal Assistants: A
  Hybrid Approach Using PLS-SEM And fsQCA","This research explores the factors driving user acceptance of Rabbit R1, a
newly developed portable intelligent personal assistant (PIPA) that aims to
redefine user interaction and control. The study extends the technology
acceptance model (TAM) by incorporating artificial intelligence-specific
factors (conversational intelligence, task intelligence, and perceived
naturalness), user interface design factors (simplicity in information design
and visual aesthetics), and user acceptance and loyalty. Using a purposive
sampling method, we gathered data from 824 users in the US and analyzed the
sample through partial least squares structural equation modeling (PLS-SEM) and
fuzzy set qualitative comparative analysis (fsQCA). The findings reveal that
all hypothesized relationships, including both direct and indirect effects, are
supported. Additionally, fsQCA supports the PLS-SEM findings and identifies
three configurations leading to high and low user acceptance. This research
enriches the literature and provides valuable insights for system designers and
marketers of PIPAs, guiding strategic decisions to foster widespread adoption
and long-term engagement.",http://arxiv.org/abs/2408.17119v1,arXiv
Information Technology,User Support,Mixed Methods,"PromptCharm: Text-to-Image Generation through Multi-modal Prompting and
  Refinement","The recent advancements in Generative AI have significantly advanced the
field of text-to-image generation. The state-of-the-art text-to-image model,
Stable Diffusion, is now capable of synthesizing high-quality images with a
strong sense of aesthetics. Crafting text prompts that align with the model's
interpretation and the user's intent thus becomes crucial. However, prompting
remains challenging for novice users due to the complexity of the stable
diffusion model and the non-trivial efforts required for iteratively editing
and refining the text prompts. To address these challenges, we propose
PromptCharm, a mixed-initiative system that facilitates text-to-image creation
through multi-modal prompt engineering and refinement. To assist novice users
in prompting, PromptCharm first automatically refines and optimizes the user's
initial prompt. Furthermore, PromptCharm supports the user in exploring and
selecting different image styles within a large database. To assist users in
effectively refining their prompts and images, PromptCharm renders model
explanations by visualizing the model's attention values. If the user notices
any unsatisfactory areas in the generated images, they can further refine the
images through model attention adjustment or image inpainting within the rich
feedback loop of PromptCharm. To evaluate the effectiveness and usability of
PromptCharm, we conducted a controlled user study with 12 participants and an
exploratory user study with another 12 participants. These two studies show
that participants using PromptCharm were able to create images with higher
quality and better aligned with the user's expectations compared with using two
variants of PromptCharm that lacked interaction or visualization support.",http://arxiv.org/abs/2403.04014v1,arXiv
Information Technology,User Support,Design and Development,Theory-based habit modeling for enhancing behavior prediction in behavior change support systems,"<jats:title>Abstract</jats:title><jats:p>Psychological theories of habit posit that when a strong habit is formed through behavioral repetition, it can trigger behavior automatically in the same environment. Given the reciprocal relationship between habit and behavior, changing lifestyle behaviors is largely a task of breaking old habits and creating new and healthy ones. Thus, representing users’ habit strengths can be very useful for behavior change support systems, for example, to predict behavior or to decide when an intervention reaches its intended effect. However, habit strength is not directly observable and existing self-report measures are taxing for users. In this paper, building on recent computational models of habit formation, we propose a method to enable intelligent systems to compute habit strength based on observable behavior. The hypothesized advantage of using computed habit strength for behavior prediction was tested using data from two intervention studies on dental behavior change (<jats:inline-formula><jats:alternatives><jats:tex-math>$$N = 36$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>36</mml:mn></mml:mrow></mml:math></jats:alternatives></jats:inline-formula>and<jats:inline-formula><jats:alternatives><jats:tex-math>$$N = 75$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>75</mml:mn></mml:mrow></mml:math></jats:alternatives></jats:inline-formula>), where we instructed participants to brush their teeth twice a day for three weeks and monitored their behaviors using accelerometers. The results showed that for the task of predicting future brushing behavior, the theory-based model that computed habit strength achieved an accuracy of 68.6% (Study 1) and 76.1% (Study 2), which outperformed the model that relied on self-reported behavioral determinants but showed no advantage over models that relied on past behavior. We discuss the implications of our results for research on behavior change support systems and habit formation.</jats:p>",https://doi.org/10.1007/s11257-022-09326-x,CrossRef
Information Technology,User Support,Design and Development,Personalized home-care support for the elderly: a field experience with a social robot at home,"<jats:title>Abstract</jats:title><jats:p>Socially assistive robotics (SAR) is getting a lot of attention for its potential in assisting elderly users. However, for robotic assistive applications to be effective, they need to satisfy the particular needs of each user and be well perceived. For this purpose, a personalization based on user’s characteristics such as personality and cognitive profile, and their dynamic changes is a crucial factor. Moreover, most of the existing solutions rely on the availability of specific technological infrastructures, generally requiring high economic investment, and that cannot be easily placed in different environments. Personalization and adaptation of assistive robotics applications to different user’s characteristics and needs, and even to different technological environments, are still not fully addressed in real environments. In the present work, the results of the UPA4SAR project are presented. The project aimed at providing a social robotic system to deliver assistive tasks for home care of patients with mild cognitive impairment in a personalized and adaptive way. We introduce the general architecture of the system and the developed robotic behaviors. Personalization and dynamic adaptation of assistive tasks are realized using a service-oriented approach by taking into account both user’s characteristics and environmental dynamic conditions. Field experimentation of the project was carried out with 7 patients, using the robotic system autonomously running in their homes for a total of 118 days. Results showed a reliable functioning of the proposed robotic system, a generally positive reaction, and a good acceptability rate from patients.</jats:p>",https://doi.org/10.1007/s11257-022-09333-y,CrossRef
Information Technology,User Support,Design and Development,Using scaffolding to formalize digital coach support for low-literate learners,"<jats:title>Abstract</jats:title><jats:p>In this study, we attempt to specify the cognitive support behavior of a previously designed embodied conversational agent coach that provides learning support to low-literates. Three knowledge gaps are identified in the existing work: an incomplete specification of the behaviors that make up ‘support,’ an incomplete specification of how this support can be personalized, and unclear speech recognition rules. We use the socio-cognitive engineering method to update our foundation of knowledge with new online banking exercises, low-level scaffolding and user modeling theory, and speech recognition. We then refine the design of our coach agent by creating comprehensive cognitive support rules that adapt support based on learner needs (the ‘Generalized’ approach) and attune the coach’s support delay to user performance in previous exercises (the ‘Individualized’ approach). A prototype is evaluated in a 3-week within- and between-subjects experiment. Results show that the specified cognitive support is effective: Learners complete all exercises, interact meaningfully with the coach, and improve their online banking self-efficacy. Counter to hypotheses, the Individualized approach does not improve on the Generalized approach. Whether this indicates suboptimal operationalization or a deeper problem with the Individualized approach remains as future work.</jats:p>",https://doi.org/10.1007/s11257-020-09278-0,CrossRef
Information Technology,User Support,Design and Development,“Tell Me Why”: using natural language justifications in a recipe recommender system to support healthier food choices,"<jats:title>Abstract</jats:title><jats:p>Users of online recipe websites tend to prefer unhealthy foods. Their popularity undermines the healthiness of traditional food recommender systems, as many users lack nutritional knowledge to make informed food decisions. Moreover, the presented information is often unrelated to nutrition or difficult to understand. To alleviate this, we present a methodology to generate natural language justifications that emphasize the nutritional content, health risks, or benefits of recommended recipes. Our framework takes a user and two recipes as input and produces an automatically generated natural language justification as output, based on the user’s characteristics and the recipes’ features, following a knowledge-based recommendation approach. We evaluated our methodology in two crowdsourcing studies. In Study 1 (<jats:inline-formula><jats:alternatives><jats:tex-math>$$N=502$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"">
                  <mml:mrow>
                    <mml:mi>N</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>502</mml:mn>
                  </mml:mrow>
                </mml:math></jats:alternatives></jats:inline-formula>), we compared user food choices for two personalized recommendation approaches, based on either a (1) single-style justification or (2) comparative justification was shown, using a no justification baseline. The recommendations were either popularity-based or health-aware, the latter based on the health and nutritional needs of the user. We found that comparative justification styles were effective in supporting choices for our health-aware recommendations, confirming the impact of our methodology on food choices. In Study 2 (<jats:inline-formula><jats:alternatives><jats:tex-math>$$N=504$$</jats:tex-math><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"">
                  <mml:mrow>
                    <mml:mi>N</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>504</mml:mn>
                  </mml:mrow>
                </mml:math></jats:alternatives></jats:inline-formula>), we used the same methodology to compare the effectiveness of eight different comparative justification strategies. We presented pairs of recipes twice to users: once <jats:italic>without</jats:italic> and once <jats:italic>with</jats:italic> a pairwise justification. Results indicated that justifications led to significantly healthier choices for first course meals, while strategies that compared food features and emphasized health risks, benefits, and a user’s lifestyle were most effective, catering to health-related choice motivations.
</jats:p>",https://doi.org/10.1007/s11257-023-09377-8,CrossRef
Information Technology,User Support,Design and Development,AI-Driven Customer Support: Transforming User Experience and Operational Efficiency,"<jats:p>The integration of artificial intelligence in customer support operations represents a transformative shift in how organizations deliver service and manage customer relationships. This comprehensive article examines how AI technologies, particularly Large Language Models, Case Deflection Systems, and Predictive Suggestion Systems, are revolutionizing traditional support paradigms. The article investigates the impact of AI implementation across multiple dimensions, including operational efficiency, customer satisfaction, cost optimization, and service quality. Through empirical evidence and case studies, this article demonstrates how AI-driven support solutions address traditional challenges while creating new opportunities for enhanced customer engagement. The findings reveal significant improvements in areas such as response time, accuracy, self-service capabilities, and agent productivity, while highlighting the importance of structured implementation approaches and ongoing optimization strategies.</jats:p>",https://doi.org/10.71097/ijsat.v16.i1.2600,CrossRef
Information Technology,User Support,Theoretical / Conceptual,Conceptual framework of intelligent decision support based on user digital life traces and ontology-based user categorisation,"<jats:title>Abstract</jats:title>
               <jats:p>The paper presents conceptual framework and information model of intelligent decision support based on traces of user digital lives and ontology-based user categorisation. The conceptual framework defines components that provide information revealed from the traces of user digital lives, generalize this information, and make ontology inference. The information model defines information flows between the components of the conceptual framework. The novelties of this research are grouping users with common preferences and decision making behaviours based on the user digital traces, and context-sensitive ontology-based categorization of users into the user groups.</jats:p>",https://doi.org/10.1088/1742-6596/1801/1/012005,CrossRef
Information Technology,User Support,Theoretical / Conceptual,Making Order in User Experience Research to Support Its Application in Design and Beyond,"<jats:p>The term User Experience (UX) was introduced to define the dynamics of the human-product interaction, and it was thought that design would have been a main recipient of UX research. However, it can be claimed that the outcomes of UX studies were not seamlessly transferred into design research and practice. Among the possible reasons, this paper addresses the fragmentary knowledge ascribable to the field of UX. The authors reviewed the literature analyzing the conceptual contributions that interpret UX, proposing definitions and/or a theoretical framework. This allowed the authors to provide an overview of recurring elements of UX, highlighting their relationships and affecting factors. This research aims to clarify the overall understanding of UX, along with its key components (the user, interaction, the system, and context) and dimensions (ergonomic, affective, and the cognitive experiences). The authors built a semantic construction inspired by the structure of a grammatical sentence to highlight the relationship between those components. Therefore, UX is defined by a subject/user who performs an action-interaction towards an object-system. A complement-context better defines the condition(s) where the action-interaction takes place. This work is expected to lay the foundations for the understanding of approaches and methods employed in UX studies, especially in design.</jats:p>",https://doi.org/10.3390/app11156981,CrossRef
Information Technology,User Support,Theoretical / Conceptual,User-Centered Evaluation Framework to Support the Interaction Design for Augmented Reality Applications,"<jats:p>The advancement of Augmented Reality (AR) technology has been remarkable, enabling the augmentation of user perception with timely information. This progress holds great promise in the field of interaction design. However, the mere advancement of technology is not enough to ensure widespread adoption. The user dimension has been somewhat overlooked in AR research due to a lack of attention to user motivations, needs, usability, and perceived value. The critical aspects of AR technology tend to be overshadowed by the technology itself. To ensure appropriate future assessments, it is necessary to thoroughly examine and categorize all the methods used for AR technology validation. By identifying and classifying these evaluation methods, researchers and practitioners will be better equipped to develop and validate new AR techniques and applications. Therefore, comprehensive and systematic evaluations are critical to the advancement and sustainability of AR technology. This paper presents a theoretical framework derived from a cluster analysis of the most efficient evaluation methods for AR extracted from 399 papers. Evaluation methods were clustered according to the application domains and the human–computer interaction aspects to be investigated. This framework should facilitate rapid development cycles prioritizing user requirements, ultimately leading to groundbreaking interaction methods accessible to a broader audience beyond research and development centers.</jats:p>",https://doi.org/10.3390/mti8050041,CrossRef
Information Technology,User Support,Theoretical / Conceptual,Geosensors to Support Crop Production: Current Applications and User Requirements,"<jats:p>Sensor technology, which benefits from high temporal measuring resolution, real-time data transfer and high spatial resolution of sensor data that shows in-field variations, has the potential to provide added value for crop production. The present paper explores how sensors and sensor networks have been utilised in the crop production process and what their added-value and the main bottlenecks are from the perspective of users. The focus is on sensor based applications and on requirements that users pose for them. Literature and two use cases were reviewed and applications were classified according to the crop production process: sensing of growth conditions, fertilising, irrigation, plant protection, harvesting and fleet control. The potential of sensor technology was widely acknowledged along the crop production chain. Users of the sensors require easy-to-use and reliable applications that are actionable in crop production at reasonable costs. The challenges are to develop sensor technology, data interoperability and management tools as well as data and measurement services in a way that requirements can be met, and potential benefits and added value can be realized in the farms in terms of higher yields, improved quality of yields, decreased input costs and production risks, and less work time and load.</jats:p>",https://doi.org/10.3390/s110706656,CrossRef
Information Technology,User Support,Theoretical / Conceptual,Transfer climate in end‐user computing,"<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>Although end‐user computing (EUC) training has received significant attention among academics and practitioners, the effective transfer of trained EUC skills is a relatively neglected issue. Analysis of factors affecting the EUC transfer process will aid in understanding and improving training transfer. Hence, the purpose of this paper is to underscore key trainee characteristics and facets of the work environment that influence EUC training transfer.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>The theoretical framework includes prior computer experience, computer anxiety, computer self‐efficacy, pre‐training motivation and perceived job utility as significant trainee factors influencing the EUC transfer process. In addition, the model includes supervisory support as an important constituent of the EUC transfer process.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>The model highlights the mediating roles of computer self‐efficacy and pre‐training motivation in predicting motivation to transfer. In addition, it points out that several factors work simultaneously to influence motivation to transfer EUC training.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p>Supervisory support in the pre‐ and post‐training environment is extremely crucial in determining EUC training success. Specifically, supervisors should be able to communicate to employees the purpose and importance of training, the relevance of computer training to their jobs and the outcomes expected.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>This paper contributes to the literature by emphasizing the importance of supervisory support and individual characteristics in predicting motivation to transfer.</jats:p></jats:sec>",https://doi.org/10.1108/09727980910972181,CrossRef
Computer Engineering,Hardware Systems,Quantitative,"Transformers for Secure Hardware Systems: Applications, Challenges, and
  Outlook","The rise of hardware-level security threats, such as side-channel attacks,
hardware Trojans, and firmware vulnerabilities, demands advanced detection
mechanisms that are more intelligent and adaptive. Traditional methods often
fall short in addressing the complexity and evasiveness of modern attacks,
driving increased interest in machine learning-based solutions. Among these,
Transformer models, widely recognized for their success in natural language
processing and computer vision, have gained traction in the security domain due
to their ability to model complex dependencies, offering enhanced capabilities
in identifying vulnerabilities, detecting anomalies, and reinforcing system
integrity. This survey provides a comprehensive review of recent advancements
on the use of Transformers in hardware security, examining their application
across key areas such as side-channel analysis, hardware Trojan detection,
vulnerability classification, device fingerprinting, and firmware security.
Furthermore, we discuss the practical challenges of applying Transformers to
secure hardware systems, and highlight opportunities and future research
directions that position them as a foundation for next-generation
hardware-assisted security. These insights pave the way for deeper integration
of AI-driven techniques into hardware security frameworks, enabling more
resilient and intelligent defenses.",http://arxiv.org/abs/2505.22605v1,arXiv
Computer Engineering,Hardware Systems,Quantitative,Information Flow Coverage Metrics for Hardware Security Verification,"Security graphs model attacks, defenses, mitigations, and vulnerabilities on
computer networks and systems. With proper attributes, they provide security
metrics using standard graph algorithms. A hyperflow graph is a
register-transfer level (RTL) hardware security graph that facilitates security
verification. A hyperflow graph models information flows and is annotated with
attributes that allow security metrics to measure flow paths, flow conditions,
and flow rates. Hyperflow graphs enable the understanding of hardware
vulnerabilities related to confidentiality, integrity, and availability, as
shown on the OpenTitan hardware root of trust under several threat models.",http://arxiv.org/abs/2304.08263v1,arXiv
Computer Engineering,Hardware Systems,Quantitative,The Emergence of Hardware Fuzzing: A Critical Review of its Significance,"In recent years, there has been a notable surge in attention towards hardware
security, driven by the increasing complexity and integration of processors,
SoCs, and third-party IPs aimed at delivering advanced solutions. However, this
complexity also introduces vulnerabilities and bugs into hardware systems,
necessitating early detection during the IC design cycle to uphold system
integrity and mitigate re-engineering costs. While the Design Verification (DV)
community employs dynamic and formal verification strategies, they encounter
challenges such as scalability for intricate designs and significant human
intervention, leading to prolonged verification durations. As an alternative
approach, hardware fuzzing, inspired by software testing methodologies, has
gained prominence for its efficacy in identifying bugs within complex hardware
designs. Despite the introduction of various hardware fuzzing techniques,
obstacles such as inefficient conversion of hardware modules into software
models impede their effectiveness. This Systematization of Knowledge (SoK)
initiative delves into the fundamental principles of existing hardware fuzzing,
methodologies, and their applicability across diverse hardware designs.
Additionally, it evaluates factors such as the utilization of golden reference
models (GRMs), coverage metrics, and toolchains to gauge their potential for
broader adoption, akin to traditional formal verification methods. Furthermore,
this work examines the reliability of existing hardware fuzzing techniques in
identifying vulnerabilities and identifies research gaps for future
advancements in design verification techniques.",http://arxiv.org/abs/2403.12812v1,arXiv
Computer Engineering,Hardware Systems,Quantitative,"Explainability as a Requirement for Hardware: Introducing Explainable
  Hardware (XHW)","In today's age of digital technology, ethical concerns regarding computing
systems are increasing. While the focus of such concerns currently is on
requirements for software, this article spotlights the hardware domain,
specifically microchips. For example, the opaqueness of modern microchips
raises security issues, as malicious actors can manipulate them, jeopardizing
system integrity. As a consequence, governments invest substantially to
facilitate a secure microchip supply chain. To combat the opaqueness of
hardware, this article introduces the concept of Explainable Hardware (XHW).
Inspired by and building on previous work on Explainable AI (XAI) and
explainable software systems, we develop a framework for achieving XHW
comprising relevant stakeholders, requirements they might have concerning
hardware, and possible explainability approaches to meet these requirements.
Through an exploratory survey among 18 hardware experts, we showcase
applications of the framework and discover potential research gaps. Our work
lays the foundation for future work and structured debates on XHW.",http://arxiv.org/abs/2302.14661v2,arXiv
Computer Engineering,Hardware Systems,Quantitative,NLS: Natural-Level Synthesis for Hardware Implementation Through GenAI,"This paper introduces Natural-Level Synthesis, an innovative approach for
generating hardware using generative artificial intelligence on both the system
level and component-level. NLS bridges a gap in current hardware development
processes, where algorithm and application engineers' involvement typically
ends at the requirements stage. With NLS, engineers can participate more deeply
in the development, synthesis, and test stages by using Gen-AI models to
convert natural language descriptions directly into Hardware Description
Language code. This approach not only streamlines hardware development but also
improves accessibility, fostering a collaborative workflow between hardware and
algorithm engineers. We developed the NLS tool to facilitate natural
language-driven HDL synthesis, enabling rapid generation of system-level HDL
designs while significantly reducing development complexity. Evaluated through
case studies and benchmarks using Performance, Power, and Area metrics, NLS
shows its potential to enhance resource efficiency in hardware development.
This work provides a extensible, efficient solution for hardware synthesis and
establishes a Visual Studio Code Extension to assess Gen-AI-driven HDL
generation and system integration, laying a foundation for future AI-enhanced
and AI-in-the-loop Electronic Design Automation tools.",http://arxiv.org/abs/2504.01981v1,arXiv
Computer Engineering,Hardware Systems,Qualitative,Generic Low-Latency Masking in Hardware,"<jats:p>In this work, we introduce a generalized concept for low-latency masking that is applicable to any implementation and protection order, and (in its most extreme form) does not require on-the-fly randomness. The main idea of our approach is to avoid collisions of shared variables in nonlinear circuit parts and to skip the share compression. We show the feasibility of our approach on a full implementation of a one-round unrolled Ascon variant and on an AES S-box case study. Additionally, we discuss possible trade-offs to make our approach interesting for practical implementations. As a result, we obtain a first-order masked AES S-box that is calculated in a single clock cycle with rather high implementation costs (60.7 kGE), and a two-cycle variant with much less implementation costs (6.7 kGE). The side-channel resistance of our Ascon S-box designs up to order three are then verified using the formal analysis tool of [BGI+18]. Furthermore, we introduce a taint checking based verification approach that works specifically for our low-latency approach and allows us to verify large circuits like our low-latency AES S-box design in reasonable time.</jats:p>",https://doi.org/10.46586/tches.v2018.i2.1-21,CrossRef
Computer Engineering,Hardware Systems,Qualitative,Prime-Field Masking in Hardware and its Soundness against Low-Noise SCA Attacks,"<jats:p>A recent study suggests that arithmetic masking in prime fields leads to stronger security guarantees against passive physical adversaries than Boolean masking. Indeed, it is a common observation that the desired security amplification of Boolean masking collapses when the noise level in the measurements is too low. Arithmetic encodings in prime fields can help to maintain an exponential increase of the attack complexity in the number of shares even in such a challenging context. In this work, we contribute to this emerging topic in two main directions. First, we propose novel masked hardware gadgets for secure squaring in prime fields (since squaring is non-linear in non-binary fields) which prove to be significantly more resource-friendly than corresponding masked multiplications. We then formally show their local and compositional security for arbitrary orders. Second, we attempt to &gt;experimentally evaluate the performance vs. security tradeoff of prime-field masking. In order to enable a first comparative case study in this regard, we exemplarily consider masked implementations of the AES as well as the recently proposed AESprime. AES-prime is a block cipher partially resembling the standard AES, but based on arithmetic operations modulo a small Mersenne prime. We present cost and performance figures for masked AES and AES-prime implementations, and experimentally evaluate their susceptibility to low-noise side-channel attacks. We consider both the dynamic and the static power consumption for our low-noise analyses and emulate strong adversaries. Static power attacks are indeed known as a threat for side-channel countermeasures that require a certain noise level to be effective because of the adversary’s ability to reduce the noise through intra-trace averaging. Our results show consistently that for the noise levels in our practical experiments, the masked prime-field implementations provide much higher security for the same number of shares. This compensates for the overheads prime computations lead to and remains true even if / despite leaking each share with a similar Signal-to-Noise Ratio (SNR) as their binary equivalents. We hope our results open the way towards new cipher designs tailored to best exploit the advantages of prime-field masking.</jats:p>",https://doi.org/10.46586/tches.v2023.i2.482-518,CrossRef
Computer Engineering,Hardware Systems,Qualitative,Provable Secure Parallel Gadgets,"<jats:p>Side-channel attacks are a fundamental threat to the security of cryptographic implementations. One of the most prominent countermeasures against side-channel attacks is masking, where each intermediate value of the computation is secret shared, thereby concealing the computation’s sensitive information. An important security model to study the security of masking schemes is the random probing model, in which the adversary obtains each intermediate value of the computation with some probability p. To construct secure masking schemes, an important building block is the refreshing gadget, which updates the randomness of the secret shared intermediate values. Recently, Dziembowski, Faust, and Zebrowski (ASIACRYPT’19) analyzed the security of a simple refreshing gadget by using a new technique called the leakage diagram. In this work, we follow the approach of Dziembowski et al. and significantly improve its methodology. Concretely, we refine the notion of a leakage diagram via so-called dependency graphs, and show how to use this technique for arbitrary complex circuits via composition results and approximation techniques. To illustrate the power of our new techniques, as a case study, we designed provably secure parallel gadgets for the random probing model, and adapted the ISW multiplication such that all gadgets can be parallelized. Finally, we evaluate concrete security levels, and show how our new methodology can further improve the concrete security level of masking schemes. This results in a compiler provable secure up to a noise level of O(1) for affine circuits and O(1/√n) in general.</jats:p>",https://doi.org/10.46586/tches.v2023.i4.420-459,CrossRef
Computer Engineering,Hardware Systems,Qualitative,Unrolled Cryptography on Silicon,"<jats:p>Cryptographic primitives with low-latency performance have gained momentum lately due to an increased demand for real-time applications. Block ciphers such as PRINCE enable data encryption (resp. decryption) within a single clock cycle at a moderately high operating frequency when implemented in a fully-unrolled fashion. Unsurprisingly, many typical environments for unrolled ciphers require protection against physical adversaries as well. Yet, recent works suggest that most common SCA countermeasures are hard to apply to low-latency circuits. Hardware masking, for example, requires register stages to offer resistance, thus adding delay and defeating the purpose of unrolling. On another note, it has been indicated that unrolled primitives without any additional means of protection offer an intrinsic resistance to SCA attacks due to their parallelism, asynchronicity and speed of execution. In this work, we take a closer look at the physical security properties provided by unrolled cryptographic IC implementations. We are able to confirm that the nature of unrolling indeed bears the potential to decrease the susceptibility of cipher implementations significantly when reset methods are applied. With respect to certain adversarial models, e.g., ciphertext-only access, an amazingly high level of protection can be achieved. While this seems to be a great result for cryptographic hardware engineers, there is an attack vector hidden in plain sight which still threatens the security of unrolled implementations remarkably – namely the static power consumption of CMOS-based circuits. We point out that essentially all reasons which make it hard to extract meaningful information from the dynamic behavior of unrolled primitives are not an issue when exploiting the static currents for key recovery. Our evaluation is based on real-silicon measurements of an unrolled PRINCE core in a custom 40nm ASIC. The presented results serve as a neat educational case study to demonstrate the broad differences between dynamic and static power information leakage in the light of technological advancement.</jats:p>",https://doi.org/10.46586/tches.v2020.i4.416-442,CrossRef
Computer Engineering,Hardware Systems,Qualitative,"Concurrent software‐hardware optimisation of adaptive estimating, identifying and filtering systems","<jats:sec><jats:title content-type=""abstract-heading"">Purpose</jats:title><jats:p>The purpose of this paper is to present the methods of concurrent optimization of the analogue and digital parts (software‐hardware) of estimating, identifying and filtering systems with adaptively adjusted analogue parts – adaptive estimation systems (AES).</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title><jats:p>Concurrent (complete) optimization of AES permits the determination of the most efficient algorithms for computing the estimates and the controls adjusting analogue units of AES in the way maximally improving the quality of observations delivered by them to the digital part. Performance of AES is assessed by the mean square error (MSE) of estimates which is constructed employing the models of input excitation, analogue and digital parts. Global extremum of MSE is searched by Bayesian methods taking into account the always bounded input range of AES and its possible overloading.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Findings</jats:title><jats:p>There are determined upper boundaries of potentially achievable accuracy of estimates, as well as optimal estimating and controlling observation units' algorithms, ensuring their achievement. New effects appearing in completely optimal AES are analysed.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Research limitations/implications</jats:title><jats:p>The paper presents the backgrounds of new and analytically complex approach. To clarify basic ideas and methods, the simplest but useful for applications single input‐single output and single input‐multiple output models of ASE were considered. The obtained results create wide field for further investigations.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Practical implications</jats:title><jats:p>The results of the paper can be applied in the development of new classes of high‐efficient adaptive data acquisition, measurement, controlling, communication and other systems.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-heading"">Originality/value</jats:title><jats:p>Concurrent optimisation of AES is important task having no general solution until now. Known approaches allow only the separate optimisation of the analogue and digital parts. Presented original approach enables the correct formalisation and solution of this task that permits the design and realization of systems with characteristics close to theoretically achievable ones and exceeding the characteristics of the known systems of similar predestination.</jats:p></jats:sec>",https://doi.org/10.1108/03684920810873245,CrossRef
Computer Engineering,Hardware Systems,Mixed Methods,LOGARITHMIC AMPLIFIERS FOR SOFTWARE HARDWARE MAGNETIC TRACKING  SYSTEMS,"<jats:p>The work deals with the problem of signal conversion in magnetic tracking devices. Magnetic tracking technology is based on computing the spatial position of an object being tracked upon measuring reference magnetic fields in low-frequency electromagnetic radiation spectrum. Magnetic tracking devices are key components of navigation sensors for virtual and augmented reality. It has been shown that the main problem one faces when developing sensory devices for magnetic tracking is the fact that signals should be measured in a wide measurement range. We have analyzed possible ways to solve the stated problem by digital and combined methods. The latter have proven to be more efficient. They consist in signal amplification due to analog compression, which is performed by logarithmic amplifiers whose negative  feedback circuits contain components with non-linear volt-ampere characteristics (typically, diodes or bipo- lar transistors are used). It has been shown that the parameters of logarithmic signal compression can be  controlled by modified circuits with auxiliary resistance dividers. The resistance dividers scale the logarithmic volt-ampere characteristics of emitter p-n junctions of bipolar n-p-n and p-n-p transistors. A substantial advantage of circuits with resistance dividers is that they provide the possibility to expand the range of the output voltage of logarithmic amplifiers and optimize the transition between the linear and logarithmic amplification regions. The work presents the results of simulation and experimental investigations into a logarithmic amplifier for a magnetic tracking system. Simulation was carried out using SPICE (Simulation Program with Integrated Circuit Emphasis) models. We applied an integrated approach,which provides collections of transient characteristics of logarithmic amplifiers at different sets of the  parameters of resistance dividers. The simulation results have been verified using our own software- firmware magnetic tracking tools – Magnetic Tracking System Integrated Development Environment. The signal converter was built upon a programmable system-on-chip PSoC 5LP by Cypress Semiconductor.</jats:p>",https://doi.org/10.15276/eltecs.33.109.2020.4,CrossRef
Computer Engineering,Hardware Systems,Mixed Methods,Non-Profiled Deep Learning-based Side-Channel attacks with Sensitivity Analysis,"<jats:p>Deep Learning has recently been introduced as a new alternative to perform Side-Channel analysis [MPP16]. Until now, studies have been focused on applying Deep Learning techniques to perform Profiled Side-Channel attacks where an attacker has a full control of a profiling device and is able to collect a large amount of traces for different key values in order to characterize the device leakage prior to the attack. In this paper we introduce a new method to apply Deep Learning techniques in a Non-Profiled context, where an attacker can only collect a limited number of side-channel traces for a fixed unknown key value from a closed device. We show that by combining key guesses with observations of Deep Learning metrics, it is possible to recover information about the secret key. The main interest of this method is that it is possible to use the power of Deep Learning and Neural Networks in a Non-Profiled scenario. We show that it is possible to exploit the translation-invariance property of Convolutional Neural Networks [CDP17] against de-synchronized traces also during Non-Profiled side-channel attacks. In this case, we show that this method can outperform classic Non-Profiled attacks such as Correlation Power Analysis. We also highlight that it is possible to break masked implementations in black-box, without leakages combination pre-preprocessing and with no assumptions nor knowledge about the masking implementation. To carry the attack, we introduce metrics based on Sensitivity Analysis that can reveal both the secret key value as well as points of interest, such as leakages and masks locations in the traces. The results of our experiments demonstrate the interests of this new method and show that this attack can be performed in practice.</jats:p>",https://doi.org/10.46586/tches.v2019.i2.107-131,CrossRef
Computer Engineering,Hardware Systems,Mixed Methods,Power Analysis on NTRU Prime,"<jats:p>This paper applies a variety of power analysis techniques to several implementations of NTRU Prime, a Round 2 submission to the NIST PQC Standardization Project. The techniques include vertical correlation power analysis, horizontal indepth correlation power analysis, online template attacks, and chosen-input simple power analysis. The implementations include the reference one, the one optimized using smladx, and three protected ones. Adversaries in this study can fully recover private keys with one single trace of short observation span, with few template traces from a fully controlled device similar to the target and no a priori power model, or sometimes even with the naked eye. The techniques target the constant-time generic polynomial multiplications in the product scanning method. Though in this work they focus on the decapsulation, they also work on the key generation and encapsulation of NTRU Prime. Moreover, they apply to the ideal-lattice-based cryptosystems where each private-key coefficient comes from a small set of possibilities.</jats:p>",https://doi.org/10.46586/tches.v2020.i1.123-151,CrossRef
Computer Engineering,Hardware Systems,Mixed Methods,Garbled Circuits from an SCA Perspective,"<jats:p>Garbling schemes, invented in the 80’s by Yao (FOCS’86), have been a versatile and fundamental tool in modern cryptography. A prominent application of garbled circuits is constant round secure two-party computation, which led to a long line of study of this object, where one of the most influential optimizations is Free-XOR (Kolesnikov and Schneider ICALP’08), introducing a global offset Δ for all garbled wire values where XOR gates are computed locally without garbling them. To date, garbling schemes were not studied per their side-channel attacks (SCA) security characteristics, even though SCA pose a significant security threat to cryptographic devices. In this research we, demonstrate that adversaries utilizing advanced SCA tools such as horizontal attacks, mixed with advanced hypothesis building and standard (vertical) SCA tools, can jeopardize garbling implementations.Our main observation is that garbling schemes utilizing a global secret Δ open a door to quite trivial side-channel attacks. We model our side-channel attacks on the garbler’s device and discuss the asymmetric setting where various computations are not performed on the evaluator side. This enables dangerous leakage extraction on the garbler and renders our attack impossible on the evaluator’s side.Theoretically, we first demonstrate on a simulated environment, that such attacks are quite devastating. Concretely, our attack is capable of extracting Δ when the circuit embeds only 8 input non-linear gates with fifth/first-order attack Success-Rates of 0.65/0.7. With as little as 3 such gates, our attack reduces the first-order Guessing Entropy of Δ from 128 to ∼ 48-bits. We further demonstrate our attack via an implementation and power measurements data over an STM 32-bit processor software implementing circuit garbling, and discuss their limitations and mitigation tactics on logical, protocol and implementation layers.</jats:p>",https://doi.org/10.46586/tches.v2023.i2.54-79,CrossRef
Computer Engineering,Hardware Systems,Mixed Methods,Efficient and Private Computations with Code-Based Masking,"<jats:p>Code-based masking is a very general type of masking scheme that covers Boolean masking, inner product masking, direct sum masking, and so on. The merits of the generalization are twofold. Firstly, the higher algebraic complexity of the sharing function decreases the information leakage in “low noise conditions” and may increase the “statistical security order” of an implementation (with linear leakages). Secondly, the underlying error-correction codes can offer improved fault resistance for the encoded variables. Nevertheless, this higher algebraic complexity also implies additional challenges. On the one hand, a generic multiplication algorithm applicable to any linear code is still unknown. On the other hand, masking schemes with higher algebraic complexity usually come with implementation overheads, as for example witnessed by inner-product masking. In this paper, we contribute to these challenges in two directions. Firstly, we propose a generic algorithm that allows us (to the best of our knowledge for the first time) to compute on data shared with linear codes. Secondly, we introduce a new amortization technique that can significantly mitigate the implementation overheads of code-based masking, and illustrate this claim with a case study. Precisely, we show that, although performing every single code-based masked operation is relatively complex, processing multiple secrets in parallel leads to much better performances. This property enables code-based masked implementations of the AES to compete with the state-of-the-art in randomness complexity. Since our masked operations can be instantiated with various linear codes, we hope that these investigations open new avenues for the study of code-based masking schemes, by specializing the codes for improved performances, better side-channel security or improved fault tolerance.</jats:p>",https://doi.org/10.46586/tches.v2020.i2.128-171,CrossRef
Computer Engineering,Hardware Systems,Design and Development,"Cycle-Accurate Evaluation of Software-Hardware Co-Design of Decimal
  Computation in RISC-V Ecosystem","Software-hardware co-design solutions for decimal computation can provide
several Pareto points to development of embedded systems in terms of hardware
cost and performance. This paper demonstrates how to accurately evaluate such
co-design solutions using RISC-V ecosystem. In a software-hardware co-design
solution, a part of solution requires dedicated hardware. In our evaluation
framework, we develop new decimal oriented instructions supported by an
accelerator. The framework can realize cycle-accurate analysis for performance
as well as hardware overhead for co-design solutions for decimal computation.
The obtained performance result is compared with an estimation with dummy
functions.",http://arxiv.org/abs/2003.05315v1,arXiv
Computer Engineering,Hardware Systems,Design and Development,"HENNC: Hardware Engine for Artificial Neural Network-based Chaotic
  Oscillators","This letter introduces a framework for the automatic generation of hardware
cores for Artificial Neural Network (ANN)-based chaotic oscillators. The
framework trains the model to approximate a chaotic system, then performs
design space exploration yielding potential hardware architectures for its
implementation. The framework then generates the corresponding synthesizable
High-Level Synthesis code and a validation testbench from a selected solution.
The hardware design primarily targets FPGAs. The proposed framework offers a
rapid hardware design process of candidate architectures superior to manually
designed works in terms of hardware cost and throughput. The source code is
available on GitHub.",http://arxiv.org/abs/2407.19165v1,arXiv
Computer Engineering,Hardware Systems,Design and Development,"Hardware Architecture of Wireless Power Transfer, RFID, and WIPT Systems","In this work, we provide an overview of the hardware architecture of wireless
power transfer (WPT), RFID, and wireless information and power transfer (WIPT)
systems. The historical milestones and structure differences among WPT, RFID,
and WIPT are introduced.",http://arxiv.org/abs/2102.06876v1,arXiv
Computer Engineering,Hardware Systems,Design and Development,"LLM4SecHW: Leveraging Domain Specific Large Language Model for Hardware
  Debugging","This paper presents LLM4SecHW, a novel framework for hardware debugging that
leverages domain specific Large Language Model (LLM). Despite the success of
LLMs in automating various software development tasks, their application in the
hardware security domain has been limited due to the constraints of commercial
LLMs and the scarcity of domain specific data. To address these challenges, we
propose a unique approach to compile a dataset of open source hardware design
defects and their remediation steps, utilizing version control data. This
dataset provides a substantial foundation for training machine learning models
for hardware. LLM4SecHW employs fine tuning of medium sized LLMs based on this
dataset, enabling the identification and rectification of bugs in hardware
designs. This pioneering approach offers a reference workflow for the
application of fine tuning domain specific LLMs in other research areas. We
evaluate the performance of our proposed system on various open source hardware
designs, demonstrating its efficacy in accurately identifying and correcting
defects. Our work brings a new perspective on automating the quality control
process in hardware design.",http://arxiv.org/abs/2401.16448v1,arXiv
Computer Engineering,Hardware Systems,Design and Development,"DRAGON (Differentiable Graph Execution) : A suite of Hardware Simulation
  and Optimization tools for Modern AI/Non-AI Workloads","We introduce DRAGON, an open-source, fast and explainable hardware simulation
and optimization toolchain that enables hardware architects to simulate
hardware designs, and to optimize hardware designs to efficiently execute
workloads.
  The DRAGON toolchain provides the following tools: Hardware Model Generator
(DGen), Hardware Simulator (DSim) and Hardware Optimizer (DOpt).
  DSim provides the simulation of running algorithms (represented as data-flow
graphs) on hardware described. DGen describes the hardware in detail, with user
input architectures/technology (represented in a custom description language).
A novel methodology of gradient descent from the simulation allows us optimize
the hardware model (giving the directions for improvements in technology
parameters and design parameters), provided by Dopt.
  DRAGON framework (DSim) is much faster than previously avaible works for
simulation, which is possible through performance-first code writing practices,
mathematical formulas for common computing operations to avoid cycle-accurate
simulation steps, efficient algorithms for mapping, and data-structure
representations for hardware state. DRAGON framework (Dopt) generates
performance optimized architectures for both AI and Non-AI Workloads, and
provides technology improvement directions for 100x-1000x better future
computing systems.",http://arxiv.org/abs/2204.06676v7,arXiv
Computer Engineering,Hardware Systems,Theoretical / Conceptual,From Hardware to Hardcore,"<jats:p>The state and relevance of Systems as a field of research and a specific form of scientific inquiry into complex real-world problem situations, can be enhanced significantly by developing and applying more formalized and coherent tools: a new ‘hardware' enabling to build a new ‘hardcore' for systems science. The basis of this new hardware stems from a line of thought emanating from George Spencer-Brown and the ‘Laws of Form', running through the work of Francisco Varela and his calculus for self-reference, being radicalized by Niklas Luhmann and his views on ‘Social Systems', and continued by Dirk Baecker with the application of form theory to management and organizations. In this contribution, the author develops an understanding and appreciation of the potentials of a form-theoretical approach to formalizing systems (real-world phenomena) as well as Systems (field of research). Central aspects will be the power of the form-theoretical hardware as regards systems storytelling, systems diagnostics and abductive reasoning.</jats:p>",https://doi.org/10.4018/ijss.2017010105,CrossRef
Computer Engineering,Hardware Systems,Theoretical / Conceptual,Uncontrolled Learning: Codesign of Neuromorphic Hardware Topology for Neuromorphic Algorithms,"<jats:p>Neuromorphic computing has the potential to revolutionize future technologies and our understanding of intelligence, yet it remains challenging to realize in practice. The learning‐from‐mistakes algorithm, inspired by the brain's simple learning rules of inhibition and pruning, is one of the few brain‐like training methods. This algorithm is implemented in neuromorphic memristive hardware through a codesign process that evaluates essential hardware trade‐offs. While the algorithm effectively trains small networks as binary classifiers and perceptrons, performance declines significantly with increasing network size unless the hardware is tailored to the algorithm. This work investigates the trade‐offs between depth, controllability, and capacity—the number of learnable patterns—in neuromorphic hardware. This highlights the importance of topology and governing equations, providing theoretical tools to evaluate a device's computational capacity based on its measurements and circuit structure. The findings show that breaking neural network symmetry enhances both controllability and capacity. Additionally, by pruning the circuit, neuromorphic algorithms in all‐memristive circuits can utilize stochastic resources to create local contrasts in network weights. Through combined experimental and simulation efforts, the parameters are identified that enable networks to exhibit emergent intelligence from simple rules, advancing the potential of neuromorphic computing.</jats:p>",https://doi.org/10.1002/aisy.202400739,CrossRef
Computer Engineering,Hardware Systems,Theoretical / Conceptual,Robust but Relaxed Probing Model,"<jats:p>Masking has become a widely applied and heavily researched method to protect cryptographic implementations against Side-Channel Analysis (SCA) attacks. The success of masking is primarily attributed to its strong theoretical foundation enabling it to formally prove security by modeling physical properties through socalled probing models. Specifically, the robust d-probing model enables us to prove the security for arbitrarily masked hardware circuits, manually or with the assistance of automated tools, even when considering the imperfect nature of physical hardware, including the occurrence of physical defaults such as glitches. However, the generic strategy employed by the robust d-probing model comes with a downside: It tends to over-conservatively model the information leakage caused by glitches meaning that the robust d-probing model considers glitches that can never occur in practice. This implies that in theory, an adversary could gain more information than she would obtain in practice. From a designer’s perspective, this entails that (1) securely designed hardware circuits may need to be withdrawn due to potential insecurity under the robust d-probing model and (2) designs that satisfy the security requirements of the robust d-probing model may incur unnecessary overhead, such as increased circuit size or latency.In this work, we refine the formal treatment of glitches within the robust d-probing model to address glitches more accurately within a formal adversary model. Unlike the robust d-probing model, our approach considers glitches based on the operations performed and the data processed, ensuring that only manifesting glitches are accounted for. As a result, we introduce the Robust but Relaxed (RR) d-probing model, a formal adversary model maintaining the same level of security as the robust d-probing model but without the overly conservative treatment of glitches. Leveraging our new model, we prove the security of LUT-based Masked Dual-Rail with Pre-charge Logic (LMDPL) gadgets, a class of physically secure gadgets reported as insecure based on the robust d-probing model. We provide manual proofs and automated security evaluations employing an updated version of PROLEAD capable of verifying the security of masked circuits under our new model.</jats:p>",https://doi.org/10.46586/tches.v2024.i4.451-482,CrossRef
Computer Engineering,Hardware Systems,Theoretical / Conceptual,Hardware for Deep Learning Acceleration,"<jats:p>Deep learning (DL) has proven to be one of the most pivotal components of machine learning given its notable performance in a variety of application domains. Neural networks (NNs) for DL are tailored to specific application domains by varying in their topology and activation nodes. Nevertheless, the major operation type (with the largest computational complexity) is commonly multiply‐accumulate operation irrespective of their topology. Recent trends in DL highlight the evolution of NNs such that they become deeper and larger, and thus their prohibitive computational complexity. To cope with the consequent prohibitive latency for computation, 1) general‐purpose hardware, e.g., central processing units and graphics processing units, has been redesigned, and 2) various DL accelerators have been newly introduced, e.g., neural processing units, and computing‐in‐memory units for deep NN‐based DL, and neuromorphic processors for spiking NN‐based DL. In this review, these accelerators and their pros and cons are overviewed with particular focus on their performance and memory bandwidth.</jats:p>",https://doi.org/10.1002/aisy.202300762,CrossRef
Computer Engineering,Hardware Systems,Theoretical / Conceptual,Diseño de Hardware y Software de Systems on Chip empleando tecnología Xilinx EDK,"<jats:p>El presente artículo resume el proceso empleado para obtener el primer System on Chip (SoC) diseñado, desarrollado, y emulado en la Escuela Politécnica del Ejército (ESPE) y en el Ecuador. Se demostrará que combinando las ventajas del diseño sobre Field Programable Gate Arrays (FPGAs) empleando la reutilización de IP Cores y plataformas, junto al uso de la tecnología de desarrollo Xilinx EDK, se puede diseñar tanto el hardware como el software de un chip de manera rápida y económicamente fiable. Además, se detalla el uso de la metodología Platform Based Design (PBD) y del concepto de co-diseño de hardware y software para diseñar las capas de hardware, sistema operativo y aplicación de un chip. La capa de hardware contiene una serie de IP Cores gobernados por un procesador MicroBlaze trabajando dentro de la arquitectura CoreConnect de IBM. Mientras que la capa de sistema operativo está conformada por drivers, librerías y el Sistema Operativo en Tiempo Real (RTOS) Xilkernel. Por último, la capa de aplicación tiene la funcionalidad de controlar una planta de temperatura, mediante la selección de dos técnicas de control: ON-OFF o PID. Cabe destacar que el co-diseño se desarrolló considerando un adecuado enfoque conceptual, arquitectural, y metodológico1.</jats:p>",https://doi.org/10.24133/maskay.v2i1.146,CrossRef
Computer Engineering,Computer Architecture,Quantitative,Direct-execution computer architecture,"<jats:p>The Direct-Execution Architecture is a language-directed computer architecture. It can accept a highlevel-language program and directly executes it without compilation, assembly, linkage editing or loading. It offers a means to eliminate compilers, loaders etc. and attacks the problem of mounting software cost. In addition, the advent of microprocessors has demonstrated that highly complex digital hardware can be built reliably and inexpensively. Using this hardware to implement the Direct-Execution Architecture redistributes apportionment of costs between the hardware and software. The paper surveys the Direct-Execution Architecture, presents the relationship between language and architecture, and explains how a Direct-Execution system works. It also brings up the use of Direct-Execution for a highly interactive program writing, debugging, execution system. With this system, program writing could proceed like English composition. This paper then discusses the issue of a single high-level machine language, and the potential role of the interpreters. Finally, it attempts to fortell what could happen to the Direct-Execution Architecture in the next five to 10 years.</jats:p>",https://doi.org/10.1145/859412.859415,CrossRef
Computer Engineering,Computer Architecture,Quantitative,"The nature of ""computer architecture""","<jats:p>""The finest words in the world are only vain sounds if you cannot understand them."" Anatole FranceThere are several different interpretations of what ""computer architecture"" is, and what the phrase means. An informal survey of professionals who routinely deal with computers was conducted to determine better the perceived meaning of ""computer architecture"". From this I hoped to identify potential areas of misunderstanding when discussing the various aspects of computer architecture.</jats:p>",https://doi.org/10.1145/859463.859464,CrossRef
Computer Engineering,Computer Architecture,Quantitative,MisSPECulation,"<jats:p>A majority of the papers published in leading computer architecture conferences use SPEC CPU2000, or its predecessor SPEC CPU95, which has become the de facto standard for measuring processor and/or memory-hierarchy performance. However, in most cases a subset of the suite's benchmarks are simulated. For example: 27 papers were published in ISCA 2002, 16 used SPEC CINT2000, 4 used the whole suite, and only 3 papers explained their omissions.This paper quantifies the extent of this phenomenon in the ISCA, Micro, and HPCA conferences: 173 papers were surveyed, 115 used benchmarks from SPEC CINT, but only 23 used the whole suite. If this current trend continues, by the year 2005 80% of the papers will use the full CINT2000 suite, a year after CPU2004 shall be announced.We claim that results based upon a subset of a benchmark suite are speculative and conflict with Amdahl's Law. The law implies that we must present the speedup of using the proposed technique on the whole suite. Projecting the law (by statistically supplying values for the missing benchmarks) to several published papers reduces promising results to average ones. Speedups are reduced from 1.42 to 1.16 in one case, from 1.43 to 1.13 in another, and from 1.76 to 1.15 in a third.Finally, we have found that the disregard for CFP2000 is unwarranted in papers that explore the data cache domain, the suite displays a higher data cache miss rate than CINT2000, which is used more frequently.</jats:p>",https://doi.org/10.1145/871656.859625,CrossRef
Computer Engineering,Computer Architecture,Quantitative,Stock Price Predictor: Implementing Stocks Predictive Model Using Deep Learning,"<jats:p>Purpose–This paper proposes a novel deep neural network model, specifically long short-term  memory  (LSTM)  networks,for  predicting  stock  prices  using  historical  data  and financial indicators.

Method–LTSM  can  handle  long  sequenceswhile  capturing temporal  dependencies, making it an excellent choice for NLP or timeseries. The model is trained and tested on the Ayala Corporation (AYALY)stock dataset from 2016 to 2019, using four financial indicators: earnings  per  share  (EPS),  EPS  growth,  price/earnings  ratio,  and  price/earnings-to-growth ratio.

Results–The results show that the model achieves high accuracy and outperforms other Deep  Neural  Network variantsas  confirmed by  assessing  its  performance  using  suitable metrics  like  mean  squared  error  and  mean  absolute  error.It  effectively  explored  and selected  relevant  financial  indicators,  implemented  data  preprocessing  techniques,  and trained the model using historical data.

Conclusion–The projecteffectively explored and selected relevant financial indicators and trained LSTM modelsusing historical data, and, thus,met its objectives todevelop a deep neural network model for stock price prediction.

Recommendations–The authors recommend that future researchers continue to explore the  integration  of  a diverse  set  of  financial  indicators, employ  rigorous  comparative analyses,  and  experiment  with  different  time  frames  for  future  predictionsto further enhance prediction accuracy.

Research  Implications–This  papercontributesto  the  ongoing  development  of  machine-learning studies, especially in the Philippines, particularly fortime-series forecasting. With more  accurate  predictions  of  stock  prices,  the study could enable  investors  to  make informed investment decisions, trading strategies,and financial decision-making processes.

Keywords–DeepNeural Network, Long Short-Term Memory (LSTM) Networks, Machine Learning, Stock Price Prediction, Time Series Forecasting</jats:p>",https://doi.org/10.25147/ijcsr.2017.001.1.209,CrossRef
Computer Engineering,Computer Architecture,Quantitative,An embedded DRAM architecture for large-scale spatial-lattice computations,"<jats:p>Spatial-lattice computations with finite-range interactions are an important class of easily parallelized computations. This class includes many simple and direct algorithms for physical simulation, virtual-reality simulation, agent-based modeling, logic simulation, 2D and 3D image processing and rendering, and other volumetric data processing tasks. The range of applicability of such algorithms is completely dependant upon the lattice-sizes and processing speeds that are computationally feasible. Using embedded DRAM and a new technique for organizing SIMD memory and communications we can efficiently utilize 1 Tbit/sec of sustained memory bandwidth in each chip in an indefinitely scalable array of chips. This allows a 10,000-fold speedup per memory chip for these algorithms compared to the CAM-8 lattice gas computer, and is about one million times faster per memory chip for these calculations than a CM-2.</jats:p>",https://doi.org/10.1145/342001.339672,CrossRef
Computer Engineering,Computer Architecture,Qualitative,"Does Unsupervised Architecture Representation Learning Help Neural
  Architecture Search?","Existing Neural Architecture Search (NAS) methods either encode neural
architectures using discrete encodings that do not scale well, or adopt
supervised learning-based methods to jointly learn architecture representations
and optimize architecture search on such representations which incurs search
bias. Despite the widespread use, architecture representations learned in NAS
are still poorly understood. We observe that the structural properties of
neural architectures are hard to preserve in the latent space if architecture
representation learning and search are coupled, resulting in less effective
search performance. In this work, we find empirically that pre-training
architecture representations using only neural architectures without their
accuracies as labels considerably improve the downstream architecture search
efficiency. To explain these observations, we visualize how unsupervised
architecture representation learning better encourages neural architectures
with similar connections and operators to cluster together. This helps to map
neural architectures with similar performance to the same regions in the latent
space and makes the transition of architectures in the latent space relatively
smooth, which considerably benefits diverse downstream search strategies.",http://arxiv.org/abs/2006.06936v2,arXiv
Computer Engineering,Computer Architecture,Qualitative,"Sensor Networks Architecture for Vehicles and Pedestrians Traffic
  Control","In this paper we present a sensor network based architecture for urban
traffic management, hierarchically structured on three layers: sensing,
processing& aggregation and control. On proposed architecture we define traffic
decongestion methods for vehicles and also for pedestrians. Finally, we
presented a case study on how traffic control can be implemented in a concrete
situation, based on the proposed architecture, pointing future directions of
development.",http://arxiv.org/abs/1807.09222v1,arXiv
Computer Engineering,Computer Architecture,Qualitative,"A Study of the Learning Progress in Neural Architecture Search
  Techniques","In neural architecture search, the structure of the neural network to best
model a given dataset is determined by an automated search process. Efficient
Neural Architecture Search (ENAS), proposed by Pham et al. (2018), has recently
received considerable attention due to its ability to find excellent
architectures within a comparably short search time. In this work, which is
motivated by the quest to further improve the learning speed of architecture
search, we evaluate the learning progress of the controller which generates the
architectures in ENAS. We measure the progress by comparing the architectures
generated by it at different controller training epochs, where architectures
are evaluated after having re-trained them from scratch. As a surprising
result, we find that the learning curves are completely flat, i.e., there is no
observable progress of the controller in terms of the performance of its
generated architectures. This observation is consistent across the CIFAR-10 and
CIFAR-100 datasets and two different search spaces. We conclude that the high
quality of the models generated by ENAS is a result of the search space design
rather than the controller training, and our results indicate that one-shot
architecture design is an efficient alternative to architecture search by ENAS.",http://arxiv.org/abs/1906.07590v1,arXiv
Computer Engineering,Computer Architecture,Qualitative,Conceptual Modeling for Computer Organization and Architecture,"Understanding computer system hardware, including how computers operate, is
essential for undergraduate students in computer engineering and science.
Literature shows students learning computer organization and assembly language
often find fundamental concepts difficult to comprehend within the topic
materials. Tools have been introduced to improve students comprehension of the
interaction between computer architecture, assembly language, and the operating
system. One such tool is the Little Man Computer (LMC) model that operates in a
way similar to a computer but that is easier to understand. Even though LMC
does not have modern CPUs with multiple cores nor executes multiple
instructions, it nevertheless shows the basic principles of the von Neumann
architecture. LMC aims to introduce students to such concepts as code and
instruction sets. In this paper, LMC is used for an additional purpose: a tool
with which to experiment using a new modeling language (i.e., a thinging
machine; TM) in the area of computer organization and architecture without
involving complexity in the subject. That is, the simplicity of LMC facilitates
the application of TM without going deep into computer
organization/architecture materials. Accordingly, the paper (a) provides a new
way for using the LMC model for whatever purpose (e.g., education) and (b)
demonstrates that TM can be used to build an abstract level of description in
the organization/architect field. The resultant schematics from the TM model of
LMC offer an initial case study that supports our thesis that TM is a viable
method for hardware/software-independent descriptions in the computer
organization and architect field of study.",http://arxiv.org/abs/2103.01773v1,arXiv
Computer Engineering,Computer Architecture,Qualitative,On the use of SPEC benchmarks in computer architecture research,"<jats:p>Benchmarks, in particular the SPEC CPU benchmarks, are frequently used in academic computer research. With ASPLOS-7 as an example, observations about such usage are reported, and suggestions are made for a meaningful use of benchmarks in computer architecture research. Forward-looking computer architecture research may need more than one benchmark collection.</jats:p>",https://doi.org/10.1145/250015.250018,CrossRef
Computer Engineering,Computer Architecture,Mixed Methods,Computer architecture courses in electrical engineering departments,"<jats:p>This paper traces the history of computer architecture courses in electrical engineering departments. Previously unpublished data from the Fall 1972 COSINE survey are given to show current computer architecture course offerings and texts. Computer architecture courses offered in 1972-73 are analyzed, compared with ACM and COSINE recommendations, and classified into five categories: introductory computer engineering courses with a computer architecture flavor, software-oriented computer organization courses, hardware-oriented computer organization courses, case study courses, and topical seminars. Future trends in computer architecture education are predicted.</jats:p>",https://doi.org/10.1145/633642.803984,CrossRef
Computer Engineering,Computer Architecture,Mixed Methods,A Software Based Many-Core Architecture Simulator,"<jats:p>As technology continuously advances, engineers are constantly faced with challenges that require numerous computational designs and implementations that, usually, go beyond practical feasibility, considering the available resources at hand. An area that might be considered for dealing with these problems relates to the use of many-core architectures for parallel processing. This type of architecture can be extremely efficient for intensive computational tasks and has the power to operate with low energy and low clock frequencies; however scalability issues attached to the process can significantly affect its design. This paper presents the technicalities involved in developing a scalable many-core software-based simulator named SImulator for Many-Cores (SIMC) that includes features such as package routing and efficient inter-process communication. It is intended as a project goal that SIMC becomes a useful software package that allows students with interests in simulating many-core based hardware projects as software systems. It is also intended that by practicing with SIMC on a diverse set of problems, students can acquire experience in analyzing metrics, such as speed and latency, among others that are commonly used in this sort of scenario. The type of practice provided by SIMC promotes a way of fixing the several hardware related concepts involved as well as to enlarge and refine student´s skills in programming. For the case study described in this paper, the validation of SIMC has been carried out by means of solving a relatively trivial problem i.e., that of the execution of simple morphological filters, where the allocation of tasks can be optimized for improving either the execution speed or latency. SIMC allows a direct comparison of values of both metrics, as well as a quantitative evaluation of the implemented network as a whole.</jats:p>",https://doi.org/10.5753/ijcae.2023.4831,CrossRef
Computer Engineering,Computer Architecture,Mixed Methods,Ten ways to waste a parallel computer,"<jats:p>As clock speed increases taper off and hardware designers struggle to scale parallelism within a chip, software developers and researchers must face the challenge of writing portable software with no clear architectural target. On the hardware side, energy considerations will dominate many of the design decisions, and will ultimately limit what systems and applications can be built. This is especially true at the high end, where the next major milestone of exascale computing will be unattainable without major improvements in efficiency.</jats:p>
          <jats:p>Although hardware designers have long worried about the efficiency of their designs, especially for battery-operated devices, software developers in general have not. To illustrate this point, I will describe some of the top ways to waste time and therefore energy waiting for communication, synchronization, or interactions with users or other systems. Data movement, rather than computation, is the big consumer of energy, yet software often moves data up and down the memory hierarchy or across a network multiple times. At the same time, hardware designers need to take into account the constraints of the computational problems that will run on their systems, as a design that is poorly matched to the computational requirements will end up being inefficient. Drawing on my own experience in scientific computing, I will give examples of how to make the combination of hardware, algorithms and software more efficient, but also describe some of the challenges that are inherent in the application problems we want to solve. The community needs to take an integrated approach to the problem, and consider how much business or science can be done per Joule, rather than optimizing a particular component of the system in isolation. This will require rethinking the algorithms, programming models, and hardware in concert, and therefore an unprecedented level of collaboration and cooperation between hardware and software designers.</jats:p>",https://doi.org/10.1145/1555815.1555755,CrossRef
Computer Engineering,Computer Architecture,Mixed Methods,Training-free Neural Architecture Search for RNNs and Transformers,"Neural architecture search (NAS) has allowed for the automatic creation of
new and effective neural network architectures, offering an alternative to the
laborious process of manually designing complex architectures. However,
traditional NAS algorithms are slow and require immense amounts of computing
power. Recent research has investigated training-free NAS metrics for image
classification architectures, drastically speeding up search algorithms. In
this paper, we investigate training-free NAS metrics for recurrent neural
network (RNN) and BERT-based transformer architectures, targeted towards
language modeling tasks. First, we develop a new training-free metric, named
hidden covariance, that predicts the trained performance of an RNN architecture
and significantly outperforms existing training-free metrics. We experimentally
evaluate the effectiveness of the hidden covariance metric on the NAS-Bench-NLP
benchmark. Second, we find that the current search space paradigm for
transformer architectures is not optimized for training-free neural
architecture search. Instead, a simple qualitative analysis can effectively
shrink the search space to the best performing architectures. This conclusion
is based on our investigation of existing training-free metrics and new metrics
developed from recent transformer pruning literature, evaluated on our own
benchmark of trained BERT architectures. Ultimately, our analysis shows that
the architecture search space and the training-free metric must be developed
together in order to achieve effective results.",http://arxiv.org/abs/2306.00288v1,arXiv
Computer Engineering,Computer Architecture,Mixed Methods,Omega: An Architecture for AI Unification,"We introduce the open-ended, modular, self-improving Omega AI unification
architecture which is a refinement of Solomonoff's Alpha architecture, as
considered from first principles. The architecture embodies several crucial
principles of general intelligence including diversity of representations,
diversity of data types, integrated memory, modularity, and higher-order
cognition. We retain the basic design of a fundamental algorithmic substrate
called an ""AI kernel"" for problem solving and basic cognitive functions like
memory, and a larger, modular architecture that re-uses the kernel in many
ways. Omega includes eight representation languages and six classes of neural
networks, which are briefly introduced. The architecture is intended to
initially address data science automation, hence it includes many problem
solving methods for statistical tasks. We review the broad software
architecture, higher-order cognition, self-improvement, modular neural
architectures, intelligent agents, the process and memory hierarchy, hardware
abstraction, peer-to-peer computing, and data abstraction facility.",http://arxiv.org/abs/1805.12069v1,arXiv
Computer Engineering,Computer Architecture,Design and Development,The IEEE Computer Society task force on computer architecture,"<jats:p>The subject of computer architecture as currently taught in most computer engineering and computer science programs is a mixture of architectural principles, organizational strategies, and implementation techniques. This blurring of the hierarchy of system levels that characterize the structure of a computer has made it very difficult for students (and often instructors as well) to determine what were the forces that led to the design decisions they have seen reflected in machines.</jats:p>
          <jats:p>In view of these circumstances, a task force was established by the IEEE Computer Society to prepare a detailed specification for a course of study in computer architecture for students whose major interest is in computer engineering or computer science.</jats:p>",https://doi.org/10.1145/633617.803545,CrossRef
Computer Engineering,Computer Architecture,Design and Development,Disbursed control computer architecture,"<jats:p>Disbursed Control Computer Architecture, comprising totally pipelined computer, a number of components interconnected in a pipeline:Control storage for control and input-output address maker, plurality of other control storage for corresponding plurality of data address makers, and another control storage for operators and operational registers.Any mentioned control storage contains, as a part of a program, a plurality of appropriate section of a control word. Control and input-output address maker issues control address to all mentioned control storage and receives its own section of control word.Plurality of data address makers receives plurality of corresponding sections of control word and issue corresponding plurality of data addresses to operands and results storage, which, in turn, is issuing and receiving data to and from operational registers and bus joint and to condition and status logic.Each clock cycle, new control address initiates new control word, sections of which contain all commands and addresses for enactment of next control and input-output address, by control and input-output address maker, and of next plurality of data addresses, by plurality of data address makers.Following in pipeline, new commands and data submitted to operators and operational registers direct performance of new primary (originated by the task) operations every clock cycle.Several clock cycles are allotted along pipeline to data address makers, allowing the most complex address preparations.Also comprising: interconnect of totally pipelined computer with other computers, operating via input-output devices with separate sending and receiving ports.</jats:p>",https://doi.org/10.1145/333680.333695,CrossRef
Computer Engineering,Computer Architecture,Design and Development,Lessons Learned Using ArchC in Computer Architecture Laboratory,"<jats:p>This paper presents a set of laboratory experiments based on the ArchC Architecture Description Language designed to fulfill the practical knowledge on Computer Architecture. These activities were designed over the last years and have been used in our discipline of Laboratory of Computer Architecture, where seventh semester students apply the knowledge they acquired in the theory classes. We present the experiments, covering 10 distinct topics in Computer Architecture, along with specific sections of the text-book to which they refer. We also show some of the experiences we acquired during the last years both on learning outcomes and student feedback.</jats:p>",https://doi.org/10.5753/ijcae.2016.4868,CrossRef
Computer Engineering,Computer Architecture,Design and Development,Integrating Continuous Assessment into Undergraduate Computer Architecture using Automated Grading,"<jats:p>Continuous assessment (CA) improves student engagement and understanding through regular evaluations and rapid feedback. This approach was integrated into a foundational Computer Architecture course using frequent quizzes. To manage large class sizes and limited teaching assistance, automated grading software was used. This paper discusses the implementation of automated grading for quizzes, detailing the process, and presenting course results and student feedback. Observations based on student feedback and outcomes suggest that integrating CA through quizzes is beneficial for student engagement and learning.</jats:p>",https://doi.org/10.5753/ijcae.2024.5341,CrossRef
Computer Engineering,Computer Architecture,Design and Development,Integrated computer architecture development system,"<jats:p>Development in systems architecture of separate soft-and hardware components introduces high inefficiencies, due to redundancies and seldom used functions on the levels of implementation. A better approach offers a methodology for an integrated software, firmware, and hardware design. Required are, however, immense computer resources with graphical capabilities. But it gives help in the development under constraints. We present such a system and show its capabilities in the development of a defect-free sorter module.</jats:p>",https://doi.org/10.1145/146628.140900,CrossRef
Computer Engineering,Computer Architecture,Theoretical / Conceptual,Modeling Network Architecture: A Cloud Case Study,"The Internet s ability to support a wide range of services depends on the
network architecture and theoretical and practical innovations necessary for
future networks. Network architecture in this context refers to the structure
of a computer network system as well as interactions among its physical
components, their configuration, and communication protocols. Various
descriptions of architecture have been developed over the years with an
unusually large number of superficial icons and symbols. This situation has
created a need for more coherent systematic representations of network
architecture. This paper is intended to refine the design, analysis, and
documentation of network architecture by adopting a conceptual model called a
thinging (abstract) machine (TM), which views all components of a network in
terms of a single notion: the flow of things in a TM. Since cloud computing has
become increasingly popular in the last few years as a model for a shared pool
of networks, servers, storage, and applications, we apply the TM to model a
real case study of cloud networks. The resultant model introduces an integrated
representation of computer networks.",http://arxiv.org/abs/2004.10350v1,arXiv
Computer Engineering,Digital Systems Design,Quantitative,Proposing Digital Design Methodology for Furniture Products by Integrating Generative Design Approach to Conventional Process,"<jats:p>Purpose: Considering the growing digital transformations in design field, this research aims to explore the advance language of design - generative design. The study focuses on integration of generative design and parametric modeling techniques with conventional furniture design process to develop a digital design and development methodology for furniture products by doing practical work in Rhino and Grasshopper.&#x0D;
Methodology: For this purpose, a comparative analysis is drawn and three practicals performed in Rhino Grasshopper.&#x0D;
Results: The author learned one of the design tool (Grasshopper) and investigate the possible ways to integrate this digital design process with the conventional process. These examples illustrate the liberty to design and explore any form that is imaginable and offers flexibility in the development process to make multiple design options. This digital way of designing leads to better accuracy, quicker adaptation to the initial concepts, ability to produce new alternatives, ease of revision, quality and realistic results and most importantly ready to manufacture products. This research was carried out, specifically, keeping the industrial environment of Pakistan Furniture Industry in reference.&#x0D;
Unique Contribution to Theory, Policy and Practice: This research proposal helped the author to create an awareness at a very initial level in the targeted scope with the hope to encourage authorities to take some serious steps towards the institutionalization of technological advancement in the design education sector. This research not only honed the author’s knowledge in the state of the art but also enabled him to share more advance knowledge about CAD tools with his students. By following and integrating advance design tools to the design process, developments in furniture industry in Pakistan can be made possible in a short time.  </jats:p>",https://doi.org/10.47941/jts.1368,CrossRef
Computer Engineering,Digital Systems Design,Quantitative,Digital Design of Smart Museum Based on Artificial Intelligence,"<jats:p>Today, as the soft power of culture is becoming more and more important, it is very important to pay attention to the learning and dissemination of culture. As the carrier of this process, the use of advanced technology to improve the museum is of great significance. This paper studies the digital design of smart museum based on artificial intelligence in order to explore the application of smart museum in artificial intelligence, analyze the spatial design of smart museum by using digital technology, explore a feasible method to give full play to the function of smart museum, and put forward some suggestions on the spatial design of smart museum. The design of the smart museum is no longer restricted by time and space and uses digital technology to double use virtual things and dynamic space. Through the detailed analysis of the application of artificial intelligence and digitization in the spatial design of the smart museum, combined with the information decision tree algorithm and data heterogeneous network algorithm, this study constructs the model of the information processing architecture of smart museum and the requirements of digital museum and makes a decision-making analysis of the comparison results of existing data. It includes the digital design of smart museum display technology, display effect, and other display-related contents. Analyzing the impact of smart museum on the object can provide data support for the feasibility of digital space design of smart museum based on artificial intelligence. The results of regression data processing show that the spatial visual sense of digital design wisdom museum is very strong, reaching the level of 5.0, and the picture aesthetic effect is up to 4.8.</jats:p>",https://doi.org/10.1155/2021/4894131,CrossRef
Computer Engineering,Digital Systems Design,Quantitative,Visual Art Design of Digital Works Guided by Big Data,"<jats:p>With the rapid development of digital technology, the development speed of digital media is also relatively fast. Digital media technology has a great impact on people’s lifestyles and aesthetic concepts, and it also has a greater impact on visual art, creative thinking communication methods, and expression methods. In this study, the quality enhancement of digital images has been intensively studied based on the guidance of big data of eye-movement gaze points. A large amount of visual data are collected from public social resources, and the optimization research of image sensory quality is carried out in-depth using the acquired big data. Next, the region of interest (ROI) is obtained by combining the data with a two-dimensional Gaussian distribution model-fitting method, and the obtained data clustered and improved based on the K-means clustering algorithm to obtain ROI fixation points. Finally, discontinuities in the choice of sharpness in graphics and video playback are pointed out, and the final fixation data analysis is utilized. Results show that targeted optimization is very effective in improving the quality of digital images and saving space, enabling users to enjoy higher-quality visual digital images. The proposed method can be used to improve the dynamic resolution of images and videos.</jats:p>",https://doi.org/10.1155/2022/5636449,CrossRef
Computer Engineering,Digital Systems Design,Quantitative,Deliberations on design,"<jats:sec>
               <jats:title content-type=""abstract-heading"">Purpose</jats:title>
               <jats:p> – The purpose of this paper is to encourage libraries to be as involved as possible in the design of the services they provide, be that traditional Web services or sophisticated discovery services. An inordinate reliance upon turn key applications that afford little to no opportunity for customization will not allow the author to be responsive to patron needs. </jats:p>
            </jats:sec>
            <jats:sec>
               <jats:title content-type=""abstract-heading"">Design/methodology/approach</jats:title>
               <jats:p> – This is a viewpoint column, and the content is exploratory in nature. </jats:p>
            </jats:sec>
            <jats:sec>
               <jats:title content-type=""abstract-heading"">Findings</jats:title>
               <jats:p> – The findings, in a sense, are that while in the past, others have been trusted to make critical design decisions, the author now needs to focus on applying information science skills to the design of his Web-based services. </jats:p>
            </jats:sec>
            <jats:sec>
               <jats:title content-type=""abstract-heading"">Originality/value</jats:title>
               <jats:p> – The quality of the digital services offered by libraries is a direct correlation of the level of investment offered. Investing at a more substantial level involves risks that need to be weighed against the potential benefits, but those risks cannot be completely ignored if the goal is a higher level of service excellence.</jats:p>
            </jats:sec>",https://doi.org/10.1108/oclc-05-2014-0023,CrossRef
Computer Engineering,Digital Systems Design,Quantitative,"Ethnographic Study: Finger Food Systems, Contribution to a Project Program in Food Design","<jats:p>This study consists of an ethnographic survey of 50 forms of finger food found by the author on the four continents of America, Europe, Africa and Asia, involving around 20 countries, presented under four morphological typologies wrapped, agglutinated, laminated and contained – and five construction systems – plate, oven, steam, water and bain-marie. The raw materials used in the collection are cereals (68%), pulses (16%), tubers (10%) and seaweed/leaves (6%). The literature review identifies exceptional qualities of combining whole grains with pulses as a dietary contribution to reducing obesity and improving public health. The results of this research will contribute to the author’s PhD thesis: design of plant-based mobile finger food, mitigating the hegemony of wheat.</jats:p>",https://doi.org/10.30682/diiddsi23t3c,CrossRef
Computer Engineering,Digital Systems Design,Qualitative,ANALYSIS AND DESIGN OF DIGITAL APPLICATION SYSTEMS FOR EMPLOYEE DATA MANAGEMENT IN XYZ STORE,"<jats:p>As technology and information systems develop in the digital era, managing employee data has become very important for organizations and companies. An efficient and accurate employee data input process is the first step in managing human resources optimally. XYZ Shop is a shop located in the Banten area, which sells retail goods to end consumers, usually on a small scale and at home. The shop provides a variety of products, ranging from clothing, food, electronic goods, to household equipment. However, the XYZ store still faces challenges in managing employee data, such as limited time and human energy in carrying out this process because it is still manual. To overcome this problem, an employee data input program was designed to be an effective solution. With this program, the employee data management process can become more efficient, accurate and organized. The author used three techniques for data collection activities, namely observation, interviews and literature study. Development of an application program system for managing employee data at XYZ store using the PHP programming language and MySql database. The application system built is web-based, so it can be accessed easily anywhere and anytime. This provides immediate benefits in day-to-day operational efficiency, allowing human resources to focus on more strategic tasks. With the ability to manage employee data efficiently, stores can easily adapt to company growth, including branch expansion and increased employment</jats:p>",https://doi.org/10.31000/digibis.v3i1.12348,CrossRef
Computer Engineering,Digital Systems Design,Qualitative,A Research on the Dynamization Effect of Brand Visual Identity Design: Mediated by Digital Information Smart Media,"<jats:p xml:lang=""en"">The article utilizes the literature research method, case study method, and practical verification method. The article discusses brand visual identity and motion graphics design principles. The article outlines dynamic brand visual identity design trends that digital information and AI enable. It explains AI generative models like GAN and diffusion models that generate graphics and effects. Examples like Stable Diffusion and Midjourney show AI's potential for diverse, abstract visuals in motion graphics. AI could also enable interactive effects by combining with AR/VR. Overall, AI can empower dynamic, personalized graphic design and branding. Key points are that dynamic design brings interactivity and better conveys brand meaning. Brand visual design is diversifying, with core brand image and dynamic performance reinforcing each other. AI can boost efficiency, innovation, and meaning in dynamic design. Though mainstream, 2D branding remains relevant. The article highlights the future potential of AI in motion graphics and visual storytelling, as it can generate new interpretations and experiences.</jats:p>",https://doi.org/10.55267/iadt.07.14078,CrossRef
Computer Engineering,Digital Systems Design,Qualitative,Principles of Logic Design with Nanoscale Thin Film Memristive Systems for High Performance Digital Circuit Applications,"<jats:p>The characteristic pinched hysteresis behavior of memristors has been reported by stacks of a variety of materials. This paper aims to examine the principles of logic design using such two terminal memristive systems for high performance digital circuit applications. As against logic design with standard CMOS, the benefits of logic design with memristors have been stated. The realization and operation of memristor based AND and OR hybrid logic gates obtained by integrating memristors with standard CMOS logic have been discussed. The IMPLY and MAGIC logic families have been demonstrated by covering MAGIC NOR and NAND logic gate implementation with MAGIC NOR in detail. A qualitative comparison has been drawn towards the end of the paper to conclude on the suitability and application space for each of the logic families studied in this paper. This work also describes the hybrid CMOS-memristive logic family known as MRL (Memristor Ratioed Logic). With the addition of CMOS inverters, this logic family's OR and AND logic gates, which are based on memristive components, are given a full logic structure and signal restoration. The MRL family, in contrast to earlier memristor-based logic families, is compatible with conventional CMOS logic.</jats:p>",https://doi.org/10.4028/p-90x9b8,CrossRef
Computer Engineering,Digital Systems Design,Qualitative,Design of Small-Phase Time-Variant Low-pass Digital Fractional Differentiators and Integrators,"<jats:p>The design method and the time-variant FIR architecture for real-time estimation of fractional and integer differentials and integrals are presented in this paper. The proposed FIR architecture is divided into two parts. Small-phase filtering, integer differentiation, and fractional differential and integration on the local data are performed by the first part, which is time-invariant. The second part, which is time-variant, handles fractional and global differentiation and integration. The separation of the two parts is necessary because real-time matrix inversion or an extensive analytical solution, which can be computationally intensive for high-order FIR architectures, would be required by a single time-variant FIR architecture. However, matrix inversion is used in the design method to achieve negligible delay in the filtered, differentiated, and integrated signals. The optimum output obtained by the method of least squares results in the negligible delay. The experimental results show that fractional and integer differentiation and integration can be performed by the proposed solution, although the fractional differentiation and integration process is sensitive to the noise and limited resolution of the measurements. In systems that require closed-loop control, disturbance observation, and real-time identification of model parameters, this solution can be implemented.</jats:p>",https://doi.org/10.14313/jamris/2-2024/15,CrossRef
Computer Engineering,Digital Systems Design,Qualitative,The Impacts of Digital Technology on Service Design and Experience Innovation: Case Study of Taiwan’s Cultural Heritage under the COVID-19 Pandemic,"<jats:p>The aim of this research is to identify the digital technology impact and experience innovation of cultural heritages in the context of the epidemic. The authors created an analytical framework and used a qualitative exploratory multi-case study of three cultural heritages in Taiwan. The findings indicate that digital technology has facilitated further innovations in cultural heritages under the epidemic to be closer to consumers’ daily life and more connected with the young generation. Compared to traditional cultural heritages, profit-making cultural heritages need sales of its products to sustain operations, while live streaming, which is interactive, is rising as a new way to promote sales. Using multiple digital platforms can maintain consumers’ interest in the cultural heritages, encouraging follow-up visits and thus resulting in more traffic online and offline. This paper illustrates the advantages of digital technology in the context of the epidemic, highlighting the innovative technology of live streaming and social platforms introduced that are different from the traditional cultural heritages.</jats:p>",https://doi.org/10.3390/systems10050184,CrossRef
Computer Engineering,Digital Systems Design,Mixed Methods,"Visualization of Mobility Digital Twin: Framework Design, Case Study,
  and Future Challenges","A Mobility Digital Twin is an emerging implementation of digital twin
technology in the transportation domain, which creates digital replicas for
various physical mobility entities, such as vehicles, drivers, and pedestrians.
Although a few work have investigated the applications of mobility digital twin
recently, the extent to which it can facilitate safer autonomous vehicles
remains insufficiently explored. In this paper, we first propose visualization
of mobility digital twin, which aims to augment the existing perception systems
in connected and autonomous vehicles through twinning high-fidelity and
manipulable geometry representations for causal traffic participants, such as
surrounding pedestrians and vehicles, in the digital space. An end-to-end
system framework, including image data crowdsourcing, preprocessing,
offloading, and edge-assisted 3D geometry reconstruction, is designed to enable
real-world development of the proposed visualization of mobility digital twin.
We implement the proposed system framework and conduct a case study to assess
the twinning fidelity and physical-to-digital synchronicity within different
image sampling scenarios and wireless network conditions. Based on the case
study, future challenges of the proposed visualization of mobility digital twin
are discussed toward the end of the paper.",http://arxiv.org/abs/2307.09666v1,arXiv
Computer Engineering,Digital Systems Design,Mixed Methods,"Precarious Experiences: Citizens' Frustrations, Anxieties and Burdens of
  an Online Welfare Benefit System","There is a significant overlap between people who are supported by
income-related social welfare benefits, often in precarious situations, and
those who experience greater digital exclusion. We report on a study of
claimants using the UK's Universal Credit online welfare benefit system
designed as, and still, ""digital by default"". Through data collection involving
remote interviews (n=11) and online surveys (n=66), we expose claimants' own
lived experiences interacting with this system. The claimants explain how
digital channels can contribute to an imbalance of power and agency, at a time
when their own circumstances mean they have reduced abilities, resources and
capacities, and where design choices can adversely affect people's utility to
leverage help from their own wider socio-technical ecosystems. We contribute
eight recommendations from these accounts to inform the future design and
development of digital welfare benefit systems for this population, to reduce
digital barriers and harms.",http://arxiv.org/abs/2405.08515v1,arXiv
Computer Engineering,Digital Systems Design,Mixed Methods,"Development and Performance Validation of a Versatile VLBI Digital
  Backend Using the ROACH2 Platform","Customized digital backends for Very Long Baseline Interferometry (VLBI) are
critical components for radio astronomy observatories. There are several
serialized products such as the Digital Baseband Converter (DBBC),
Reconfigurable Open Architecture Computing Hardware (ROACH) Digital BackEnd
(RDBE), and Chinese Data Acquisition System (CDAS). However, the reliance on
high-speed analog-to-digital converters (ADC) and Field Programmable Gate
Arrays (FPGAs) often necessitates dedicated hardware platforms with long
development cycles and prohibitive cost, limiting scalability and adaptability
to evolving observational needs. To address these challenges, we propose a
design leveraging the versatile and cost-effective ROACH2 hardware platform,
developed by CASPER (Collaboration for Astronomy Signal Processing and
Electronics Research). ROACH2's mature technology and streamlined firmware
development capabilities significantly reduce the hardware platform's
development cycle and cost, making it ideal for modern astronomical
applications. This VLBI digital backend, based on the ROACH2 platform,
incorporates key technologies such as Polyphase Filter Banks (PFB) algorithm
implementation, digital complex-to-real baseband signal conversion, Mark5B data
formatter design and two-bit optimal threshold quantization. These features
ensure compatibility with existing systems while providing enhanced
performance. The backend's performance was validated through multi-station VLBI
experiments, demonstrating its ability to achieve good correlation fringes
compared to the customized CDAS2-D system. Furthermore, this platform offers
flexibility for rapid deployment of additional digital backends, such as those
for spectral line observations, showcasing its potential for broader
astronomical applications.",http://arxiv.org/abs/2502.15446v1,arXiv
Computer Engineering,Digital Systems Design,Mixed Methods,Design propositions for nudging in healthcare: Adoption of national electronic health record systems,"<jats:sec><jats:title>Objectives</jats:title><jats:p> Electronic health records (EHRs) are considered important for improving efficiency and reducing costs of a healthcare system. However, the adoption of EHR systems differs among countries and so does the way the decision to participate in EHRs is presented. Nudging is a concept that deals with influencing human behaviour within the research stream of behavioural economics. In this paper, we focus on the effects of the choice architecture on the decision for the adoption of national EHRs. Our study aims to link influences on human behaviour through nudging with the adoption of EHRs to investigate how choice architects can facilitate the adoption of national information systems. </jats:p></jats:sec><jats:sec><jats:title>Methods</jats:title><jats:p> We employ a qualitative explorative research design, namely the case study method. Using theoretical sampling, we selected four cases (i.e., countries) for our study: Estonia, Austria, the Netherlands, and Germany. We collected and analyzed data from various primary and secondary sources: ethnographic observation, interviews, scientific papers, homepages, press releases, newspaper articles, technical specifications, publications from governmental bodies, and formal studies. </jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p> The findings from our European case studies show that designing for EHR adoption should encompass choice architecture elements (i.e., defaults), technical elements (i.e., choice granularity and access transparency), and institutional elements (i.e., regulations for data protection, information campaigns, and financial incentives) in combination. </jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p> Our findings provide insights on the design of the adoption environments of large-scale, national EHR systems. Future research could estimate the magnitude of effects of the determinants. </jats:p></jats:sec>",https://doi.org/10.1177/20552076231181208,CrossRef
Computer Engineering,Digital Systems Design,Mixed Methods,Design of Manufacturing Systems Based on Digital Shadow and Robust Engineering,"<jats:p>In the era of digital transformation, industry is facing multiple challenges due to the need for implementation of the Industry 4.0 standards, as well as the volatility of customer demands. The latter has created the need for the design and operation of more complex manufacturing systems and networks. A case study derived from Process Industries (PIs) is adopted in this research work in order to design a framework for flexible design of production lines, automation of quality control points, and improvement of the performance of the manufacturing system. Therefore, a Digital Shadow of a production line is developed to collect, analyze and identify potential issues (bottlenecks). An edge computing system for reliable and low-latency communications is also implemented. The digital model is validated using statistical Design Of Experiments (DOE) and ANalysis Of VAriance (ANOVA). For the assessment of what-if scenarios, the Digital Shadow model will be used in order to evaluate and find the desired solution. Ultimately, the goal of this research work is to improve the design and performance of the industry’s production section, as well as to increase the production rate and the product mix.</jats:p>",https://doi.org/10.3390/app13085184,CrossRef
Computer Engineering,Digital Systems Design,Design and Development,Integrated Digital Health Systems Design,"<p>The application of information technology in healthcare has focused primarily on the implementation of specific systems such as electronic health records (EHRs) and clinical decision support systems (CDSSs), mainly for intra-enterprise use. However, for the integrated health system (IHS) to function effectively in a complex inter-enterprise healthcare delivery environment, designers must focus on approaches such as soft systems methodology (SSM) to enable the design of robust integrated digital health systems (IDHSs). A service-oriented architecture (SOA) offers a flexible framework for IDHSs to become de-centralized, fully functional and modular systems with interoperability. This article identifies the design issues in IDHS and explores the potential of an SSM methodology-based SOA for the development of interoperable IDHSs. In the process we compare and contrast the functionalist socio-technical approach to the interpretive SSM. We also describe a prototype SOA application for an IDHS setting and discuss challenges in the application of SOA to healthcare.</p>",https://doi.org/10.4018/jitsa.2009070102,CrossRef
Computer Engineering,Digital Systems Design,Design and Development,Systems Architecture Design Pattern Catalog for Developing Digital Twins,"<jats:p>A digital twin is a digital replica of a physical entity to which it is remotely connected. A digital twin can provide a rich representation of the corresponding physical entity and enables sophisticated control for various purposes. Although the concept of the digital twin is largely known, designing digital twins based systems has not yet been fully explored. In practice, digital twins can be applied in different ways leading to different architectural designs. To guide the architecture design process, we provide a pattern-oriented approach for architecting digital twin-based systems. To this end, we propose a catalog of digital twin architecture design patterns that can be reused in the broad context of systems engineering. The patterns support the various phases in the systems engineering life cycle process, and are described using a well-defined pattern documentation template. For illustrating the application of digital twin patterns, we adopt a multi-case study approach in the agriculture and food domain.</jats:p>",https://doi.org/10.3390/s20185103,CrossRef
Computer Engineering,Digital Systems Design,Design and Development,Promotion of Intelligent Digital Computer-Aided Design to the Improvement of Rural Public Environment Design,"<jats:p>In order to optimize the rural public environment, protect and build the characteristics and integrity of the rural public environment. This paper introduces the development and research of a digital design platform based on digital computer, taking parallel robot (cross IV) as the object. The basic functional requirement of the digital integrated design platform is parallel robot. In the design stage, it mainly needs six functional modules, including conceptual design module, detailed design module, simulation design module, electromechanical coupling design module, optimization design module, and database module. Based on the analysis and summary of the digital integrated design process of parallel robot, combined with the actual functional requirements, the key technologies and solutions of the digital design platform are determined, the overall framework of the digital design platform is drawn up, and the implementation process of the platform is planned in detail. The Design personnel involved in the design process and their corresponding responsible contents, and the user authority is managed reasonably. Set the parameters of the driving arm and the driven arm. The outer diameter of the driving arm is 58 mm and the thickness is 2 mm. The outer diameter of the driven arm is 14 mm, and the thickness is 2 mm. Their length can be calculated directly according to the parameters provided in the conceptual design. According to the rules set by the software, the automatic assembly operation is carried out to obtain the whole machine model. After entering the SolidWorks motion module, the dynamic interference inspection is carried out first, and then the simulation analysis is carried out. The results show that the selection is correct when the actual model is consistent with the theoretical model. The correct selection and wide application of CAD technology in the production and construction of building decoration engineering will inevitably bring great changes in construction management and concept and become a new trend and trend for construction enterprises to enhance their competitiveness. The aforementioned computer-aided design is a broad application category. It provides a strong foundation for the design of rural public environment.</jats:p>",https://doi.org/10.1155/2022/6253292,CrossRef
Computer Engineering,Digital Systems Design,Design and Development,Digital twin enhanced agile design of ship pipeline systems,"<ns3:p>The shipbuilding industry plays a pivotal role in national strategic security and economic development, and one critical challenge is the pipeline layout design problem. It predominantly relies on designers’ subjective experience, and it is marked by a lack of efficient knowledge sharing and the absence of smart pipe routing algorithms. This paper proposes an agile design system, which integrates ship pipeline design knowledge management, semi-automatic design that involves frequent interaction with human designers, and automatic rule checking. The framework is refined by digital twin concepts, facilitating close collaboration between physical and digital systems. The paradigm shift holds the potential to substantially enhance the efficiency of ship pipeline layout design, while concurrently reducing the reliance on manual labor.</ns3:p>",https://doi.org/10.12688/digitaltwin.17918.2,CrossRef
Computer Engineering,Digital Systems Design,Design and Development,Digital twin enhanced agile design of ship pipeline systems,"<ns3:p>The shipbuilding industry plays a pivotal role in national strategic security and economic development, and one critical challenge is the pipeline layout design problem. It predominantly relies on designers’ subjective experience, and it is marked by a lack of efficient knowledge sharing and the absence of smart pipe routing algorithms. This paper proposes an agile design system, which integrates ship pipeline design knowledge management, semi-automatic design that involves frequent interaction with human designers, and automatic rule checking. The framework is refined by digital twin concepts, facilitating close collaboration between physical and digital systems. The paradigm shift holds the potential to substantially enhance the efficiency of ship pipeline layout design, while concurrently reducing the reliance on manual labor.</ns3:p>",https://doi.org/10.12688/digitaltwin.17918.1,CrossRef
Computer Engineering,Digital Systems Design,Theoretical / Conceptual,Digital Omotenashi: Toward a Smart Tourism Design Systems,"<jats:p>The tourism industry is currently facing many challenges; one of the main challenges is the lack of having smart tourism systems that make use of the recent advances in information and communication technology. Another challenge is designing such smart tourism systems while embracing diversified tourists’ sustainable values of experience (functional values, social values, emotional values, and epistemic values). In light of these challenges, the overall objective of this work is to design a smart tourism experience-centered system that considers social and technical perspectives. The Socio-Technical Systems theory was adopted as a theoretical foundation, and the Design Science Research methodology was used to develop a smart tourism system and a practical design artifact. A case study from the Japanese tourism context was studied by exploring tourists’ sustainable values of experiences and local staffs’ behaviors. The main problem was the dysfunctional communication between local service staffs and foreign tourists during the service process. After identifying the problem and the objectives, a relevant smart tourism system was synthesized and tested as a design artifact. The results of the utility test of the proposed artifact showed its effectiveness and efficiency in facilitating the service process and in creating multi-dimensional values of experience.</jats:p>",https://doi.org/10.3390/su9122175,CrossRef
Computer Engineering,Digital Systems Design,Theoretical / Conceptual,Online Simulation of Illustration Patterns Based on Digital Art Design,"<jats:p>Illustration art is an important part of graphic design, and is an intuitive means of artistic expression. Based on the gradual development of social networks and modern illustration art, digital illustration technology has become the main means of expression in modern graphic design. Starting from the basic concept of illustration art in the digital age, this study analyzes the application of digital illustration art and promotes the innovative development of the artistic expression of graphic design in China. This study will combine the traditional Chinese painting language with digital illustration creation in an inquiry-based manner, optimize the standard genetic algorithm, verify it through experimental simulation, and find that the model is more innovative based on the original model. This study provides a certain theoretical basis for the development of digital illustration in China and has a certain propaganda effect on the traditional Chinese painting language.</jats:p>",https://doi.org/10.1155/2022/3273364,CrossRef
Computer Engineering,Digital Systems Design,Theoretical / Conceptual,ELDERLY-CENTERED DESIGN APPROACH FOR MOBILE CHAT APPLICATION,"<jats:p>Connection and communication are heavily contingent on the internet and mobile apps, especially in this pandemic. With the blessings of Information and Communication Technology (ICT), the population of this digital era are highly dependent on several mobile applications. Undoubtedly, mobile apps have made our life easier and more straightforward, especially in the communication sector. There are at least hundreds of chat applications worldwide. Most of these communication platforms have been developed with many interactive and complex features designed mainly for the younger generation. However, in the perspective of Bangladesh, the older generation is unfamiliar and less comfortable with the concept and lacks the necessary digital literacy to use these advanced communication platforms. They frequently encounter difficulties in comprehending and using those applications. Thus, the authors described human-computer interaction (HCI) and briefly discussed some of the design issues from older adults' experiences in this report. Furthermore, a suitably designed chat application was  proposed based on understanding the challenges faced by the elderly. With a user-centered approach, the design emphasizes simplicity and minimalism, making it easier for the elderly to communicate through mobile technology.</jats:p>",https://doi.org/10.31436/jisdt.v4i1.263,CrossRef
Computer Engineering,Digital Systems Design,Theoretical / Conceptual,Design and Research of Digital Media Art Display Based on Virtual Reality and Augmented Reality,"<jats:p>Purpose. To serve as a reference for the evolution of the online digital art display industry, as well as to conduct further studies in the field of digital entertainment art in the showcase design business in order to give solid evidentiary assurances, this article presents a virtual reality which is a technology reality-based digital media art exhibition design with the goal of examining the new construction trend of online media art with in context of existing period and the use of sophisticated technology. Implications. This paper enriches the theory, skills, and means of digital cultural communication, opens up a broader space and vision for digital media art communication, enriches the communication skills and means of digital media, and provides flexible and efficient ideas and methods for the dissemination of digital media art, which is of practical significance for realizing the active and effective dissemination of digital media art. Methodology. The method of this paper is to use the digital three-dimensional panorama technology of virtual reality to explore the digital media art display and digital media art expression form of augmented reality. The role of these methods is to solve the problem of spatial positioning of virtual 3D objects in real scenes and to judge the final detection model, the problem of model making to satisfy AR computing power, and the problem of scene interaction. Research Findings. Through a mix of digital content artwork and virtual reality and augmented reality technologies, this research examines the impact of AR and VR in digital art and analyzes and summarizes a series of design strategies for digital media art display projects. The results show that people are 33.6% more satisfied with VR and AR displays than traditional displays.</jats:p>",https://doi.org/10.1155/2022/6606885,CrossRef
Computer Engineering,Digital Systems Design,Theoretical / Conceptual,ANFIS Based Thermal Estimation of Ultradeep Submicron Digital Circuit Design,"<jats:p>In this paper, the use of the Adaptive Neuro Fuzzy Inference System (ANFIS) to model the CMOS inverter is discussed as a tool for developing and simulating CMOS logic circuits at the ultradeep submicron technology node of 22nm. The ANFIS structures are built and trained using MATLAB software. The ANFIS network was trained using data obtained from the analytical model (at 298.15K and 398.15K). For training, two methodologies are used: a hybrid learning method based on back-propagation and least-squares estimation, and back-propagation. The effect of the ANFIS model's structure on the accuracy and performance of the CMOS inverter has also been investigated. Further, simulation through HSPICE using (Predictive Technology Model) PTM nominal parameters has been done to compare with ANFIS (trained using an analytical model) results. The comparison of ANFIS and HSPICE suggests the ANFIS modelling procedure's practicality and correctness. The findings demonstrate that the ANFIS simulation is significantly faster and more comparable than the HSPICE simulation and that it can be easily integrated into software tools for designing and simulating complicated CMOS logic circuits.</jats:p>",https://doi.org/10.29292/jics.v16i3.507,CrossRef
Computer Engineering,Signal Processing,Quantitative,To further understand graph signals,"Graph signal processing (GSP) is a framework to analyze and process
graph-structured data. Many research works focus on developing tools such as
Graph Fourier transforms (GFT), filters, and neural network models to handle
graph signals. Such approaches have successfully taken care of ``signal
processing'' in many circumstances. In this paper, we want to put emphasis on
``graph signals'' themselves. Although there are characterizations of graph
signals using the notion of bandwidth derived from GFT, we want to argue here
that graph signals may contain hidden geometric information of the network,
independent of (graph) Fourier theories. We shall provide a framework to
understand such information, and demonstrate how new knowledge on ``graph
signals'' can help with ``signal processing''.",http://arxiv.org/abs/2203.00832v2,arXiv
Computer Engineering,Signal Processing,Quantitative,Topological Signal Processing over Simplicial Complexes,"The goal of this paper is to establish the fundamental tools to analyze
signals defined over a topological space, i.e. a set of points along with a set
of neighborhood relations. This setup does not require the definition of a
metric and then it is especially useful to deal with signals defined over
non-metric spaces. We focus on signals defined over simplicial complexes. Graph
Signal Processing (GSP) represents a special case of Topological Signal
Processing (TSP), referring to the situation where the signals are associated
only with the vertices of a graph. Even though the theory can be applied to
signals of any order, we focus on signals defined over the edges of a graph and
show how building a simplicial complex of order two, i.e. including triangles,
yields benefits in the analysis of edge signals. After reviewing the basic
principles of algebraic topology, we derive a sampling theory for signals of
any order and emphasize the interplay between signals of different order. Then
we propose a method to infer the topology of a simplicial complex from data. We
conclude with applications to real edge signals and to the analysis of discrete
vector fields to illustrate the benefits of the proposed methodologies.",http://arxiv.org/abs/1907.11577v2,arXiv
Computer Engineering,Signal Processing,Quantitative,Recovery of Graph Signals from Sign Measurements,"Sampling and interpolation have been extensively studied, in order to
reconstruct or estimate the entire graph signal from the signal values on a
subset of vertexes, of which most achievements are about continuous signals.
While in a lot of signal processing tasks, signals are not fully observed, and
only the signs of signals are available, for example a rating system may only
provide several simple options. In this paper, the reconstruction of
band-limited graph signals based on sign sampling is discussed and a greedy
sampling strategy is proposed. The simulation experiments are presented, and
the greedy sampling algorithm is compared with random sampling algorithm, which
verify the validity of the proposed approach.",http://arxiv.org/abs/2109.12576v1,arXiv
Computer Engineering,Signal Processing,Quantitative,Fast and Accurate Amplitude Demodulation of Wideband Signals,"Amplitude demodulation is a classical operation used in signal processing.
For a long time, its effective applications in practice have been limited to
narrowband signals. In this work, we generalize amplitude demodulation to
wideband signals. We pose demodulation as a recovery problem of an oversampled
corrupted signal and introduce special iterative schemes belonging to the
family of alternating projection algorithms to solve it. Sensibly chosen
structural assumptions on the demodulation outputs allow us to reveal the high
inferential accuracy of the method over a rich set of relevant signals. This
new approach surpasses current state-of-the-art demodulation techniques apt to
wideband signals in computational efficiency by up to many orders of magnitude
with no sacrifice in quality. Such performance opens the door for applications
of the amplitude demodulation procedure in new contexts. In particular, the new
method makes online and large-scale offline data processing feasible, including
the calculation of modulator-carrier pairs in higher dimensions and poor
sampling conditions, independent of the signal bandwidth. We illustrate the
utility and specifics of applications of the new method in practice by using
natural speech and synthetic signals.",http://arxiv.org/abs/2102.04832v2,arXiv
Computer Engineering,Signal Processing,Quantitative,Parametric Modeling of Non-Stationary Signals,"Parametric modeling of non-stationary signals is addressed in this article.
We present several models based on the characteristic features of the modeled
signal, together with the methods for accurate estimation of model parameters.
Non-stationary signals, viz. transient system response, speech phonemes, and
electrocardiograph signal are fitted by these feature-based models.",http://arxiv.org/abs/1801.09045v1,arXiv
Computer Engineering,Signal Processing,Qualitative,Tensor Low Rank Modeling and Its Applications in Signal Processing,"Modeling of multidimensional signal using tensor is more convincing than
representing it as a collection of matrices. The tensor based approaches can
explore the abundant spatial and temporal structures of the mutlidimensional
signal. The backbone of this modeling is the mathematical foundations of tensor
algebra. The linear transform based tensor algebra furnishes low complex and
high performance algebraic structures suitable for the introspection of the
multidimensional signal. A comprehensive introduction of the linear transform
based tensor algebra is provided from the signal processing viewpoint. The rank
of a multidimensional signal is a precious property which gives an insight into
the structural aspects of it. All natural multidimensional signals can be
approximated to a low rank signal without losing significant information. The
low rank approximation is beneficial in many signal processing applications
such as denoising, missing sample estimation, resolution enhancement,
classification, background estimation, object detection, deweathering,
clustering and much more applications. Detailed case study of the ways and
means of the low rank modeling in the above said signal processing applications
are also presented.",http://arxiv.org/abs/1912.03435v1,arXiv
Computer Engineering,Signal Processing,Qualitative,Defining Fundamental Frequency for Almost Harmonic Signals,"In this work, we consider the modeling of signals that are almost, but not
quite, harmonic, i.e., composed of sinusoids whose frequencies are close to
being integer multiples of a common frequency. Typically, in applications, such
signals are treated as perfectly harmonic, allowing for the estimation of their
fundamental frequency, despite the signals not actually being periodic. Herein,
we provide three different definitions of a concept of fundamental frequency
for such inharmonic signals and study the implications of the different choices
for modeling and estimation. We show that one of the definitions corresponds to
a misspecified modeling scenario, and provides a theoretical benchmark for
analyzing the behavior of estimators derived under a perfectly harmonic
assumption. The second definition stems from optimal mass transport theory and
yields a robust and easily interpretable concept of fundamental frequency based
on the signals' spectral properties. The third definition interprets the
inharmonic signal as an observation of a randomly perturbed harmonic signal.
This allows for computing a hybrid information theoretical bound on estimation
performance, as well as for finding an estimator attaining the bound. The
theoretical findings are illustrated using numerical examples.",http://arxiv.org/abs/2003.10767v2,arXiv
Computer Engineering,Signal Processing,Qualitative,"Hilbert Transform, Analytic Signal, and Modulation Analysis for Graph
  Signal Processing","We propose Hilbert transform (HT) and analytic signal (AS) construction for
signals over graphs. This is motivated by the popularity of HT, AS, and
modulation analysis in conventional signal processing, and the observation that
complementary insight is often obtained by viewing conventional signals in the
graph setting. Our definitions of HT and AS use a conjugate-symmetry-like
property exhibited by the graph Fourier transform (GFT). We show that a real
graph signal (GS) can be represented using smaller number of GFT coefficients
than the signal length. We show that the graph HT (GHT) and graph AS (GAS)
operations are linear and shift-invariant over graphs. Using the GAS, we define
the amplitude, phase, and frequency modulations for a graph signal (GS).
Further, we use convex optimization to develop an alternative definition of
envelope for a GS. We illustrate the proposed concepts by showing applications
to synthesized and real-world signals. For example, we show that the GHT is
suitable for anomaly detection/analysis over networks and that GAS reveals
complementary information in speech signals.",http://arxiv.org/abs/1611.05269v3,arXiv
Computer Engineering,Signal Processing,Qualitative,An interstellar communication method: system design and observations,"A system of synchronized radio telescopes is utilized to search for
hypothetical wide bandwidth interstellar communication signals. Transmitted
signals are hypothesized to have characteristics that enable high channel
capacity and minimally low energy per information bit, while containing
energy-efficient signal elements that are readily discoverable, distinct from
random noise. A hypothesized transmitter signal is described. Signal reception
and discovery processes are detailed. Observations using individual and
multiple synchronized radio telescopes, during 2017 - 2021, are described.
Conclusions and further work are suggested.",http://arxiv.org/abs/2105.03727v2,arXiv
Computer Engineering,Signal Processing,Qualitative,Blind Deconvolution of Graph Signals: Robustness to Graph Perturbations,"We study blind deconvolution of signals defined on the nodes of an undirected
graph. Although observations are bilinear functions of both unknowns, namely
the forward convolutional filter coefficients and the graph signal input, a
filter invertibility requirement along with input sparsity allow for an
efficient linear programming reformulation. Unlike prior art that relied on
perfect knowledge of the graph eigenbasis, here we derive stable recovery
conditions in the presence of small graph perturbations. We also contribute a
provably convergent robust algorithm, which alternates between blind
deconvolution of graph signals and eigenbasis denoising in the Stiefel
manifold. Reproducible numerical tests showcase the algorithm's robustness
under several graph eigenbasis perturbation models.",http://arxiv.org/abs/2412.15133v1,arXiv
Computer Engineering,Signal Processing,Mixed Methods,"Teaching Digital Signal Processing by Partial Flipping, Active Learning
  and Visualization","Effectiveness of teaching digital signal processing can be enhanced by
reducing lecture time devoted to theory, and increasing emphasis on
applications, programming aspects, visualization and intuitive understanding.
An integrated approach to teaching requires instructors to simultaneously teach
theory and its applications in storage and processing of audio, speech and
biomedical signals. Student engagement can be enhanced by engaging students to
work in groups during the class where students can solve short problems and
short programming assignments or take quizzes. These approaches will increase
student interest in learning the subject and student engagement.",http://arxiv.org/abs/2102.00561v1,arXiv
Computer Engineering,Signal Processing,Mixed Methods,Signal Processing on Directed Graphs,"This paper provides an overview of the current landscape of signal processing
(SP) on directed graphs (digraphs). Directionality is inherent to many
real-world (information, transportation, biological) networks and it should
play an integral role in processing and learning from network data. We thus lay
out a comprehensive review of recent advances in SP on digraphs, offering
insights through comparisons with results available for undirected graphs,
discussing emerging directions, establishing links with related areas in
machine learning and causal inference in statistics, as well as illustrating
their practical relevance to timely applications. To this end, we begin by
surveying (orthonormal) signal representations and their graph frequency
interpretations based on novel measures of signal variation for digraphs. We
then move on to filtering, a central component in deriving a comprehensive
theory of SP on digraphs. Indeed, through the lens of filter-based generative
signal models, we explore a unified framework to study inverse problems (e.g.,
sampling and deconvolution on networks), statistical analysis of random
signals, and topology inference of digraphs from nodal observations.",http://arxiv.org/abs/2008.00586v1,arXiv
Computer Engineering,Signal Processing,Mixed Methods,"Unique Bispectrum Inversion for Signals with Finite Spectral/Temporal
  Support","Retrieving a signal from its triple correlation spectrum, also called
bispectrum, arises in a wide range of signal processing problems. Conventional
methods do not provide an accurate inversion of bispectrum to the underlying
signal. In this paper, we present an approach that uniquely recovers signals
with finite spectral support (band-limited signals) from at least $3B$
measurements of its bispectrum function (BF), where $B$ is the signal's
bandwidth. Our approach also extends to time-limited signals. We propose a
two-step trust region algorithm that minimizes a non-convex objective function.
First, we approximate the signal by a spectral algorithm and then refine the
attained initialization based on a sequence of gradient iterations. Numerical
experiments suggest that our proposed algorithm is able to estimate
band-/time-limited signals from its BF for both complete and undersampled
observations.",http://arxiv.org/abs/2111.06479v3,arXiv
Computer Engineering,Signal Processing,Mixed Methods,A Geometric Algebra Framework for a Multidimensional Analytic Signal,"This work examines the problem of extending the one-dimensional analytic
signal, which is ubiquitous throughout signal processing, to higher dimensional
signals. Bulow et al. and Felsberg et al. have previously used techniques from
Clifford algebra and analysis to extend the one-dimensional analytic signal to
higher dimensions. However, each author sets forth a different definition of a
multidimensional analytic signal. Herein we follow an observation of Brackx et
al. and adopt a general definition of an analytic signal that encompasses both
the hypercomplex signal of Bulow et al. and the monogenic signal of Felsberg et
al. within the same mathematical framework. The crux of our approach is
captured by the following statement: A multidimensional analytic signal is
generated by an idempotent. We develop this notion more specifically using
examples from geometric algebra.",http://arxiv.org/abs/2411.10412v1,arXiv
Computer Engineering,Signal Processing,Mixed Methods,"3-D generalized analytic signal associated with linear canonical
  transform in Clifford biquaternion domain","The analytic signal is a useful mathematical tool. It separates qualitative
and quantitative information of a signal in form of the local phase and local
amplitude. The Clifford Fourier transform (CFT) plays a vital role in the
representation of multidimensional signals. By generalizing the CFT to the
Clifford linear canonical transform (CLCT), we present a new type of Clifford
biquaternionic analytic signal. Due to the advantages of more freedom, the
envelop detection problems of 3D images, with the help of this new analytic
signal, can get a better visual appearance. Synthesis examples are presented to
demonstrate these advantages.",http://arxiv.org/abs/2204.12787v1,arXiv
Computer Engineering,Signal Processing,Design and Development,Graph signal processing with categorical perspective,"In this paper, we propose a framework for graph signal processing using
category theory. The aim is to generalize a few recent works on probabilistic
approaches to graph signal processing, which handle signal and graph
uncertainties.",http://arxiv.org/abs/2302.12421v1,arXiv
Computer Engineering,Signal Processing,Design and Development,Signal Transformation for Effective Multi-Channel Signal Processing,"Electroencephalography (EEG) is an non-invasive method to record the
electrical activity of the brain. The EEG signals are low bandwidth and
recorded from multiple electrodes simultaneously in a time synchronized manner.
Typical EEG signal processing involves extracting features from all the
individual channels separately and then fusing these features for downstream
applications. In this paper, we propose a signal transformation, using basic
signal processing, to combine the individual channels of a low-bandwidth
signal, like the EEG into a single-channel high-bandwidth signal, like audio.
Further this signal transformation is bi-directional, namely the high-bandwidth
single-channel can be transformed to generate the individual low-bandwidth
signals without any loss of information. Such a transformation when applied to
EEG signals overcomes the need to process multiple signals and allows for a
single-channel processing. The advantage of this signal transformation is that
it allows the use of pre-trained single-channel pre-trained models, for
multi-channel signal processing and analysis. We further show the utility of
the signal transformation on publicly available EEG dataset.",http://arxiv.org/abs/2412.17478v1,arXiv
Computer Engineering,Signal Processing,Design and Development,"Towards Goal-Oriented Semantic Signal Processing: Applications and
  Future Challenges","Advances in machine learning technology have enabled real-time extraction of
semantic information in signals which can revolutionize signal processing
techniques and improve their performance significantly for the next generation
of applications. With the objective of a concrete representation and efficient
processing of the semantic information, we propose and demonstrate a formal
graph-based semantic language and a goal filtering method that enables
goal-oriented signal processing. The proposed semantic signal processing
framework can easily be tailored for specific applications and goals in a
diverse range of signal processing applications. To illustrate its wide range
of applicability, we investigate several use cases and provide details on how
the proposed goal-oriented semantic signal processing framework can be
customized. We also investigate and propose techniques for communications where
sensor data is semantically processed and semantic information is exchanged
across a sensor network.",http://arxiv.org/abs/2109.11885v1,arXiv
Computer Engineering,Signal Processing,Design and Development,"Signal denoising based on the Schrödinger operator's eigenspectrum and
  a curvature constraint","Recently, a new Signal processing method, named Semi-Classical Signal
Analysis (SCSA), has been proposed for denoising Magnetic Resonance
Spectroscopy (MRS) signals. It is based on the Schr\""odinger Operator's
eigenspectrum. It allows an efficient noise reduction while preserving MRS
signal's peaks. In this paper, we propose to extend this approach to different
signals, in particular pulse shaped signals, by including an optimization that
considers curvature constraints. The performance of the method is measured by
analyzing noisy signal data and comparing with other denoising methods. Results
indicate that the proposed method not only produces good denoising performance
but also guarantees the peaks are well preserved in the denoising process.",http://arxiv.org/abs/1908.07758v1,arXiv
Computer Engineering,Signal Processing,Design and Development,Number Theoretic Transforms for Secure Signal Processing,"Multimedia contents are inherently sensitive signals that must be protected
whenever they are outsourced to an untrusted environment. This problem becomes
a challenge when the untrusted environment must perform some processing on the
sensitive signals; a paradigmatic example is Cloud-based signal processing
services. Approaches based on Secure Signal Processing (SSP) address this
challenge by proposing novel mechanisms for signal processing in the encrypted
domain and interactive secure protocols to achieve the goal of protecting
signals without disclosing the sensitive information they convey.
  This work presents a novel and comprehensive set of approaches and primitives
to efficiently process signals in an encrypted form, by using Number Theoretic
Transforms (NTTs) in innovative ways. This usage of NTTs paired with
appropriate signal pre- and post-coding enables a whole range of easily
composable signal processing operations comprising, among others, filtering,
generalized convolutions, matrix-based processing or error correcting codes.
The main focus is on unattended processing, in which no interaction from the
client is needed; for implementation purposes, efficient lattice-based somewhat
homomorphic cryptosystems are used. We exemplify these approaches and evaluate
their performance and accuracy, proving that the proposed framework opens up a
wide variety of new applications for secured outsourced-processing of
multimedia contents.",http://arxiv.org/abs/1607.05229v2,arXiv
Computer Engineering,Signal Processing,Theoretical / Conceptual,"The performance of microwave photonic signal processors based on
  microcombs with different input signal waveforms","Microwave photonic (MWP) signal processors, which process microwave signals
based on pho-tonic technologies, bring advantages intrinsic to photonics such
as low loss, large processing bandwidth, and strong immunity to electromagnetic
interference. Optical microcombs can offer a large number of wavelength
channels and compact device footprints, which make them powerful
multi-wavelength sources for MWP signal processors to realize a variety of
processing functions. In this paper, we experimentally demonstrate the
capability of microcomb-based MWP signal processors to handle diverse input
signal waveforms. In addition, we quantify the processing accuracy for
different input signal waveforms, including Gaussian, triangle, parabolic,
super Gaussian, and nearly square waveforms. Finally, we analyze the factors
contributing to the dif-ference in the processing accuracy among the different
input waveforms, and our theoretical analysis well elucidates the experimental
results. These results provide a guidance for micro-comb-based MWP signal
processors when processing microwave signals of various waveforms.",http://arxiv.org/abs/2403.12978v1,arXiv
Computer Engineering,Signal Processing,Theoretical / Conceptual,"Theory for the Accuracy of Microcomb Photonic Microwave Transversal
  Signal Processors","Photonic RF transversal signal processors, which are equivalent to
reconfigurable electrical digital signal processors but implemented with
photonic technologies, have been widely used for modern high-speed information
processing. With the capability of generating large numbers of wavelength
channels with compact micro-resonators, optical microcombs bring new
opportunities for realizing photonic RF transversal signal processors that have
greatly reduced size, power consumption, and complexity. Recently, a variety of
signal processing functions have been demonstrated using microcomb-based
photonic RF transversal signal processors. Here, we provide detailed analysis
for quantifying the processing accuracy of microcomb-based photonic RF
transversal signal processors. First, we investigate the theoretical
limitations of the processing accuracy determined by tap number, signal
bandwidth, and pulse waveform. Next, we discuss the practical error sources
from different components of the signal processors. Finally, we analyze the
contributions of the theoretical limitations and the experimental factors to
the overall processing inaccuracy both theoretically and experimentally. These
results provide a useful guide for designing microcomb-based photonic RF
transversal signal processors to optimize their accuracy.",http://arxiv.org/abs/2304.04130v1,arXiv
Computer Engineering,Signal Processing,Theoretical / Conceptual,Modelling Underwater Acoustic Propagation using One-way Wave Equations,"The primary contribution of this paper is to characterize the propagation of
acoustic signal carrying information through any medium and the interaction of
the travelling acoustic signal with the surrounding medium. We will use the
concept of damped harmonic oscillator to model the medium and Milne's
oscillator technique to map the interaction of the acoustic signal with the
medium. The acoustic signal itself will be modelled using the one-way wave
equation formulated in terms of acoustic pressure and velocity of acoustic
waves through the medium. Using the above-mentioned concepts, we calculated the
effective signal strength, phase shift and time period of the communicated
signal. Numerical results are generated to present the evolution of signal
strength and received signal envelope in underwater environment.",http://arxiv.org/abs/2202.06559v1,arXiv
Computer Engineering,Signal Processing,Theoretical / Conceptual,"Near-Field Localization and Sensing with Large-Aperture Arrays: From
  Signal Modeling to Processing","The signal processing community is currently witnessing a growing interest in
near-field signal processing, driven by the trend towards the use of large
aperture arrays with high spatial resolution in the fields of communication,
localization, sensing, imaging, etc. From the perspective of localization and
sensing, this trend breaks the basic far-field assumptions that have dominated
the array signal processing research in the past, presenting new challenges and
promising opportunities.",http://arxiv.org/abs/2406.10941v2,arXiv
Computer Engineering,Signal Processing,Theoretical / Conceptual,"An Overview of Signal Processing Techniques for Joint Communication and
  Radar Sensing","Joint communication and radar sensing (JCR) represents an emerging research
field aiming to integrate the above two functionalities into a single system,
sharing a majority of hardware and signal processing modules and, in a typical
case, sharing a single transmitted signal. It is recognised as a key approach
in significantly improving spectrum efficiency, reducing device size, cost and
power consumption, and improving performance thanks to potential close
cooperation of the two functions. Advanced signal processing techniques are
critical for making the integration efficient, from transmission signal design
to receiver processing. This paper provides a comprehensive overview of JCR
systems from the signal processing perspective, with a focus on
state-of-the-art. A balanced coverage on both transmitter and receiver is
provided for three types of JCR systems, communication-centric, radar-centric,
and joint design and optimization.",http://arxiv.org/abs/2102.12780v1,arXiv
Computer Engineering,Cyber-Physical Systems,Quantitative,Fuzzy Control in Cyber-Physical Systems,"<p>Controllers are devices regulating the operation of other devices or systems. Fuzzy controllers analyze the input data in terms of variables which take on continuous values in the interval [0, 1]. Since fuzzy logic has the advantage of expressing the solution of the problems in the natural language, the use of fuzzy instead of traditional controllers makes easier the mechanization of tasks that have been already successfully performed by humans. In the present paper a theoretical fuzzy control model is developed for the braking system of autonomous vehicles, which are included among the most characteristic examples of Cyber-Physical Systems. For this, a simple geometric approach is followed using triangular fuzzy numbers as the basic tools.</p>",https://doi.org/10.4018/ijcps.2020070103,CrossRef
Computer Engineering,Cyber-Physical Systems,Quantitative,Evaluating Robustness of Learning-Enabled Medical Cyber-Physical Systems with Naturally Adversarial Datasets,"<jats:p>
            Medical cyber-physical systems (MCPS) are increasingly adopting learning-enabled components (LECs) to enhance their decision-making capabilities. Due to the safety-critical nature of MCPS, these systems must maintain high performance on both expected and unexpected input data. Therefore, ensuring the robustness of LE-MCPS is crucial for their successful deployment. Existing research predominantly focuses on robustness to
            <jats:italic>synthetic adversarial examples</jats:italic>
            , crafted by adding imperceptible perturbations to clean input data. However, these synthetic adversarial examples do not accurately reflect the most challenging real-world scenarios, especially in the context of healthcare data. Consequently, robustness to synthetic adversarial examples may not necessarily translate to robustness against
            <jats:italic>naturally occurring adversarial examples</jats:italic>
            . We propose a method to evaluate the robustness of LE-MCPS to natural adversarial examples. The method curates naturally adversarial datasets leveraging probabilistic labels obtained from automated weakly-supervised labeling which combines noisy and cheap-to-obtain labeling heuristics. Based on these labels, the method adversarially orders the input data and uses this ordering to construct a sequence of increasingly adversarial datasets for assessing robustness. Our evaluation on six medical CPS case studies and two non-medical case studies demonstrates (1) the efficacy and statistical validity of our approach to generating naturally adversarial datasets, and (2) the utility of our robustness evaluation in classifying robust and non-robust LE-MCPS.
          </jats:p>",https://doi.org/10.1145/3734695,CrossRef
Computer Engineering,Cyber-Physical Systems,Quantitative,Model Conformance for Cyber-Physical Systems,"<jats:p>Model-based development is an important paradigm for developing cyber-physical systems (CPS). The underlying assumption is that the functional behavior of a model is related to the behavior of a more concretized model or the real system. A formal definition of such a relation is called conformance relation. There are a variety of conformance relations, and the question arises of how to select a conformance relation for the development of CPS. The contribution of this article is a survey of the definitions and algorithms of conformance relations for CPS. Additionally, the article compares several conformance relations and provides guidance on which relation to select for specific problems. Finally, we discuss how to select inputs for testing conformance.</jats:p>",https://doi.org/10.1145/3306157,CrossRef
Computer Engineering,Cyber-Physical Systems,Quantitative,Adaptive embedded control of cyber‐physical systems using reinforcement learning,"<jats:p>Embedded control parameters of cyber‐physical systems (CPS), such as sampling rate, are typically invariant and designed with a worst case scenario in mind. In an over‐engineered system, control parameters are assigned values that satisfy system‐wide performance requirements at the expense of excessive energy and resource overheads. Dynamic and adaptive control parameters can reduce the overhead but are complex and require in‐depth knowledge of the CPS and its operating environment – which typically is unavailable during design time. The authors investigate the application of reinforcement learning (RL) to dynamically adapt high‐level system parameters, at run time, as a function of the system state. RL is an alternative approach to the classical control theory for CPSs that can learn and adapt control properties without the need of an in‐depth controller model. Specifically, we show that RL can modulate sampling times to save processing power without compromising control quality. We apply a novel statistical cloud‐based evaluation framework to study the validity of our approach for the cart‐pole balancing control problem as well as the well‐known mountain car problem. The results show an improved real‐world power efficiency of up to 20% compared with an optimal system with fixed controller settings.</jats:p>",https://doi.org/10.1049/iet-cps.2017.0048,CrossRef
Computer Engineering,Cyber-Physical Systems,Quantitative,Modelling and evaluation of the security of cyber‐physical systems using stochastic Petri nets,"<jats:p>This study proposes a stochastic Petri net model for evaluating the security and resilience of cyber‐physical systems (CPSs) in the face of malicious attacks. The basic idea behind the proposed model is to evaluate the security of control loops equipped with intrusion detection systems (IDSs) faced with security attacks. The quantitative analysis is performed in terms of system‐focused quantitative security measures, such as mean time‐to‐failure and availability. By using this model, one can investigate the effects of some attacks and defensive parameters, including the detection interval, the time to physical disruption, and the false‐positive probability of IDSs. This evaluation results can help to improve the security countermeasures of CPSs.</jats:p>",https://doi.org/10.1049/iet-cps.2018.0008,CrossRef
Computer Engineering,Cyber-Physical Systems,Qualitative,Systems of Gibbons-Tsarev type and integrable 3-dimensional models,"We review the role of Gibbons-Tsarev-type systems in classification of
integrable multi-dimensional hydrodynamic-type systems. Our main observation is
an universality of Gibbons-Tsarev-type systems. We also constract explicitly a
wide class of 3-dimensional hydrodynamic-type systems corresponding to the
simplest possible Gibbons-Tsarev-type system.",http://arxiv.org/abs/0906.3509v1,arXiv
Computer Engineering,Cyber-Physical Systems,Qualitative,Confidentiality Breach Through Acoustic Side-Channel in Cyber-Physical Additive Manufacturing Systems,"<jats:p>In cyber-physical systems, due to the tight integration of the computational, communication, and physical components, most of the information in the cyber-domain manifests in terms of physical actions (such as motion, temperature change, etc.). This leads to the system being prone to physical-to-cyber domain attacks that affect the confidentiality. Physical actions are governed by energy flows, which may be observed. Some of these observable energy flows unintentionally leak information about the cyber-domain and hence are known as the side-channels. Side-channels such as acoustic, thermal, and power allow attackers to acquire the information without actually leveraging the vulnerability of the algorithms implemented in the system. As a case study, we have taken cyber-physical additive manufacturing systems (fused deposition modeling-based three-dimensional (3D) printer) to demonstrate how the acoustic side-channel can be used to breach the confidentiality of the system. In 3D printers, geometry, process, and machine information are the intellectual properties, which are stored in the cyber domain (G-code). We have designed an attack model that consists of digital signal processing, machine-learning algorithms, and context-based post processing to steal the intellectual property in the form of geometry details by reconstructing the G-code and thus the test objects. We have successfully reconstructed various test objects with an average axis prediction accuracy of 86% and an average length prediction error of 11.11%.</jats:p>",https://doi.org/10.1145/3078622,CrossRef
Computer Engineering,Cyber-Physical Systems,Qualitative,Analysing Mission-critical Cyber-physical Systems with AND/OR Graphs and MaxSAT,"<jats:p>
            Cyber-Physical Systems (CPS) often involve complex networks of interconnected software and hardware components that are logically combined to achieve a common goal or mission; for example, keeping a plane in the air or providing energy to a city. Failures in these components may jeopardise the mission of the system. Therefore, identifying the minimal set of critical CPS components that is most likely to fail, and prevent the global system from accomplishing its mission, becomes essential to ensure reliability. In this article, we present a novel approach to identifying the
            <jats:italic>Most Likely Mission-critical Component Set (MLMCS)</jats:italic>
            using AND/OR dependency graphs enriched with independent failure probabilities. We address the MLMCS problem as a Maximum Satisfiability (MaxSAT) problem. We translate probabilities into a negative logarithmic space to linearise the problem within MaxSAT. The experimental results conducted with our open source tool LDA4CPS indicate that the approach is both effective and efficient. We also present a case study on complex aircraft systems that shows the feasibility of our approach and its applicability to mission-critical cyber-physical systems. Finally, we present two MLMCS-based security applications focused on system hardening and forensic investigations.
          </jats:p>",https://doi.org/10.1145/3451169,CrossRef
Computer Engineering,Cyber-Physical Systems,Qualitative,Holistic Cyber-Physical Management for Dependable Wireless Control Systems,"<jats:p>Wireless sensor-actuator networks (WSANs) are gaining momentum in industrial process automation as a communication infrastructure for lowering deployment and maintenance costs. In traditional wireless control systems, the plant controller and the network manager operate in isolation, which ignores the significant influence of network reliability on plant control performance. To enhance the dependability of industrial wireless control, we propose a holistic cyber-physical management framework that employs runtime coordination between the plant control and network management. Our design includes a holistic controller that generates actuation signals to physical plants and reconfigures the WSAN to maintain the desired control performance while saving wireless resources. As a concrete example of holistic control, we design a holistic manager that dynamically reconfigures the number of transmissions in the WSAN based on online observations of physical and cyber variables. We have implemented the holistic management framework in the wireless cyber-physicalsimulator (WCPS). A systematic case study is presented based on two five-state plants and a load positioning system using a 16-node WSAN. Simulation results show that the holistic management design has significantly enhanced the dependability of the system against both wireless interferences and physical disturbances, while effectively reducing the number of wireless transmissions.</jats:p>",https://doi.org/10.1145/3185510,CrossRef
Computer Engineering,Cyber-Physical Systems,Qualitative,In the mind of an insider attacker on cyber‐physical systems and how not being fooled,"<jats:p>Insider attacks are one of the most serious threats for cyber‐physical systems, they have potentials to inflict destructive damages on physical processes while remaining stealthy. This study dissects several insider attacks by examining their modes of data tampering. To set the scene, a general framework of a cyber‐physical system is constructed, a pattern characterising insider attacks is introduced in the form of attack goals, resources, constraints, modes, and attack paths. The conditions under which the attackers can maintain stealthy are examined in both temporal and spatial domains. With the inside knowledge, an attacker can use an attack graph to exploit system vulnerabilities and determine the high impact targets. To demonstrate the effectiveness of this analysis, a cyber‐physical system is constructed by using networks and a nuclear process control test facility with ports deliberately left open for attackers. Two attack scenarios are staged, and their characteristics and impacts are examined. This case study demonstrates how an insider attacker might mount an attack by using data tampering and how they can maintain stealthy before major damages are done to the physical system. The significance of this study is to uncover the techniques of insider attackers so that vulnerabilities can be mended.</jats:p>",https://doi.org/10.1049/iet-cps.2019.0087,CrossRef
Computer Engineering,Cyber-Physical Systems,Mixed Methods,Extending Signal Temporal Logic with Quantitative Semantics by Intervals for Robust Monitoring of Cyber-physical Systems,"<jats:p>
            Monitoring is the core procedure of runtime verification of cyber-physical systems (CPS) and provides an evaluation of a signal with respect to a given specification. For formally specifying requirements with time constraints for CPS, Signal Temporal Logic (STL) is a well-known specification language with powerful semantics. However, with existing semantics of STL it is not feasible to monitor signals with spatial deviation and time delay. Therefore, we introduce
            <jats:italic>STL with Quantitative Interval Semantics</jats:italic>
            to solve this problem. Based on this newly developed semantics, we derived an algorithm called
            <jats:italic>RoMoTeS</jats:italic>
            (Robust Monitoring for Temporal Specifications) to monitor a signal with finite length with respect to an STL formula. It provides a real-valued interval for every point in time, which contains all possible signed distances between the deviation polytope determined by the measured data point (exposed to deviation and delay) and the permissive space specified by an STL specification. Furthermore, it is proven that no satisfaction is reported when the signal is potentially falsifying such a specification. Finally, an automatic transmission controller model was used as a case study to show the applicability and usefulness of the proposed algorithm.
          </jats:p>",https://doi.org/10.1145/3377868,CrossRef
Computer Engineering,Cyber-Physical Systems,Mixed Methods,Meta-Learning to Improve Unsupervised Intrusion Detection in Cyber-Physical Systems,"<jats:p>
            <jats:bold>Artificial Intelligence (AI)-</jats:bold>
            based classifiers rely on
            <jats:bold>Machine Learning (ML)</jats:bold>
            algorithms to provide functionalities that system architects are often willing to integrate into critical
            <jats:bold>Cyber-Physical Systems (CPSs)</jats:bold>
            . However, such algorithms may misclassify observations, with potential detrimental effects on the system itself or on the health of people and of the environment. In addition, CPSs may be subject to threats that were not previously known, motivating the need for building
            <jats:bold>Intrusion Detectors (IDs)</jats:bold>
            that can effectively deal with zero-day attacks. Different studies were directed to compare misclassifications of various algorithms to identify the most suitable one for a given system. Unfortunately, even the most suitable algorithm may still show an unsatisfactory number of misclassifications when system requirements are strict. A possible solution may rely on the adoption of meta-learners, which build ensembles of base-learners to reduce misclassifications and that are widely used for supervised learning. Meta-learners have the potential to reduce misclassifications with respect to non-meta learners: however, misleading base-learners may let the meta-learner leaning towards misclassifications and therefore their behavior needs to be carefully assessed through empirical evaluation. To such extent, in this paper we investigate, expand, empirically evaluate, and discuss meta-learning approaches that rely on ensembles of unsupervised algorithms to detect (zero-day) intrusions in CPSs. Our experimental comparison is conducted by means of public datasets belonging to network intrusion detection and biometric authentication systems, which are common IDSs for CPSs. Overall, we selected 21 datasets, 15 unsupervised algorithms and 9 different meta-learning approaches. Results allow discussing the applicability and suitability of meta-learning for unsupervised anomaly detection, comparing metric scores achieved by base algorithms and meta-learners. Analyses and discussion end up showing how the adoption of meta-learners significantly reduces misclassifications when detecting (zero-day) intrusions in CPSs.
          </jats:p>",https://doi.org/10.1145/3467470,CrossRef
Computer Engineering,Cyber-Physical Systems,Mixed Methods,Out-of-distribution Detection in Dependent Data for Cyber-physical Systems with Conformal Guarantees,"<jats:p>Uncertainty in the predictions of learning-enabled components hinders their deployment in safety-critical cyber-physical systems (CPS). A shift from the training distribution of a learning-enabled component (LEC) is one source of uncertainty in the LEC’s predictions. Detection of this shift or out-of-distribution (OOD) detection on individual datapoints has therefore gained attention recently. But in many applications, inputs to CPS form a temporal sequence. Existing techniques for OOD detection in time-series data for CPS either do not exploit temporal relationships in the sequence or do not provide any guarantees on detection. We propose using deviation from the in-distribution temporal equivariance as the non-conformity measure in conformal anomaly detection framework for OOD detection in time-series data for CPS. Computing independent predictions from multiple conformal detectors based on the proposed measure and combining these predictions by Fisher’s method leads to the proposed detector CODiT with bounded false alarms.</jats:p>
          <jats:p>CODiT performs OOD detection on fixed-length windows of consecutive time-series datapoints by using Fisher value of the input window. We further propose performing OOD detection on real-time time-series traces of variable lengths with bounded false alarms. This can be done by using CODiT to compute Fisher values of the sliding windows in the input trace and combining these values by a merging function. Merging functions such as Harmonic Mean, Arithmetic Mean, Geometric Mean, Bonferroni Method, and so on, can be used to combine Fisher values of the sliding windows in the input trace, and the combined value can be used for OOD detection on the trace with bounded false alarm rate guarantees.</jats:p>
          <jats:p>
            We illustrate the efficacy of CODiT by achieving state-of-the-art results in two case studies for OOD detection on fixed-length windows. The first one is on an autonomous driving system with perception (or vision) LEC. The second case study is on a medical CPS for walking pattern or GAIT analysis where physiological (non-vision) data is collected with force-sensitive resistors attached to the subject’s body. For OOD detection on variable length traces, we consider the same case studies on the autonomous driving system and medical CPS for GAIT analysis. We report our results with four merging functions on the Fisher values computed by CODiT on the sliding windows of the input trace. We also compare the false alarm rate guarantees by these four merging functions in the autonomous driving system case study. Code, data, and trained models are available at
            <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""url"" xlink:href=""https://github.com/kaustubhsridhar/time-series-OOD"">https://github.com/kaustubhsridhar/time-series-OOD</jats:ext-link>
            .
          </jats:p>",https://doi.org/10.1145/3648005,CrossRef
Computer Engineering,Cyber-Physical Systems,Mixed Methods,Characterization of Link Quality Fluctuation in Mobile Wireless Sensor Networks,"<jats:p>
            Wireless sensor networks accommodating the mobility of nodes will play important roles in the future. In residential, rehabilitation, and clinical settings, sensor nodes can be attached to the body of a patient for long-term and uninterrupted monitoring of vital biomedical signals. Likewise, in industrial settings, workers as well as mobile robots can carry sensor nodes to augment their perception and to seamlessly interact with their environments. Nevertheless, such applications require reliable communications as well as high throughput. Considering the primary design goals of the sensing platforms (low-power, affordable cost, large-scale deployment, longevity, operating in the ISM band), maintaining reliable links is a formidable challenge. This challenge can partially be alleviated if the nature of link quality fluctuation can be known or estimated on time. Indeed, higher-level protocols such as handover and routing protocols rely on knowledge of link quality fluctuation to seamlessly transfer communication to alternative routes when the quality of existing routes deteriorates. In this article, we present the result of extensive experimental study to characterise link quality fluctuation in mobile environments. The study focuses on slow movements (&lt;5 km h
            <jats:sup>-1</jats:sup>
            ) signifying the movement of people and robots and transceivers complying to the IEEE 802.15.4 specification. Hence, we deployed mobile robots that interact with strategically placed stationary relay nodes. Our study considered different types of link quality characterisation metrics that provide complementary and useful insights. To demonstrate the usefulness of our experiments and observations, we implemented a link quality estimation technique using a Kalman Filter. To set up the model, we employed two link quality metrics along with the statistics we established during our experiments. The article will compare the performance of four proposed approaches with ours.
          </jats:p>",https://doi.org/10.1145/3448737,CrossRef
Computer Engineering,Cyber-Physical Systems,Mixed Methods,Modeling Adversarial Physical Movement in a Railway Station,"<jats:p>
            Many real-world attacks on cyber-physical systems involve physical intrusions that directly cause damage or facilitate cyber attacks. Hence, in this work, we investigate the security risk of organizations with respect to different adversarial models of physical movement behavior. We study the case in which an intrusion detection mechanism is in place to alert the system administrator when users deviate from their normal movement behavior. We then analyze how different user behaviors may present themselves as different levels of threats in terms of their normal movement behavior within a given building topology. To quantify the differences in movement behavior, we define a
            <jats:italic>WeightTopo</jats:italic>
            metric that takes into account the building topology in addition to the movement pattern. We demonstrate our approach on a railway system case study and show how certain user roles, when abused by attackers, are especially vulnerable in terms of the physical intrusion detection probability. We also evaluate quantitatively how the similarity between an attacker’s movement behavior and a user’s movement behavior affects the detection probability of the evaluated intrusion detection system. Certain individual users are found to pose a higher threat, implying the need for customized monitoring.
          </jats:p>",https://doi.org/10.1145/3349584,CrossRef
Computer Engineering,Cyber-Physical Systems,Design and Development,Architectural Modelling of Cyber Physical Systems Using UML,"<jats:p>Cyber-physical systems (CPS) is an exciting emerging research area that has drawn the attention of many researchers. However, the difficulties of computing and physical paradigm introduce a lot of trials while developing CPS, such as incorporation of heterogeneous physical entities, system verification, security assurance, and so on. A common or unified architecture plays an important role in the process of CPS design. This article introduces the architectural modeling representation of CPS. The layers of models are integrated from high level to lower level to get the general Meta model. Architecture captures the essential attributes of a CPS. Despite the rapid growth in IoT and CPS a general principled modeling approach for the systematic development of these new engineering systems is still missing. System modeling is one of the important aspects of developing abstract models of a system wherein, each model represents a different view or perspective of that system. With Unified Modeling Language (UML), the graphical analogy of such complex systems can be successfully presented.</jats:p>",https://doi.org/10.4018/ijcps.2019070102,CrossRef
Computer Engineering,Cyber-Physical Systems,Design and Development,Resilience quantification model for cyber‐physical power systems,"<jats:title>Abstract</jats:title><jats:p>As power grids develop, their structure becomes more complex, multi‐dimensional, and digitalised—hence, it is referred to as cyber‐physical infrastructure. The sensitivity of grids to extreme cyber and physical events is becoming a hot research topic due to the increasing rate of such events and their catastrophic consequences. To produce accurate and comprehensive measures, modelling and assessing the resilience of power systems must include both the physical and cyber domains. However, resilience quantification models that include both domains have not received sufficient attention. A novel resilience model and quantification framework are proposed. The model is based on a resilience trapezoid that depicts the different phases of the cyber and physical domains during severe natural or anthropogenic events. A resilience index is also proposed to measure the resilience levels of local nodes and entire systems, including various factors that contribute to the modelled degradation states. Severe weather conditions were modelled to examine the impact of this category of events on the proposed resilience model.</jats:p>",https://doi.org/10.1049/cps2.12098,CrossRef
Computer Engineering,Cyber-Physical Systems,Design and Development,Traffic-type Assignment for TSN-based Mixed-criticality Cyber-physical Systems,"<jats:p>This article focuses on mixed-criticality applications with functions that have different timing requirements, i.e., hard real-time (HRT), soft real-time (SRT), and functions that are not time-critical (NC). The applications are implemented on distributed cyber-physical systems that use IEEE Time-sensitive Networking (TSN). TSN is the product of an IEEE effort to bring deterministic real-time capabilities to IEEE 802.3 Ethernet. TSN supports the convergence of multiple traffic types, i.e., critical, real-time, and regular “best-effort” traffic within a single network: Time-triggered (TT), where the messages are transmitted based on static schedule tables, Audio-video Bridging (AVB), for dynamically scheduled messages with a guaranteed bandwidth and bounded delays, and Best Effort (BE), for which no timing guarantees are provided. The HRT messages have deadlines, whereas we capture the quality-of-service for the SRT messages using “utility functions.” Given the network topology, the set of application messages, including their routing, and the set of available AVB classes, we are interested in determining the traffic type of each message, such that all the HRT messages are schedulable and the total utility for the SRT messages is maximized. We propose a Tabu Search-based metaheuristic to solve this optimization problem. The proposed proof-of-concept tool has been evaluated using several benchmarks, including two realistic test cases.</jats:p>",https://doi.org/10.1145/3371708,CrossRef
Computer Engineering,Cyber-Physical Systems,Design and Development,Detecting covert channel attacks on cyber‐physical systems,"<jats:title>Abstract</jats:title><jats:p>Cyberattacks on cyber‐physical systems (CPS) have the potential to cause widespread disruption and affect the safety of millions of people. Machine learning can be an effective tool for detecting attacks on CPS, including the most stealthy types of attacks, known as covert channel attacks. In this study, the authors describe a novel hierarchical ensemble architecture for detecting covert channel attacks in CPS. Our proposed approach uses a combination of TCP payload entropy and network flows for feature engineering. Our approach achieves high detection performance, shortens the model training duration, and shows promise for effective detection of covert channel communications. This novel architecture closely mirrors the CPS attack stages in real‐life, providing flexibility and adaptability in detecting new types of attacks.</jats:p>",https://doi.org/10.1049/cps2.12078,CrossRef
Computer Engineering,Cyber-Physical Systems,Design and Development,Control Software Engineering Approaches for Cyber-Physical Systems: A Systematic Mapping Study,"<jats:p>Cyber-Physical Systems (CPS), robotics, the Internet of Things (IoT), and automotive systems are integral to modern technology. They are characterized by their safety criticality, accuracy, and real-time control requirements. Control software plays a crucial role in achieving these objectives by managing and coordinating the operations of various sub-systems. This article presents a novel Systematic Mapping Study (SMS) for control software engineering, analyzing 115 peer-reviewed papers. The study identifies, classifies, and maps existing solutions, providing a comprehensive and structured overview for practitioners and researchers. Our contributions include (1) a unique classification of literature into six research themes—engineering phases, engineering approaches, engineering paradigms, engineering artifacts, target application domains, and engineering concerns; (2) insights into the specificity of approaches to target technologies and phases; (3) the prominence of model-driven approaches for design and testing; (4) the lack of end-to-end engineering support in existing approaches; and (5) the emerging role of agile-based methods versus the dominance of waterfall-based methods. This article's significance lies in its thorough analysis and the high-level mapping of the solution space, offering new perspectives and a detailed roadmap for future research and innovation in control software engineering. The findings will guide advancements and best practices in the field, underscoring the article's impact.</jats:p>",https://doi.org/10.1145/3704737,CrossRef
Computer Engineering,Cyber-Physical Systems,Theoretical / Conceptual,Complex Systems + Systems Engineering = Complex Systems Engineeri,"One may define a complex system as a system in which phenomena emerge as a
consequence of multiscale interaction among the system's components and their
environments. The field of Complex Systems is the study of such
systems--usually naturally occurring, either bio-logical or social. Systems
Engineering may be understood to include the conceptualising and building of
systems that consist of a large number of concurrently operating and
interacting components--usually including both human and non-human elements. It
has become increasingly apparent that the kinds of systems that systems
engineers build have many of the same multiscale characteristics as those of
naturally occurring complex systems. In other words, systems engineering is the
engineering of complex systems. This paper and the associated panel will
explore some of the connections between the fields of complex systems and
systems engineering.",http://arxiv.org/abs/cs/0603127v1,arXiv
Computer Engineering,Cyber-Physical Systems,Theoretical / Conceptual,"Expansion of situations theory for exploring shared awareness in
  human-intelligent autonomous systems","Intelligent autonomous systems are part of a system of systems that interact
with other agents to accomplish tasks in complex environments. However,
intelligent autonomous systems integrated system of systems add additional
layers of complexity based on their limited cognitive processes, specifically
shared situation awareness that allows a team to respond to novel tasks.
Intelligent autonomous systems' lack of shared situation awareness adversely
influences team effectiveness in complex task environments, such as military
command-and-control. A complementary approach of shared situation awareness,
called situations theory, is beneficial for understanding the relationship
between system of systems shared situation awareness and effectiveness. The
current study elucidates a conceptual discussion on situations theory to
investigate the development of an system of systems shared situational
awareness when humans team with intelligent autonomous system agents. To ground
the discussion, the reviewed studies expanded situations theory within the
context of a system of systems that result in three major conjectures that can
be beneficial to the design and development of future systems of systems.",http://arxiv.org/abs/2406.04956v1,arXiv
Computer Engineering,Cyber-Physical Systems,Theoretical / Conceptual,Fractal trajectories of the dynamical system,"The scope of the paper is a theoretical analysis of the dynamical system, the
model of which was reduced to Weierstrasse function. A fractal structure of the
trajectory was proved and the entropy of the system information designated.",http://arxiv.org/abs/1604.03152v1,arXiv
Computer Engineering,Cyber-Physical Systems,Theoretical / Conceptual,Encrypted Control System with Quantizer,"This paper considers the design of encrypted control systems to secure data
privacy when the control systems operate over a network. In particular, we
propose to combine Paillier cryptosystem with a quantizer whose sensitivity
changes with the evolution of the system. This allows the encrypted control
system to balance between the cipher strength and processing time. Such an
ability is essential for control systems that are expected to run real-time. It
also allows the closed-loop system to achieve the asymptotic stability for
linear systems. Extensions to event-triggered control and nonlinear control
systems are also discussed.",http://arxiv.org/abs/1807.06717v1,arXiv
Computer Engineering,Cyber-Physical Systems,Theoretical / Conceptual,The Bouncing Penny and Nonholonomic Impacts,"The evolution of a Lagrangian mechanical system is variational. Likewise,
when dealing with a hybrid Lagrangian system (a system with discontinuous
impacts), the impacts can also be described by variations. These variational
impacts are given by the so-called Weierstrass-Erdmann corner conditions.
Therefore, hybrid Lagrangian systems can be completely understood by
variational principles.
  Unlike typical (unconstrained / holonomic) Lagrangian systems,
nonholonomically constrained Lagrangian systems are not variational. However,
by using the Lagrange-d'Alembert principle, nonholonomic systems can be
described as projections of variational systems. This paper works out the
analogous version of the Weierstrass-Erdmann corner conditions for nonholonomic
systems and examines the billiard problem with a rolling disk.",http://arxiv.org/abs/1909.11192v1,arXiv
